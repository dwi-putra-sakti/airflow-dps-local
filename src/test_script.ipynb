{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Myconn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import calendar\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from contextlib import contextmanager\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import Integer, String, CHAR, Date, Text, text, Float\n",
    "import os\n",
    "from modules.db_connection import get_engine\n",
    "import gc\n",
    "import math\n",
    "from sqlalchemy import Integer, String, CHAR, Date, Text, text, Float\n",
    "import os\n",
    "from datetime import datetime\n",
    "from modules.queries import df__penju, df__tokoawal, df__transaction, df__cus\n",
    "from modules.proses import (save_to_sql, add_index_and_foreign_keys, clean_and_transform_data, \n",
    "                            insert_disc2_grouping, insert_margin, rename_col_py_to_sql, insert_pk,\n",
    "                            get_tgl, get_stock, df__aging)\n",
    "from modules.queries import (df__tglso, df__tglsa, df__disc2, df__margin, df__artsizecode, df__stock_so,\n",
    "                             df__stock_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from django.db import connection\n",
    "def execute_raw_sql(query, params=None):\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(query, params or [])\n",
    "        columns = [col[0] for col in cursor.description]  # Get column names from cursor description\n",
    "        result = cursor.fetchall()\n",
    "\n",
    "        # Convert result to a list of dictionaries where column names are keys\n",
    "        formatted_result = [dict(zip(columns, row)) for row in result]\n",
    "    return formatted_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.queries import (df__penju, df__tokoawal, df__transaction, df__cus, get_monthly_sales_data, \n",
    "                             df__sales_thru_24, df__sales_thru_25, df__sales_thru_25_count_artikel, df__artsizecode, df__gdmd, fetch_data_margin,\n",
    "                             df__sales_, df__customers_pb, df__artikel_pb, df__artikel_pb_read, \n",
    "                             df__customers_pb_read, df__cus_user, df__sales_invoice,  df__sales_bz_disc2nd,\n",
    "                             df__sales_bz, df__sales_consi_disc2nd, df__sales_online_disc2nd, df__otsdm, df__otsdo,\n",
    "                             df__penju_today, df__artikel_status, df__tglso_, df__tglsa_ ) \n",
    "from modules.proses import (save_to_sql, add_index_and_foreign_keys, clean_and_transform_data, \n",
    "                            insert_disc2_grouping, insert_margin, rename_col_py_to_sql, insert_pk,\n",
    "                            get_tgl, get_stock, get_stock_data, merge_and_calculate, merge_detail, \n",
    "                            generate_rename_dict,trx_rt, process_data_margin,rename_col_sql_to_xlsx_gis,\n",
    "                            get_artikel_md, get_new_or_changed_rows,upsert_data, process_data_cus,\n",
    "                            sls_disctok,proc_pivot_table_n, proc_add_aging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## md size 1 all store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cus= df__cus()\n",
    "# SELECT fn_sizeid, fv_artsizecode, fn_artsizeid from gis_db1.articlesize_tb\n",
    "df_artsq= df__artsizecode(col='fv_artsizecode, fv_barcode, fv_sizename,fv_configname',stat='imp')\n",
    "# print(df_artsq.columns)\n",
    "\n",
    "# df_sqq24= df__Sales_thru_24('*')\n",
    "df_sq25= df__sales_thru_25('*')\n",
    "\n",
    "# df_sq24=df_sqq24.copy()\n",
    "# df_sq25=df_sqq25.copy()\n",
    "\n",
    "# df_sq24 = pd.merge(df_sq24, df_artsq, on='fv_artsizecode', how='left')\n",
    "# df_sq24 = pd.merge(df_sq24,df_cus,on='fn_whconsiid',how='left')\n",
    "\n",
    "df_sq25 = pd.merge(df_sq25, df_artsq, on='fv_artsizecode', how='left', copy=False)\n",
    "df_sq25.info(memory_usage='deep')\n",
    "df_sq25 = pd.merge(df_sq25,df_cus,on='fn_whconsiid',how='left', copy=False)\n",
    "df_sq25.info(memory_usage='deep')\n",
    "# df_sq24['fn_artsizeid']=df_sq24['fn_artsizeid'].astype(int)\n",
    "df_sq25['fn_artsizeid'] = pd.to_numeric(df_sq25['fn_artsizeid'], downcast='integer')\n",
    "df_sq25.info(memory_usage='deep')\n",
    "\n",
    "# df_sq24['pk_id']=df_sq24['fn_whconsiid'].astype(str)+\"/\"+df_sq24['fn_artsizeid'].astype(str)\n",
    "df_sq25['pk_id']=df_sq25['fn_whconsiid'].astype(str)+\"/\"+df_sq25['fn_artsizeid'].astype(str)\n",
    "df_sq25=df_sq25.drop(columns=['fn_cusid'])\n",
    "#############\n",
    "# print(df_sq25.columns)  \n",
    "# df_sq24 = generate_rename_dict(2024, 2024, df_sq24)\n",
    "df_sq25 = generate_rename_dict(2025, 2025, df_sq25)\n",
    "# kolom_baru24 = (['pk_id','toko','code toko','artikel','size'] + \n",
    "#                 [col for col in df_sq24.columns if col not in \n",
    "#                 ['pk_id','toko','code toko','artikel', 'size','barcode','fv_artsizecode','fn_artsizeid','fn_whconsiid']])\n",
    "\n",
    "kolom_baru25 = (['pk_id','toko','code toko','artikel','size'] + \n",
    "                [col for col in df_sq25.columns if col not in \n",
    "                ['pk_id','toko','code toko','artikel', 'size','barcode','fv_artsizecode','fn_artsizeid','fn_sizeid','fn_whconsiid']])\n",
    "\n",
    "\n",
    "# df_sq24 = df_sq24[kolom_baru24]\n",
    "df_sq25 = df_sq25[kolom_baru25]\n",
    "del df_artsq, df_cus\n",
    "# gc.collect()\n",
    "# df_sq24 = df_sq24.loc[~(df_sq24['artikel'].isnull())]\n",
    "df_sq25 = df_sq25.loc[~(df_sq25['artikel'].isnull())]\n",
    "df = proc_pivot_table_n(df_sq25,\n",
    "                values='stock akhir',\n",
    "                col_index=['code toko','artikel'],\n",
    "                columns=['size'],\n",
    "                col_ket=['fv_configname'],col_select=['code toko','artikel','status'])\n",
    "df_sq25=pd.merge(df_sq25,df[['code toko','artikel','status']],how='left',on=['code toko','artikel'])\n",
    "del df\n",
    "# gc.collect()\n",
    "df_sq25=df_sq25.rename(columns={'fv_configname':'bot/top'})\n",
    "# df_result24=df_sq24.select_dtypes(include='number').sum()\n",
    "df_sq25=proc_add_aging(df_sq25)\n",
    "\n",
    "# df_sq24.to_excel('/opt/share/For MDstock artikel all store 2024.xlsx', index=False)\n",
    "total_rows = len(df_sq25)\n",
    "chunks = 3\n",
    "chunk_size = math.ceil(total_rows / chunks)  # otomatis bagi 3\n",
    "\n",
    "with pd.ExcelWriter('/opt/share/For MD/stock artikel all store 2025.xlsx', engine='xlsxwriter') as writer:\n",
    "    for i in range(chunks):\n",
    "        start_row = i * chunk_size\n",
    "        end_row = min(start_row + chunk_size, total_rows)\n",
    "        sheet_df = df_sq25.iloc[start_row:end_row]\n",
    "        sheet_df.to_excel(writer, index=False, sheet_name=f'data_{i+1}')\n",
    "# df_sq24.to_excel('/opt/share/For MDstock artikel all store 2024.xlsx', index=False)\n",
    "# df_sq25.to_excel('/opt/share/For MD/stock artikel all store 2025.xlsx', index=False)\n",
    "\n",
    "# df_result24.to_excel('df_result24.xlsx')\n",
    "# df_result25.to_excel('/opt/share/PB DB/df_result25.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sq25=df_sq25.rename(columns={'artikel':'fv_barcode'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sq25=proc_add_aging(df_sq25)\n",
    "\n",
    "# df_sq24.to_excel('/opt/share/For MDstock artikel all store 2024.xlsx', index=False)\n",
    "total_rows = len(df_sq25)\n",
    "chunks = 3\n",
    "chunk_size = math.ceil(total_rows / chunks)  # otomatis bagi 3\n",
    "\n",
    "# with pd.ExcelWriter('/opt/share/For MD/stock artikel all store 2025.xlsx', engine='xlsxwriter') as writer:\n",
    "#     for i in range(chunks):\n",
    "#         start_row = i * chunk_size\n",
    "#         end_row = min(start_row + chunk_size, total_rows)\n",
    "#         sheet_df = df_sq25.iloc[start_row:end_row]\n",
    "#         sheet_df.to_excel(writer, index=False, sheet_name=f'data_{i+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=f'''\n",
    "    sELECT  fv_toko, fv_spv,fv_barcode, a.fv_artsizecode,fv_sizename,  fn_stock, fv_name_doc,fv_stat, fd_tgltrx FROM gis_local.trxstx_pb25 a\n",
    "    left join artikel_pb b on a.fv_artsizecode=b.fv_artsizecode  \n",
    "    left join customers_pb c on a.fn_whconsiid=c.fn_whconsiid\n",
    "    where fv_spv = 'Muhamad Ali Alfaridzy'\n",
    "    and fd_tgltrx between '2025-06-01' and '2025-08-22' ;\n",
    "    '''\n",
    "with get_engine('imp') as engine_imp:\n",
    "    df = pd.read_sql(query, engine_imp)\n",
    "    df.to_excel('Exekusi Ali.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sq25.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_index=['code toko','artikel']\n",
    "col_ket=['fv_configname']\n",
    "columns=['size']\n",
    "df=df_sq25\n",
    "values='stock akhir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sizes_bottom = ['25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38',\n",
    "                        '39', '40', '42', '44', 'ALLSIZE']\n",
    "all_sizes_top = ['FS', 'XS', 'SS', 'S', 'M', 'L', 'XL', 'XXL', 'XXXL']\n",
    "all_sizes = all_sizes_bottom + all_sizes_top\n",
    "df_ket = df[col_index + col_ket].drop_duplicates()\n",
    "# Buat pivot table\n",
    "pivot_df = pd.pivot_table(\n",
    "    df,\n",
    "    values=values,\n",
    "    index=col_index,\n",
    "    columns=columns,\n",
    "    aggfunc='sum',\n",
    "    fill_value=0).reset_index()\n",
    "\n",
    "# Merge kembali kolom tambahan\n",
    "pivot_table = pd.merge(pivot_df, df_ket, on=col_index, how='left')\n",
    "for size in all_sizes:\n",
    "    if size not in pivot_table.columns:\n",
    "        pivot_table[size] = 0\n",
    "    pivot_table[size] = pd.to_numeric(pivot_table[size], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Atur ulang urutan: non-size columns dulu, lalu size columns\n",
    "size_columns = [size for size in all_sizes if size in pivot_table.columns]\n",
    "other_columns = [col for col in pivot_table.columns if col not in size_columns]\n",
    "pivot_table = pivot_table[other_columns + size_columns]\n",
    "\n",
    "# Hitung status (BROKEN / SEHAT)\n",
    "top_columns = [\"L\", \"M\", \"S\", \"XL\", \"XS\", \"XXL\", \"XXXL\"]\n",
    "bottom_columns = ['25', '26', '27', '28', '29', '30', '31', '32', '33', '34',\n",
    "                    '35', '36', '37', '38', '39', '40', '42', '44']\n",
    "\n",
    "pivot_table['status'] = np.where(\n",
    "    (pivot_table[\"fv_configname\"] == \"TOP\") & (pivot_table[top_columns].gt(0).sum(axis=1) < 3), \"BROKEN\",\n",
    "    np.where(\n",
    "        (pivot_table[\"fv_configname\"] == \"BOTTOM\") & (pivot_table[bottom_columns].gt(0).sum(axis=1) < 5), \"BROKEN\",\n",
    "        \"SEHAT\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Hitung total jumlah qty size (sumQty)\n",
    "pivot_table['sumQty'] = pivot_table[size_columns].sum(axis=1)\n",
    "pivot_table=pivot_table.drop(columns=['fv_configname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.merge(df,pivot_table[['code toko','artikel','status']],how='left',on=['code toko','artikel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## md size 2 all artikel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artsq= df__artsizecode(col='fv_artsizecode, fv_barcode, fv_sizename,fv_configname',stat='imp')\n",
    "df_sqq24= df__sales_thru_24('*')\n",
    "df_sqq25= df__sales_thru_25('*')\n",
    "df_sg= df__gdmd()\n",
    "\n",
    "df_sq24=df_sqq24.copy()\n",
    "df_sq25=df_sqq25.copy()\n",
    "df_sq24=df_sq24.drop(columns=[df_sq24.columns[1]])\n",
    "df_sq25=df_sq25.drop(columns=[df_sq25.columns[1]])\n",
    "df_sq24['fn_artsizeid']=df_sq24['fn_artsizeid'].astype(str)\n",
    "df_sq25['fn_artsizeid']=df_sq25['fn_artsizeid'].astype(str)\n",
    "df_sq24 = df_sq24.groupby(['fv_artsizecode','fn_artsizeid'])[df_sq24.columns[2:]].sum().reset_index()\n",
    "df_sq25 = df_sq25.groupby(['fv_artsizecode','fn_artsizeid'])[df_sq25.columns[2:]].sum().reset_index()\n",
    "\n",
    "# Buat dictionary rename berdasarkan data\n",
    "df_sq24 = generate_rename_dict(2024, 2024, df_sq24)\n",
    "df_sq25 = generate_rename_dict(2025, 2025, df_sq25)\n",
    "df=pd.merge(df_sq24,df_sq25,how='outer',on=['fn_artsizeid','fv_artsizecode'])\n",
    "# df['qty retur']=df['qty retur_x'].fillna(0)+df['qty retur_y'].fillna(0)\n",
    "print(df.columns)\n",
    "df=df.rename(columns={\n",
    "    'qty retur_x': 'retur 2024',\n",
    "    'qty retur_y': 'retur 2025',})\n",
    "df['stock akhir']=df['stock akhir_y']\n",
    "df['total sales qty']=df['total sales qty_x'].fillna(0)+df['total sales qty_y'].fillna(0)\n",
    "df['total stock']=df['total sales qty']+df['stock akhir'].fillna(0)\n",
    "df=df.drop(columns=['total stock_x', 'stock akhir_x', 'stock akhir_y',\n",
    "                    'total stock_y','total sales qty_x','total sales qty_y'])\n",
    "df_sg['fv_artsizecode']=df_sg['fv_artsizecode'].str.upper()\n",
    "df = df.groupby(['fv_artsizecode'], as_index=False).sum()\n",
    "df = pd.merge(df, df_sg, on='fv_artsizecode', how='outer')\n",
    "df = pd.merge(df, df_artsq, on='fv_artsizecode', how='left')\n",
    "df =generate_rename_dict(2025, 2025, df)\n",
    "# df\n",
    "df = df.loc[~(df['artikel'].isnull())]\n",
    "df2=df.fillna(0)\n",
    "df2=df2[df2['artikel']!=0]\n",
    "kolom_baru24 = ['artikel', 'size'] + [col for col in df2.columns if col not in ['artikel', 'size','fv_artsizecode','fn_artsizeid']]\n",
    "df2 = df2[kolom_baru24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''\n",
    "    SELECT fv_artsizecode, sum(stock) stock, fv_whtypename FROM gis_local.stock_gudang_pb where fv_whtypename in ('RETUR','PUSAT','KONSINETTO','QC') group by fv_artsizecode\n",
    "    '''\n",
    "with get_engine('imp') as engine_imp:\n",
    "    df = pd.read_sql(query, engine_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.loc[~df['fv_artsizecode'].isnull(),'stock']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg['stock'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stock gudang'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['artikel'].isnull()) & df['stock gudang']>0,'stock gudang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = proc_pivot_table_n(df2,\n",
    "                    values='stock akhir',\n",
    "                    col_index=['artikel'],\n",
    "                    columns=['size'],\n",
    "                    col_ket=['fv_configname'])\n",
    "df2=pd.merge(df2,df[['artikel','status']],how='left',on=['artikel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sq25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.rename(columns={'fv_configname':'bot/top'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.merge(df2,df[['artikel','status']],how='left',on=['artikel'])\n",
    "df2=df2.rename(columns={'fv_configname':'bot/top'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_files = generate_stock_files(base_path, start_year, end_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_data_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_data_25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test sales to sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import calendar\n",
    "from contextlib import contextmanager\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import Integer, String, CHAR, Date, Text, text, Float\n",
    "import os\n",
    "from datetime import datetime\n",
    "from modules.queries import (df__penju, df__tokoawal, df__transaction, df__cus, get_monthly_sales_data, \n",
    "                             df__sales_thru_24, df__sales_thru_25, df__artsizecode, df__gdmd, fetch_data_margin,\n",
    "                             df__sales_, df__customers_pb, df__artikel_pb) \n",
    "from modules.proses import (save_to_sql, add_index_and_foreign_keys, clean_and_transform_data, \n",
    "                            insert_disc2_grouping, insert_margin, rename_col_py_to_sql, insert_pk,\n",
    "                            get_tgl, get_stock, get_stock_data, merge_and_calculate, merge_detail, \n",
    "                            generate_rename_dict,trx_rt, process_data_margin,rename_col_sql_to_xlsx_gis)\n",
    "def margin_tb(year):\n",
    "    df_trx, df_artimp, df_artred, df_atrxu, df_mrgincat, df_mrgin = fetch_data_margin(year)\n",
    "    df_trx_final = process_data_margin(df_trx, df_artimp, df_artred, df_atrxu, df_mrgincat, df_mrgin)\n",
    "    save_to_sql(df_trx_final, f'margin{str(year)[-2:]}_pb', 'imp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year=2025\n",
    "start_time2= datetime.now().strftime('%Y-%m-%d %H:%M:%S')    \n",
    "print(\"\\n[INFO] Script sales_to_sql mulai dijalankan.\")\n",
    "print(f\"[INFO] Timestamp: {start_time2}\\n\")\n",
    "\n",
    "df1= df__penju(year)\n",
    "df= df1.copy()\n",
    "df= clean_and_transform_data(df)\n",
    "df=insert_disc2_grouping(df)\n",
    "margin_tb(year)\n",
    "df=insert_margin(df,year)\n",
    "sum_col=['fn_jualpersize','total_penjualan2','total_disc2']\n",
    "agg_col=['fn_whconsiid', 'fv_artsizecode', 'tgltrx']\n",
    "fisrt_col=[col for col in df.columns if col not in sum_col + agg_col]\n",
    "col_col=fisrt_col+[col for col in df.columns if col not in fisrt_col]\n",
    "df=df[col_col]\n",
    "df['event']=df['event'].fillna(df['status_artikel'])\n",
    "df['fn_margin']=df['fn_margin'].fillna(0)\n",
    "df=rename_col_py_to_sql(df).sort_values(by='fn_idurut')\n",
    "df=insert_pk(df)\n",
    "df['fn_nett_amount'] = (df['fn_totalpenjualan']-df['fn_total_disc2']) * (1-df['fn_margin']/100)\n",
    "df['fn_sim_amount'] = np.where(df['fc_status'] == \"X\", df['fn_totalpenjualan'], df['fn_nett_amount'])\n",
    "df['fn_dpp'] = df['fn_nett_amount']/1.11\n",
    "df['fn_dpp2'] = df['fn_sim_amount']/1.11\n",
    "table_name = f\"pb_penjualan_{year}\"\n",
    "save_to_sql(df, table_name, 'imp')\n",
    "\n",
    "add_index_and_foreign_keys(\n",
    "    'imp',\n",
    "    table_name,\n",
    "    index_cols=[\"fn_whconsiid\", \"fv_artsizecode\"],\n",
    "    foreign_keys=[\n",
    "        {\n",
    "            \"column\": \"fn_whconsiid\",\n",
    "            \"ref_table\": \"customers_pb\",\n",
    "            \"ref_column\": \"fn_whconsiid\",\n",
    "            \"constraint_name\": f\"fk_whconsiid_pb_{year}\"\n",
    "        },\n",
    "        {\n",
    "            \"column\": \"fv_artsizecode\",\n",
    "            \"ref_table\": \"artikel_tb\",\n",
    "            \"ref_column\": \"fv_artsizecode\",\n",
    "            \"constraint_name\": f\"fk_sales_artsize_pb_{year}\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fn_nett_amount'] = df['fn_totalpenjualan'] * (1-df['fn_margin']/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_sql(df, table_name, 'imp')\n",
    "\n",
    "add_index_and_foreign_keys(\n",
    "    'imp',\n",
    "    table_name,\n",
    "    index_cols=[\"fn_whconsiid\", \"fv_artsizecode\"],\n",
    "    foreign_keys=[\n",
    "        {\n",
    "            \"column\": \"fn_whconsiid\",\n",
    "            \"ref_table\": \"customers_pb\",\n",
    "            \"ref_column\": \"fn_whconsiid\",\n",
    "            \"constraint_name\": f\"fk_whconsiid_pb_{year}\"\n",
    "        },\n",
    "        {\n",
    "            \"column\": \"fv_artsizecode\",\n",
    "            \"ref_table\": \"artikel_pb\",\n",
    "            \"ref_column\": \"fv_artsizecode\",\n",
    "            \"constraint_name\": f\"fk_artsize_pb_{year}\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _df_cek = define_sql_types(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _df_cek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.queries import (df__tglso, df__tglsa, df__disc2, df__margin, df__artsizecode, df__stock_so,\n",
    "                             df__stock_sa, df__retur, df__size, generate_months_for_year, df__rtt)\n",
    "from modules.db_connection import get_engine\n",
    "from modules.proses import (save_to_sql, add_index_and_foreign_keys, clean_and_transform_data)\n",
    "from sqlalchemy import Integer, String, CHAR, Date, Text, text, Float\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trx_rt():\n",
    "    df_trxstx= df__rtt()\n",
    "    df_trxstx['tgltrx'] = pd.to_datetime(df_trxstx['tgltrx'], format='%m-%d-%Y')\n",
    "    df_trxstx['fn_whconsiid']=df_trxstx['fn_whconsiid'].astype(int)\n",
    "    df_trxstx['stock']=df_trxstx['stock'].astype(int)\n",
    "    df_trxstx=df_trxstx.rename(columns={\n",
    "        'stock':'fn_stock', \n",
    "        'tgltrx':'fd_tgltrx', \n",
    "        'stat':'fv_stat'\n",
    "    })\n",
    "    save_to_sql(df_trxstx, 'trxrt_pb', 'imp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trx_rt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.db_connection import get_engine\n",
    "from datetime import datetime, date\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_str = date.today().strftime('%Y-%m-%d')\n",
    "with get_engine('write') as engine:\n",
    "        query=f'''\n",
    "        SELECT fn_whconsiid, fv_artsizecode, IFNULL(SUM(totalplus), 0) AS stock, tgltrx, stat\n",
    "    FROM (\n",
    "    select\n",
    "        SUBSTRING_INDEX(a.fv_noreceiving, '/', -1) AS fn_whconsiid,\n",
    "        b.fv_artsizecode, \n",
    "        b.fn_qtyterima AS totalplus,\n",
    "        DATE_FORMAT(a.fd_tglreceive, '%%m-%%d-%%Y') tgltrx, 'recieve' as stat\n",
    "        FROM     t_receivingmst a\n",
    "        INNER JOIN t_receivingdtl b ON a.fv_noreceiving = b.fv_noreceiving\n",
    "        WHERE a.fc_status <> 'F'\n",
    "        AND (a.fd_tglsistem BETWEEN '{today_str} 00:00:00' and '{today_str} 23:59:00'))\n",
    "        AS subquery\n",
    "    GROUP BY fn_whconsiid, fv_artsizecode, tgltrx;'''\n",
    "        df= pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import margin_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_tb(2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AS1 cosiment sales mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modules.proses import \n",
    "from modules.queries import df__sales_, df__customers_pb, df__artikel_pb\n",
    "from modules.proses import rename_col_sql_to_xlsx_gis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df__sales_(2023,'fn_whconsiid,fv_artsizecode,fv_event,fd_tgltrx,fn_jualpersize,fn_totalpenjualan,fn_nett_amount')\n",
    "df_artpb=df__artikel_pb('fv_artsizecode,fv_barcode,fv_catname,fv_fitname,fv_brandname,fv_divname,fv_sizename,fm_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cuspb=df__customers_pb('fv_toko,fc_code,fn_whconsiid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.merge(df, df_artpb, on='fv_artsizecode', how='left')\n",
    "df=pd.merge(df, df_cuspb, on='fn_whconsiid', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['fd_tgltrx','fc_code','fv_toko','fv_barcode','fv_catname','fv_fitname','fv_brandname',\n",
    "    'fv_divname','fv_sizename','fv_event','fm_price','fn_totalpenjualan','fn_nett_amount','fn_jualpersize']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=rename_col_sql_to_xlsx_gis(df)\n",
    "# name_col = {\n",
    "#         'fd_tgltrx': 'Sales Date',\n",
    "#         'fc_code': 'Code Store',\n",
    "#         'fv_toko': 'Store',\n",
    "#         'fv_barcode':'Code Articel',\n",
    "#         'fv_catname': 'Kategori',\n",
    "#         'fv_fitname': 'Name Articel',\n",
    "#         'fv_brandname': 'Brand',\n",
    "#         'fv_divname': 'Division',\n",
    "#         'fv_sizename': 'Size',\n",
    "#         'fv_event': 'Status Article',\n",
    "#         'fm_price': 'HET',\n",
    "#         'fn_totalpenjualan': 'Total Nett Kassa',\n",
    "#         'fn_nett_amount': 'Nett Amount',\n",
    "#         'fn_jualpersize':'Qty'\n",
    "#     }\n",
    "#     # Lakukan rename dan sort\n",
    "# df = df.rename(columns=name_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = r'\\\\192.168.1.254\\Share\\_From Valdi\\For Alokator\\Sales cosiment\\2023'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sales Date'] = pd.to_datetime(df['Sales Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tambahkan kolom bulan (format YYYY-MM)\n",
    "df['bulan_num'] = df['Sales Date'].dt.strftime('%m')\n",
    "df['bulan'] = df['Sales Date'].dt.strftime('%b')  # Nama bulan 3 huruf\n",
    "df['tahun'] = df['Sales Date'].dt.strftime('%y')  # Tahun dua digit\n",
    "\n",
    "# Simpan ke Excel per bulan\n",
    "for (bulan_num, bulan, tahun), group in df.groupby(['bulan_num', 'bulan', 'tahun']):\n",
    "    nama_file = f\"{output_folder}\\Sale Consi {bulan_num} {bulan} {tahun}.xlsx\"\n",
    "    group.drop(columns=['bulan', 'tahun', 'bulan_num']).to_excel(nama_file, index=False)\n",
    "    print(f\"✅ File disimpan: {nama_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AS2 Consiment Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.db_connection import get_engine\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.queries import (fetch_data_margin,df__artikel_pb, df__customers_pb,df__disc2)\n",
    "from modules.proses import (save_to_sql, clean_and_transform_data,insert_disc2_grouping, insert_margin, process_data_margin, rename_col_py_to_sql, insert_pk,)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "bulan_str = 'FEB'\n",
    "bulan_str = 'MAR'\n",
    "year = 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import calendar\n",
    "\n",
    "# 1. Konversi nama bulan ke angka\n",
    "bulan_angka = datetime.datetime.strptime(bulan_str, '%b').month\n",
    "\n",
    "# 2. Dapatkan awal bulan\n",
    "awal_bulan = datetime.date(year, bulan_angka, 1)\n",
    "\n",
    "# 3. Dapatkan akhir bulan\n",
    "akhir_hari = calendar.monthrange(year, bulan_angka)[1]  # jumlah hari di bulan tsb\n",
    "akhir_bulan = datetime.date(year, bulan_angka, akhir_hari)\n",
    "query = f'''SELECT \n",
    "    b.fn_idurut,\n",
    "    a.fv_nopenjualan, \n",
    "    DATE_FORMAT(a.fd_tgljual, '%%m-%%d-%%Y') tgltrx,\n",
    "    DATE_FORMAT(a.fd_tglinput, '%%m-%%d-%%Y %%H:%%i:%%s') AS tglinput,\n",
    "    a.fn_cusid fn_whconsiid,\n",
    "    b.fv_artsizecode,\n",
    "    b.fn_hargajualawal price,\n",
    "    b.fn_disc disc,\n",
    "    b.fn_hargasatuan ,\n",
    "    b.fn_jualpersize ,\n",
    "    b.fn_hargasatuan*b.fn_jualpersize as total_penjualan2,\n",
    "    e.fv_nameevent 'EVENT',\n",
    "    a.fn_userid,\n",
    "    b.fn_discid disc_id, \n",
    "    b.fv_namemark status_artikel, b.fv_descevent \"desc\",\n",
    "    d.fv_namedisc 'Disc Name',\n",
    "    d.fv_namemark 'event',\n",
    "    b.fn_tabel,\n",
    "    a.fv_nobonlantai,\n",
    "    f.fv_articlename,\n",
    "    b.fc_bonus,\n",
    "    a.fn_grpcusid, \n",
    "    a.fv_namacust\n",
    "FROM \n",
    "    t_penjualanmst a \n",
    "INNER JOIN \n",
    "    t_penjualandtl_goods b ON a.fv_nopenjualan = b.fv_nopenjualan\n",
    "LEFT JOIN \n",
    "    (SELECT a.fn_discid, b.fv_namemark, a.fv_namedisc FROM discount_tb a\n",
    "        INNER JOIN markdown_tb b ON a.fn_markid = b.fn_markid) d ON b.fn_discid = d.fn_discid\n",
    "LEFT JOIN event_tb e ON b.fn_idevent=e.fn_idevent\n",
    "LEFT JOIN article_tb f on b.fv_articlecode=f.fv_articlecode\n",
    "WHERE \n",
    "    a.fv_nopenjualan LIKE 'PJ%%' and b.fc_status <> 'F' \n",
    "    and a.fc_status <> 'F' \n",
    "    and a.fd_tgljual BETWEEN '{awal_bulan} 00:00:01' and '{akhir_bulan} 23:59:59'\n",
    "'''\n",
    "with get_engine('read') as engine:  # Menggunakan context manager dengan with\n",
    "    df = pd.read_sql(query, engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consi= pd.read_csv(f'CONSI SALES {bulan_str} 2025.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consi.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x=df.copy()\n",
    "df11=df_x.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11= clean_and_transform_data(df11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11=insert_disc2_grouping(df11,'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11.loc[df11['fv_nopenjualan']=='PJ/DPS/2503/00001/1143',['tglinput','artsize_voucher','artsize_affiliate','artsize_premi','artsize_servisfee']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# margin_tb(year)\n",
    "df11=insert_margin(df11,year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artpb=df__artikel_pb('*')\n",
    "df_cuspb=df__customers_pb('*')\n",
    "df_markdown= query = f'''\n",
    "SELECT * FROM markdown_tb\n",
    "'''\n",
    "with get_engine('read') as engine:  # Menggunakan context manager dengan with\n",
    "    df_markdown = pd.read_sql(query, engine)\n",
    "    \n",
    "query = f'''\n",
    "SELECT * FROM user_tb\n",
    "'''\n",
    "with get_engine('read') as engine:  # Menggunakan context manager dengan with\n",
    "    df_user = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Margin SHUU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_engine('read') as engine:\n",
    "        query='''select a.fv_nopenjualan, fn_potonganharga, fn_totalqty qty from\n",
    "        (SELECT fv_nopenjualan, fn_potonganharga FROM t_penjualanmst\n",
    "        where fn_potonganharga <> 0) a \n",
    "        left join (SELECT fv_nopenjualan, sum(fn_jualpersize) fn_totalqty FROM t_penjualandtl_goods\n",
    "        group by fv_nopenjualan) b on a.fv_nopenjualan=b.fv_nopenjualan\n",
    "        '''\n",
    "        df_disc2nd= pd.read_sql(query, engine)\n",
    "\n",
    "with get_engine('read') as engine:\n",
    "        query='''select a.fv_nopenjualan, servisfee, premi, affiliate, voucher, fn_totalqty qty from\n",
    "        (SELECT fv_nopenjualan, fm_asuransi servisfee, fm_ongkir premi, fm_voucher affiliate, fm_lainlain voucher FROM gis_db1.t_penjualanpotongan\n",
    "        ) a \n",
    "        left join (SELECT fv_nopenjualan, sum(fn_jualpersize) fn_totalqty FROM t_penjualandtl_goods\n",
    "        group by fv_nopenjualan) b on a.fv_nopenjualan=b.fv_nopenjualan\n",
    "        '''\n",
    "        df_disc3nd= pd.read_sql(query, engine)\n",
    "df_disc2nd=pd.concat([df_disc2nd, df_disc3nd])\n",
    "\n",
    "# Jumlahkan semua kolom kecuali 'qty', dan ambil 'qty' pertama\n",
    "agg_dict = {col: 'sum' for col in df_disc2nd.columns if col not in ['fv_nopenjualan', 'qty']}\n",
    "agg_dict['qty'] = 'first'\n",
    "\n",
    "# Grouping\n",
    "df_disc2nd = df_disc2nd.groupby('fv_nopenjualan', as_index=False).agg(agg_dict)\n",
    "\n",
    "df_disc2nd['artsize_disc2nd'] = np.where(df_disc2nd['fn_potonganharga'] == 0,0,(df_disc2nd['fn_potonganharga'] / df_disc2nd['qty']).round(2))\n",
    "df_disc2nd['artsize_servisfee'] = np.where(df_disc2nd['servisfee'] == 0,0,(df_disc2nd['servisfee'] / df_disc2nd['qty']).round(2))\n",
    "df_disc2nd['artsize_premi'] = np.where(df_disc2nd['premi'] == 0,0,(df_disc2nd['premi'] / df_disc2nd['qty']).round(2))\n",
    "df_disc2nd['artsize_affiliate'] = np.where(df_disc2nd['affiliate'] == 0,0,(df_disc2nd['affiliate'] / df_disc2nd['qty']).round(2))\n",
    "df_disc2nd['artsize_voucher'] = np.where(df_disc2nd['voucher'] == 0,0,(df_disc2nd['voucher'] / df_disc2nd['qty']))\n",
    "df_disc2nd.replace([np.inf, -np.inf], 0, inplace=True) \n",
    "# # Mendapatkan nama kolom pertama dan terakhir\n",
    "# col0 = df_disc2nd.columns[0]\n",
    "# col_last = df_disc2nd.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disc2nd = df_disc2nd.fillna(0)\n",
    "# df_disc2nd = df_disc2nd.groupby(['fv_nopenjualan'], as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas==1.5.3 sqlalchemy==1.4.46\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disc2nd.loc[df_disc2nd['fv_nopenjualan']=='PJ/DPS/2501/00113/1128']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_sql(df_disc2nd, f'disc2artsize_pb', 'imp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "df_disc2= df__disc2()\n",
    "df = pd.merge(df, df_disc2, on='fv_nopenjualan', how='left')\n",
    "df = df.loc[~df.duplicated()]\n",
    "df[['artsize_disc2nd','artsize_servisfee','artsize_premi','artsize_affiliate','artsize_voucher']] = df[\n",
    "    ['artsize_disc2nd','artsize_servisfee','artsize_premi','artsize_affiliate','artsize_voucher']].fillna(0)\n",
    "df['total_disc2']=df['artsize_disc2nd']*df['fn_jualpersize']\n",
    "sum_col=['fn_jualpersize','total_penjualan2','total_disc2',\n",
    "            'artsize_servisfee','artsize_premi','artsize_affiliate','artsize_voucher']\n",
    "agg_col=['fv_nopenjualan','fn_whconsiid', 'fv_artsizecode', 'tgltrx']\n",
    "fisrt_col=[col for col in df.columns if col not in sum_col + agg_col]\n",
    "print(f\" {sum_col}\")\n",
    "print(f\" {fisrt_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['fv_nopenjualan']=='PJ/DPS/2503/00001/1143']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_margin= query = f'''\n",
    "SELECT * FROM margin25_pb\n",
    "'''\n",
    "with get_engine('imp') as engine:  # Menggunakan context manager dengan with\n",
    "    df_margin = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.queries import (fetch_data_margin)\n",
    "df_trx, df_artimp, df_artred, df_atrxu, df_mrgincat, df_mrgin = fetch_data_margin(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_engine('read') as engine:\n",
    "    query_trx = f'''\n",
    "        SELECT fn_idurut, fn_whconsiid, fd_tgljual fd_tgltrx, fv_artsizecode \n",
    "        FROM gis_db1.t_penjualandtl_goods where fd_tgljual BETWEEN '{year}-01-01' AND '{year}-12-31'\n",
    "        '''\n",
    "    df_trx = pd.read_sql(query_trx, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trxartcat = pd.merge(df_trx, df_artimp, on='fv_artsizecode', how='left')\n",
    "df_trxartcat = pd.merge(df_trxartcat, df_artred, on='fv_barcode', how='left')\n",
    "df_trxartcat = pd.merge(df_trxartcat, df_atrxu, on='fn_idurut', how='left')\n",
    "df_trxartcat[\"fn_markid\"] = df_trxartcat[\"fn_markid\"].combine_first(df_trxartcat[\"markid2\"])\n",
    "df_trxartcat['fn_markid'] = df_trxartcat['fn_markid'].astype(int)\n",
    "df_trxartcat[\"fd_tgltrx\"] = pd.to_datetime(df_trxartcat[\"fd_tgltrx\"])\n",
    "\n",
    "# Merge with margin data\n",
    "merged_df = pd.merge(df_trxartcat, df_mrgincat, on=[\"fn_whconsiid\", \"fn_catid\", \"fn_markid\"], how=\"left\")\n",
    "filtered_df = merged_df[(merged_df[\"fd_tgltrx\"] >= merged_df[\"fd_begindate\"]) & (merged_df[\"fd_tgltrx\"] <= merged_df[\"fd_enddate\"])]\n",
    "df_mgcatfin = filtered_df[['fn_idurut', 'fn_margin']]\n",
    "\n",
    "merged_df2 = pd.merge(df_trxartcat, df_mrgin, on=[\"fn_whconsiid\", \"fn_markid\"], how=\"left\")\n",
    "filtered_df2 = merged_df2[merged_df2[\"fd_tgltrx\"] >= merged_df2[\"fd_effectivedate\"]]\n",
    "df_mgfin = filtered_df2.loc[filtered_df2.groupby([\"fn_idurut\", \"fn_whconsiid\", \"fn_catid\", \"fn_markid\"])[\"fd_effectivedate\"].idxmax()]\n",
    "\n",
    "# Merge final margin data\n",
    "df_trx_final = df_trx[['fn_idurut']]\n",
    "df_trx_final = pd.merge(df_trx_final, df_mgcatfin, on=\"fn_idurut\", how=\"left\")\n",
    "df_trx_final = pd.merge(df_trx_final, df_mgfin[[\"fn_idurut\", 'fn_margin']], on=\"fn_idurut\", how=\"left\", suffixes=(\"\", \"_from_mgfin\"))\n",
    "df_trx_final[\"fn_margin\"] = df_trx_final[\"fn_margin\"].combine_first(df_trx_final[\"fn_margin_from_mgfin\"])\n",
    "df_trx_final.drop(columns=[\"fn_margin_from_mgfin\"], inplace=True, errors=\"ignore\")\n",
    "df_trx_final = df_trx_final.fillna(0)\n",
    "df_trx_final = df_trx_final[~df_trx_final.duplicated()]\n",
    "df_trx_final['fc_status'] = np.where(df_trx_final['fn_margin'] == 0, 'X', 'O')\n",
    "df_trx_final2= pd.merge(df_trx_final, df_trxartcat[['fn_whconsiid','fn_markid','fn_idurut']], on='fn_idurut', how=\"left\")\n",
    "df_trx_final3= df_mrgin.sort_values('fd_effectivedate', ascending=False)\n",
    "df_trx_final3= df_trx_final3.drop_duplicates(subset=['fn_whconsiid','fn_markid'], keep='first')\n",
    "df_trx_final3 = df_trx_final3.groupby('fn_whconsiid')['fn_margin'].agg(lambda x: x.mode().iloc[0]).reset_index()\n",
    "df_trx_final4= pd.merge(df_trx_final2, df_trx_final3, on=[\"fn_whconsiid\"], how=\"left\", suffixes=(\"\", \"_sim\"))\n",
    "df_trx_final4['fn_margin'] = df_trx_final4.apply(lambda row: row['fn_margin_sim'] if row['fn_margin'] == 0 else row['fn_margin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trx_final4.loc[df_trx_final4['fn_whconsiid']==186]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_sql(df_trx_final4, f'margin{str(year)[-2:]}_pb', 'imp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_consi.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['fv_nopenjualan'].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Praproses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11['tglinput']=pd.to_datetime(df11['tglinput'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung selisih hari\n",
    "df11['Day Input'] = (df11['tglinput'] - df11['tgltrx']).dt.days\n",
    "\n",
    "# Beri skor berdasarkan selisih hari\n",
    "def hitung_score(hari):\n",
    "    if hari <= 3:\n",
    "        return 'BAIK'\n",
    "    elif 4 <= hari <= 10:\n",
    "        return 'SEDANG'\n",
    "    else:\n",
    "        return 'BURUK'\n",
    "\n",
    "df11['Score'] = df11['Day Input'].apply(hitung_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11['fv_nobonlantai'] = \"'\" + df11['fv_nobonlantai']\n",
    "df11['fv_nobonlantai'] = df11['fv_nobonlantai'].str.replace(',', '', regex=False)\n",
    "df11['fv_nobonlantai'] = df11['fv_nobonlantai'].str.replace(\"â€™\", \"’\", regex=False)\n",
    "df11['fv_nobonlantai'] = df11['fv_nobonlantai'].str.replace(\"â\", \"\", regex=False)\n",
    "df11['desc']=df11['desc'].str.replace(',', '', regex=False)\n",
    "df11.loc[df11['fv_articlename']=='HIRT (CREW NECK) REGULAR','fv_articlename']= 'SHORT SLEEVE T-SHIRT (CREW NECK) REGULAR'\n",
    "df11['fn_margin']= df11['fn_margin'].fillna(0)\n",
    "df11['desc']= df11['desc'].fillna('None')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11.loc[df11['fv_nopenjualan']=='PJ/DPS/2503/00532/325',['fv_nobonlantai','artsize_affiliate','artsize_premi','artsize_servisfee']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['fv_nopenjualan']=='PJ/DPS/2502/00094/1128',['fv_artsizecode','fc_bonus','fn_jualpersize','total_penjualan2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Main proses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12= df11.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12.loc[df12['fv_nopenjualan']=='PJ/DPS/2503/00001/1143',['tglinput']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12=pd.merge(df12, df_cuspb[['fn_whconsiid','fc_code','fv_toko']], on='fn_whconsiid', how='left')\n",
    "df12 = pd.merge(df12, df_artpb[['fv_artsizecode', 'fv_barcode', 'fv_catname', 'fv_fitname', 'fv_brandname', 'fv_divname', 'fv_sizename','fm_price']], on='fv_artsizecode', how='left')\n",
    "df12 = pd.merge(df12, df_user[['fn_userid','fv_username']], on='fn_userid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12['event']=df12['event'].fillna(df12['status_artikel'])\n",
    "df12['event']=df12['event'].replace('DISC 50+20%','DISC 50%+20%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12 = pd.merge(df12, df_markdown[['fv_namemark','fm_amount']], left_on='event',right_on='fv_namemark', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12['fm_price2'] = np.where(df12['event'].str.contains('SP|NP'), df12['price'], df12['fm_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    df12['fc_bonus'].str.contains('T'),                    # Kondisi 1\n",
    "    df12['event'].str.contains('SP|NP'),                     # Kondisi 2\n",
    "    df12['event'].str.contains('DISC 50%\\\\+20%') ,                # Kondisi 3\n",
    "    df12['event'].str.contains('DISC 50%'),                    # Kondisi 4\n",
    "    df12['event'].str.contains('POTONGAN'),                    # Kondisi 5\n",
    "    df12['event'].str.contains('DISC 70%'),                    # Kondisi 6\n",
    "    df12['event'].str.contains('NORMAL')                        # Kondisi 7\n",
    "]\n",
    "choices_price = [\n",
    "    df12['fm_amount'],           # Jika kondisi 1 terpenuhi\n",
    "    df12['fm_amount'],  # Jika kondisi 2 terpenuhi\n",
    "    df12['fm_price'],        # Jika kondisi 3 terpenuhi\n",
    "    df12['fm_price'],        # Jika kondisi 4 terpenuhi\n",
    "    df12['price'],           # Jika kondisi 5 terpenuhi\n",
    "    df12['fm_price'],        # Jika kondisi 6 terpenuhi\n",
    "    df12['fm_price'],        # Jika kondisi 7 terpenuhi\n",
    "]\n",
    "choices_disc = [\n",
    "    df12['disc'],           # Jika kondisi 1 terpenuhi\n",
    "    df12['disc']*0,         # Jika kondisi 2 terpenuhi\n",
    "    df12['fm_price']*0.6,   # Jika kondisi 3 terpenuhi\n",
    "    df12['fm_price']*0.5,   # Jika kondisi 4 terpenuhi\n",
    "    df12['disc'],           # Jika kondisi 5 terpenuhi\n",
    "    df12['fm_price']*0.7,   # Jika kondisi 6 terpenuhi\n",
    "    df12['disc']*0,         # Jika kondisi 7 terpenuhi\n",
    "]\n",
    "# Else: pakai nilai default, misalnya fm_price\n",
    "df12['price'] = np.select(conditions, choices_price, default=df12['fm_price'])\n",
    "df12['disc'] = np.select(conditions, choices_disc, default=df12['disc'])\n",
    "df12['disc'] =round(df12['disc']).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12.loc[df12['fc_bonus']=='T',['event','fm_amount','price','fc_bonus']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12['Total Brutto'] = np.where(df12['event'].str.contains('SP|NP'), \n",
    "                                df12['fm_amount']*df12['fn_jualpersize'], \n",
    "                                df12['price']*df12['fn_jualpersize'])\n",
    "df12['Total Disc'] = df12['disc']*df12['fn_jualpersize']\n",
    "df12['Total Net Kassa']=df12['Total Brutto']- df12['Total Disc'] - df12['total_disc2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12['Margin Amount (pcs)'] = (df12['Total Net Kassa']/ df12['fn_jualpersize'] * df12['fn_margin'] / 100).round(0) \n",
    "df12['Margin Amount'] = (df12['Total Net Kassa'] * df12['fn_margin'] / 100).round(0)\n",
    "df12['Net Amount'] = df12['Total Net Kassa'] - df12['Margin Amount']\n",
    "df12['DPP'] = (df12['Net Amount'] / 1.11).round(2)\n",
    "df12['Ppn'] = (df12['Net Amount'] / 1.11 * 0.11) .round(2)\n",
    "df12.loc[df12['fn_grpcusid']==404,['DPP','Ppn']]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12['artsize_servisfee'] = df12['artsize_servisfee'] * df12['fn_jualpersize']\n",
    "df12['artsize_premi'] = df12['artsize_premi'] * df12['fn_jualpersize']\n",
    "df12['artsize_affiliate'] = df12['artsize_affiliate'] * df12['fn_jualpersize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12.loc[df12['fn_grpcusid']==404,'DPP+PPN']=df12['Total Net Kassa']-df12['artsize_voucher']\n",
    "df12['DPP+PPN'] = df12['DPP+PPN'].fillna(0)\n",
    "df12['DPP Online'] = np.where(df12['DPP+PPN'] == 0,0,((df12['DPP+PPN'] / 1.11)).round(2).fillna(0))\n",
    "df12['PPN Online'] = np.where(df12['DPP+PPN'] == 0,0,((df12['DPP+PPN'] / 1.11 * 0.11)).round(2).fillna(0))\n",
    "df12.loc[df12['fn_grpcusid']==404,'Net Amount'] = (df12['DPP+PPN'] - df12['Margin Amount'] - df12['artsize_servisfee']\n",
    "                                                - df12['artsize_premi'] - df12['artsize_affiliate']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12.loc[df12['fv_nopenjualan']=='PJ/DPS/2502/00391/1128',['event','fm_amount','DPP Online','PPN Online','DPP+PPN','Total Disc','total_disc2','Total Net Kassa']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Cleaning and Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df13=df12.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df13['fv_toko']=df13['fv_toko'].str.replace(',', '', regex=False)\n",
    "df13.loc[df13['fn_grpcusid']==415,'fv_toko']='VALINO OUTLET'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teks salah encoding\n",
    "text = \"937â€™934â€™941â€™943942â€™932â€™938â€™936\"\n",
    "\n",
    "# Perbaiki encoding: asumsikan teks dibaca sebagai latin-1, ubah ke utf-8\n",
    "# fixed_text = text.encode('latin1').decode('utf-8')\n",
    "clean_text = text.replace(\"â€™\", \"’\")\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df13=df13.rename(columns={\n",
    "    'fv_nopenjualan': 'No. Document',\n",
    "    'tgltrx': 'Sales Date',\n",
    "    # 'Day Input'\n",
    "    # 'Score'\n",
    "    'fc_code': 'Code Store',\n",
    "    'fv_toko': 'Store',\n",
    "    'fv_barcode': 'Code Article',\n",
    "    'fv_artsizecode': 'Article',\n",
    "    # 'fv_catname': 'Name Article',\n",
    "    # 'fv_fitname': 'Name Article',\n",
    "    'fv_articlename': 'Name Article',\n",
    "    'fv_brandname': 'Brand',\n",
    "    'fv_divname': 'Division',\n",
    "    'fv_sizename': 'Size',\n",
    "    'event': 'Status Article',\n",
    "    'fm_price': 'HET',\n",
    "    'price': 'Price',\n",
    "    'disc': 'Disc',\n",
    "    'fn_hargasatuan': 'Net Kassa',  \n",
    "    'fn_jualpersize': 'Qty',\n",
    "    'total_disc2':'Total Disc 2',\n",
    "    # 'total_penjualan2': 'Total Net Kassa',  \n",
    "    'fn_margin': 'Margin',\n",
    "    'EVENT': 'Event',\n",
    "    # 'Disc Name': 'Disc Name'\n",
    "    'tglinput': 'Input Date',\n",
    "    'fv_username': 'User Input',\n",
    "    'desc': 'Description',\n",
    "    'fn_tabel': 'Table',\n",
    "    'artsize_servisfee': 'Service Fee',\n",
    "    'artsize_premi': 'Premi',\n",
    "    'artsize_affiliate': 'Affiliate',\n",
    "    'artsize_voucher': 'Voucher',\n",
    "    'fv_nobonlantai': 'No. Bon Lantai',\n",
    "\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_df=['No. Document', 'Sales Date','Day Input','Score', 'Code Store', 'Store',\n",
    "        'Code Article', 'Article', 'Name Article', 'Brand', 'Division', 'Size','Status Article',\n",
    "        'HET', 'Price', 'Disc', 'Net Kassa', 'Qty','Total Brutto','Total Disc','Total Disc 2',\n",
    "        'Total Net Kassa', 'Margin', 'DPP+PPN', 'DPP Online', 'PPN Online',\n",
    "        'Margin Amount (pcs)','Margin Amount',\n",
    "        'Service Fee','Premi','Affiliate','Voucher',\n",
    "        'Net Amount','DPP','Ppn',\n",
    "        'Event','Disc Name', 'Input Date', 'User Input', 'Description', 'Table', 'No. Bon Lantai',]\n",
    "df13=df13[col_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df13.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consi2=df_consi.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consi.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consi2=df_consi.copy()\n",
    "# Misal: indeks kolom yang ingin dihapus\n",
    "index_to_drop = [0,27,43,39]\n",
    "\n",
    "# Ambil nama kolom berdasarkan index\n",
    "cols_to_drop = df_consi2.columns[index_to_drop]\n",
    "\n",
    "# Drop kolom tersebut\n",
    "df_consi2 = df_consi2.drop(columns=cols_to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 CHECKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df13.sort_values(['No. Document','Article'])\n",
    "df2=df_consi2\n",
    "df2['Article']=df2['Article'].str.upper()\n",
    "df2=df2.sort_values(['No. Document','Article'])\n",
    "df2=df2.rename(columns={'Margin Amount / pcs':'Margin Amount (pcs)'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Input Date'] = df1['Input Date'].dt.strftime('%#m/%#d/%Y %H:%M')\n",
    "df1['Input Date']=pd.to_datetime(df1['Input Date'], format='mixed', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Input Date']=pd.to_datetime(df2['Input Date'], format='mixed', errors='coerce')\n",
    "# df2.loc[df2['No. Document']=='PJ/DPS/2503/00001/1143','Input Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pastikan index dan kolomnya sama\n",
    "df1 = df1.reset_index(drop=True).fillna('None')\n",
    "\n",
    "df2 = df2.reset_index(drop=True).fillna('None')\n",
    "# df1['Sales Date'] = pd.to_datetime(df1['Sales Date']).dt.strftime('%-m/%-d/%Y')\n",
    "# df1['Input Date'] = pd.to_datetime(df1['Sales Date']).dt.strftime('%-m/%-d/%Y')\n",
    "\n",
    "if not df1.columns.equals(df2.columns):\n",
    "    print(\"⚠️ Kolom tidak sama\")\n",
    "    print(\"Kolom df1:\", df1.columns)\n",
    "    print(\"Kolom df2:\", df2.columns)\n",
    "\n",
    "# Buat DataFrame mask selisih nilai\n",
    "mask = df1 != df2\n",
    "\n",
    "# Tampilkan perbedaan (baris dan kolom)\n",
    "diff = []\n",
    "kolom_fix=mask.columns[(~mask).all()].tolist()\n",
    "for row in range(len(df1)):\n",
    "    for col in df1.columns:\n",
    "        if col in kolom_fix:\n",
    "            continue  # lewati kolom yang sudah fix\n",
    "        if mask.loc[row, col]:\n",
    "            diff.append({\n",
    "                \"index\": row,\n",
    "                \"kolom\": col,\n",
    "                \"df1\": df1.loc[row, col],\n",
    "                \"df2\": df2.loc[row, col]\n",
    "            })\n",
    "\n",
    "# Convert ke DataFrame\n",
    "df_diff = pd.DataFrame(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitung_selisih(row):\n",
    "    try:\n",
    "        # Coba konversi ke float\n",
    "        num1 = float(row['df1'])\n",
    "        num2 = float(row['df2'])\n",
    "        selisih = abs(num1 - num2)\n",
    "        status = \"OK\" if selisih < 0.1 else \"Not OK\"\n",
    "    except:\n",
    "        selisih = 'N/A'\n",
    "        status = \"OK\"\n",
    "    return pd.Series([selisih, status], index=[\"selisih\", \"status\"])\n",
    "\n",
    "# Tambahkan kolom 'selisih' dan 'status'\n",
    "# df_diff[['selisih', 'status']] = df_diff.apply(hitung_selisih, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_diff.loc[df_diff['status'] == 'Not OK', ['index', 'kolom', 'df1', 'df2', 'selisih']].sort_values(by='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.merge(df_diff,df2[['nomor','No. Document','Article']],how='left',left_on='index',right_on='nomor').to_excel('File Diff.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff.loc[df_diff['kolom']=='Net Kassa']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff['kolom'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kolom consi feb\n",
    "Name Article           532\n",
    "DPP Online             245 \n",
    "Net Amount             169\n",
    "Margin Amount (pcs)    165\n",
    "Margin Amount          164\n",
    "Margin                  78\n",
    "DPP                     78\n",
    "Ppn                     78\n",
    "Total Net Kassa          8\n",
    "DPP+PPN                  7\n",
    "Price                    4\n",
    "Size                     3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['nomor'] = range(len(df1))\n",
    "df2['nomor'] = range(len(df2))\n",
    "# Fungsi untuk mengubah ke float jika bisa, kalau tidak biarkan string\n",
    "def parse_number(val):\n",
    "    try:\n",
    "        # Cek apakah string hanya angka atau notasi ilmiah\n",
    "        return float(val)\n",
    "    except:\n",
    "        return val  # Biarkan tetap string\n",
    "\n",
    "# Terapkan ke kolom\n",
    "df2['Article'] = df2['Article'].apply(parse_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hapus spasi, newline, dan tab tersembunyi\n",
    "df1.columns = df1.columns.str.replace('\\n', '', regex=False).str.replace('\\t', '', regex=False).str.strip()\n",
    "df2.columns = df2.columns.str.replace('\\n', '', regex=False).str.replace('\\t', '', regex=False).str.strip()\n",
    "\n",
    "col_merge=['nomor','No. Document','Article','Status Article'\n",
    "        #    '\n",
    "        #    'Qty','HET','Price',\n",
    "        ]\n",
    "\n",
    "con_on=['Total Net Kassa','Total Brutto','Total Disc','Total Disc 2']\n",
    "col_cek = [fr'Premi']\n",
    "col_cek_x= [col + '_x' for col in col_cek]\n",
    "col_cek_y= [col + '_y' for col in col_cek]\n",
    "col_all =col_merge+col_cek+con_on\n",
    "df_check=pd.merge(df1[col_all],df2[col_all],how='inner',on=col_merge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_check=pd.merge(df1[col_all],df2[col_all],how='inner',on=col_merge)\n",
    "\n",
    "# Ambil daftar index unik dari df_diff\n",
    "# index_list = df_diff.loc[df_diff['status'] == 'Not OK','index'].unique()\n",
    "\n",
    "# Filter df1 berdasarkan index yang ada di index_list\n",
    "# df1_filtered = df1.loc[df1.index.isin(index_list)]\n",
    "# df2_filtered = df2.loc[df2.index.isin(index_list)]\n",
    "# df_checkall=pd.merge(df1_filtered, df2_filtered, how='outer', on=col_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_check[col_cek_x].values != df_check[col_cek_y].values).any(axis=1)\n",
    "df_beda=df_check[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1['Input Date'].to_excel('sec.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_beda= df_beda['nomor'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitung_selisih(row):\n",
    "    try:\n",
    "        # Coba konversi ke float\n",
    "        num1 = float(row[col_cek_x])\n",
    "        num2 = float(row[col_cek_y])\n",
    "        selisih = abs(num1 - num2)\n",
    "        status = \"OK\" if selisih < 0.1 else \"Not OK\"\n",
    "    except:\n",
    "        selisih = 'N/A'\n",
    "        status = \"OK\"\n",
    "    return pd.Series([selisih, status], index=[\"selisih\", \"status\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tambahkan kolom 'selisih' dan 'status'\n",
    "# df_beda['']\n",
    "# df_beda[['selisih', 'status']] = df_beda.apply(hitung_selisih, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beda=pd.merge(df_beda,df[['fv_nopenjualan','fn_grpcusid','fn_whconsiid']],how='left',left_on='No. Document',right_on='fv_nopenjualan').drop_duplicates(subset=['nomor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beda['fn_grpcusid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloc=((df_beda['fn_grpcusid'] != 2),'fn_grpcusid'  )\n",
    "df_beda.loc[dloc].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloc=((df_beda['fn_grpcusid'] == 2),'fn_whconsiid'  )\n",
    "df_beda.loc[dloc].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beda['fn_grpcusid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_beda.loc[((df_beda['status']!= 'OK') ,['fn_grpcusid'])].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_checkall.to_excel('consi.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.loc[df1['nomor'].isin(index_list_beda)].to_excel('dif1.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_beda.to_excel('diff.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.to_excel('consi feb.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AS3 max time input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.db_connection import get_engine, get_db\n",
    "from datetime import datetime, date\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_time_input(col_tgl_input,stat,table):\n",
    "    db=get_db(stat)\n",
    "    query = f'''\n",
    "    select max({col_tgl_input}) max_input from {db}.{table}\n",
    "    '''\n",
    "    with get_engine(stat) as engine:  # Menggunakan context manager dengan with\n",
    "        df = pd.read_sql(query, engine)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time_input('fd_tglinput','write','t_mutasimst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AS4 stock ots DO/DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.queries import (df__otsdo, df__otsdm, df__otsdo_rt, max_time_input, df__customers_pb, min_time_input,df__otsdo_del)\n",
    "from modules.proses import (save_to_sql,add_index,add_foreign_keys,add_primary_key,upsert_data)\n",
    "from modules.db_connection import get_engine, get_db\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_engine('write') as engine:\n",
    "    OutstandingDO_query = '''SELECT \n",
    "                fn_whconsiid ,fv_artsizecode, sum(fn_qty) OstDO\n",
    "                FROM gis_db.dodtl_tb a \n",
    "                inner join gis_db.domst_tb b on a.fc_nodoc=b.fc_nodoc\n",
    "                inner join gis_db.t_ordermst c on b.fc_noreff=fv_noorder\n",
    "                left join gis_db.t_receivingmst d on b.fc_nodoc=d.fc_nosj\n",
    "                where  fv_noreceiving is null and fd_date <= '{input_value}'and b.fc_status=\"F\" \n",
    "                group by fn_whconsiid, fv_artsizecode, fd_date '''\n",
    "    df= pd.read_sql(OutstandingDO_query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_list=['MD563']\n",
    "placeholders =  \", \".join([f\"'{s}'\" for s in store_list])\n",
    "# df1['kode_list'] = df1['kode'].str.split(',')\n",
    "\n",
    "# 2. Explode untuk meratakan\n",
    "# df1_exploded = df1.explode('kode_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df__customers_pb_n(placeholders):\n",
    "    query = f'''\n",
    "        SELECT fn_cusid,fn_whconsiid FROM gis_local.customers_pb\n",
    "        where fc_code in ({placeholders})\n",
    "        '''\n",
    "    with get_engine('imp') as engine:  # Menggunakan context manager dengan with\n",
    "        df = pd.read_sql(query, engine)\n",
    "    return df\n",
    "df_cus=df__customers_pb_n(placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_list= df_cus['fn_cusid'].tolist()\n",
    "cus_list =  \", \".join([f\"'{s}'\" for s in cus_list])\n",
    "whconsi_list=df_cus['fn_whconsiid'].tolist()\n",
    "whconsi_list =  \", \".join([f\"'{s}'\" for s in whconsi_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df__customers_pb_n(placeholders):\n",
    "    query = f'''\n",
    "        SELECT fn_cusid,fn_whconsiid FROM gis_local.customers_pb\n",
    "        where fc_code in ({placeholders})\n",
    "        '''\n",
    "    with get_engine('imp') as engine:  # Menggunakan context manager dengan with\n",
    "        df = pd.read_sql(query, engine)\n",
    "    return df\n",
    "df_cus=df__customers_pb_n(placeholders)\n",
    "cus_list= df_cus['fn_cusid'].tolist()\n",
    "cus_list =  \", \".join([f\"'{s}'\" for s in cus_list])\n",
    "def change_cusid_to_whconsiid(df):\n",
    "    df_cus=df__customers_pb('fn_cusid,fn_whconsiid')\n",
    "    df=pd.merge(df,df_cus,how='left',on='fn_cusid')\n",
    "    df['fn_cusid']=df['fn_whconsiid']\n",
    "    df=df.drop(columns='fn_whconsiid')\n",
    "    df=df.rename(columns={'fn_cusid':'fn_whconsiid'})\n",
    "    return df\n",
    "\n",
    "def get_ostdo_(cus_list):\n",
    "    query = f'''\n",
    "        SELECT fv_artsizecode,fv_barcode from artikel_pb\n",
    "        '''\n",
    "    with get_engine('imp') as engine:  # Menggunakan context manager dengan with\n",
    "        df_art = pd.read_sql(query, engine)\n",
    "    with get_engine('write') as engine:\n",
    "        query= f'''SELECT \n",
    "        c.fn_cusid ,fv_artsizecode, sum(fn_qty) fn_qty\n",
    "        FROM gis_db.dodtl_tb a \n",
    "        inner join gis_db.domst_tb b on a.fc_nodoc=b.fc_nodoc\n",
    "        inner join gis_db.t_ordermst c on b.fc_noreff=fv_noorder\n",
    "        left join gis_db.t_receivingmst d on b.fc_nodoc=d.fc_nosj\n",
    "        where  fv_noreceiving is null and c.fn_cusid in ({cus_list}) and b.fc_status=\"F\" \n",
    "        group by fn_cusid, fv_artsizecode, fd_date \n",
    "        '''\n",
    "        df= pd.read_sql(query, engine)\n",
    "        print(query)\n",
    "    df=change_cusid_to_whconsiid(df)\n",
    "    df=pd.merge(df, df_art, on='fv_artsizecode', how='left')\n",
    "    df=df.groupby(['fn_whconsiid','fv_barcode',], as_index=False)['fn_qty'].sum()\n",
    "    return df\n",
    "# print(query)\n",
    "df=get_ostdo_(cus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ostdm_(whconsi_list):\n",
    "    query = f'''\n",
    "        SELECT fv_artsizecode,fv_barcode from artikel_pb\n",
    "        '''\n",
    "    with get_engine('imp') as engine:  # Menggunakan context manager dengan with\n",
    "        df_art = pd.read_sql(query, engine)\n",
    "    with get_engine('write') as engine:\n",
    "        query= f'''\n",
    "        SELECT \n",
    "            fn_whconsiid2 fn_whconsiid,fv_artsizecode, sum(fn_qty) fn_qty\n",
    "            FROM gis_db.dodtl_tb a \n",
    "            inner join gis_db.domst_tb b on a.fc_nodoc=b.fc_nodoc\n",
    "            inner join gis_db.t_mutasimst c on b.fc_noreff=c.fv_nomutasi\n",
    "            left join gis_db.t_receivingmst d on b.fc_nodoc=d.fc_nosj\n",
    "            where  fv_noreceiving is null and fn_whconsiid2 in ({whconsi_list})\n",
    "            and b.fc_status=\"F\" \n",
    "            group by fn_whconsiid2, fv_artsizecode, fd_date \n",
    "        '''\n",
    "        df= pd.read_sql(query, engine)\n",
    "    # df=change_cusid_to_whconsiid(df)\n",
    "    df=pd.merge(df, df_art, on='fv_artsizecode', how='left')\n",
    "    df=df.groupby(['fn_whconsiid','fv_barcode',], as_index=False)['fn_qty'].sum()\n",
    "    return df\n",
    "df=get_ostdm_(cus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "        SELECT fv_artsizecode, fv_barcode from artikel_pb\n",
    "        '''\n",
    "with get_engine('imp') as engine:  # Menggunakan context manager dengan with\n",
    "    df_art = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OutstandingDM_df=pd.merge(OutstandingDM_df, df_art[['fv_artsizecode','fv_barcode']], on='fv_artsizecode', how='left')\n",
    "# OutstandingDM_df= OutstandingDM_df.groupby(['fn_whconsiid','fv_barcode',], as_index=False)['OstDM'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.merge(df, df_art[['fv_artsizecode','fv_barcode']], on='fv_artsizecode', how='left')\n",
    "# df=df.groupby(['fn_whconsiid','fv_barcode',], as_index=False)['fn_qty'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.groupby(['fn_whconsiid','fv_barcode',], as_index=False)['fn_qty'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_cusid_to_whconsiid(df):\n",
    "    df_cus=df__customers_pb('fn_cusid,fn_whconsiid')\n",
    "    df=pd.merge(df,df_cus,how='left',on='fn_cusid')\n",
    "    df['fn_cusid']=df['fn_whconsiid']\n",
    "    df=df.drop(columns='fn_whconsiid')\n",
    "    df=df.rename(columns={'fn_cusid':'fn_whconsiid'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_df = max_time_input('fd_dateinput','imp','dt_OSDO')\n",
    "max_input_str =max_input_df.iloc[0, 0]\n",
    "max_date = max_input_str.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "min_input_df = min_time_input('fd_dateinput','imp','dt_OSDO')\n",
    "min_input_str =min_input_df.iloc[0, 0]\n",
    "min_date = min_input_str.strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_del, df_main =df__otsdo_del(max_date,min_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_del['pk_id']=df_del['fc_nodoc'].astype(str)+\"/\"+df_del['fv_artsizecode'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.loc[~df_main['pk_id'].isin(df_del['pk_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=change_cusid_to_whconsiid(df)\n",
    "df[['fn_qty','fn_whconsiid']]=df[['fn_qty','fn_whconsiid']].fillna(0).astype(int)\n",
    "df['pk_id']=df['fc_nodoc'].astype(str)+\"/\"+df['fv_artsizecode'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df= df__otsdo()\n",
    "# df[['fn_qty','fn_whconsiid']]=df[['fn_qty','fn_whconsiid']].fillna(0).astype(int)\n",
    "# df['pk_id']=df['fc_nodoc'].astype(str)+\"/\"+df['fv_artsizecode'].astype(str)\n",
    "# engine='imp'\n",
    "# table_name='dt_OSDO'\n",
    "# col_pk='pk_id'\n",
    "# save_to_sql(df,table_name,engine)\n",
    "# add_index(engine,table_name,['fn_whconsiid','fv_artsizecode'])\n",
    "# add_primary_key(engine,table_name,col_pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_df = max_time_input('fd_dateinput','imp','dt_OSDO')\n",
    "max_input_str =max_input_df.iloc[0, 0]\n",
    "max_date = max_input_str.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# max_date= '2025-01-01'\n",
    "df=df__otsdo_rt(max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=change_cusid_to_whconsiid(df)\n",
    "df[['fn_qty','fn_whconsiid']]=df[['fn_qty','fn_whconsiid']].fillna(0).astype(int)\n",
    "df['pk_id']=df['fc_nodoc'].astype(str)+\"/\"+df['fv_artsizecode'].astype(str)\n",
    "engine='imp'\n",
    "table_name='dt_OSDO'\n",
    "col_pk='pk_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_data(engine,table_name,df,col_pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ini cukup:\n",
    "max_input_df = max_time_input('fd_dateinput','imp','dt_OSDO')\n",
    "max_date = max_input_df.iloc[0, 0]  # Sudah bertipe datetime.date\n",
    "\n",
    "print(max_date)\n",
    "print(type(max_date))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "if isinstance(max_date, datetime):\n",
    "    max_date = max_date.date()\n",
    "print(type(max_input_df.iloc[0, 0]))  # Bisa datetime.date atau datetime.datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AS5 freestock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_list=['MD255']\n",
    "placeholders =  \", \".join([f\"'{s}'\" for s in store_list])\n",
    "def df__customers_pb_n(placeholders):\n",
    "    query = f'''\n",
    "        SELECT fn_cusid,fn_whconsiid FROM gis_local.customers_pb\n",
    "        where fc_code in ({placeholders})\n",
    "        '''\n",
    "    # results = execute_raw_sql(query)\n",
    "    # df = pd.DataFrame(results)\n",
    "    with get_engine('imp') as engine:  # Menggunakan context manager dengan with\n",
    "        df = pd.read_sql(query, engine) \n",
    "    return df\n",
    "df_cus=df__customers_pb_n(placeholders)\n",
    "whconsi_list=df_cus['fn_whconsiid'].tolist()\n",
    "whconsi_list =  \", \".join([f\"'{s}'\" for s in whconsi_list])\n",
    "def get_freestok_(whconsi_list):\n",
    "    query = f'''\n",
    "        SELECT fn_whconsiid, fn_qtykirim,fn_qtyverified , fv_artsizecode, t.fc_status from t_returdtl_pre t\n",
    "        LEFT JOIN t_returmst_pre m on t.fv_noretur = m.fv_noretur\n",
    "        WHERE t.fc_status in ('T','X') and fn_whconsiid in ({whconsi_list})\n",
    "        '''\n",
    "    # results = DatabaseConnector.execute_raw_sql(query_1, using=StockDashboardAPIView.dbsave)\n",
    "    # df = pd.DataFrame(results)\n",
    "    with get_engine('write') as engine:  # Menggunakan context manager dengan with\n",
    "        df = pd.read_sql(query, engine) \n",
    "    df['stok_f']=np.where(df['fc_status'] == \"X\", df['fn_qtyverified'], df['fn_qtykirim'])  \n",
    "    df=df[['fn_whconsiid','stok_f','fv_artsizecode']]\n",
    "    return df\n",
    "df=get_freestok_(whconsi_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=get_freestok_(whconsi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AS5 Update Artikel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.queries import (df__penju, df__tokoawal, df__transaction, df__cus, get_monthly_sales_data, \n",
    "                             df__sales_thru_24, df__sales_thru_25, df__artsizecode, df__gdmd, fetch_data_margin,\n",
    "                             df__sales_, df__customers_pb, df__artikel_pb, df__artikel_pb_read, \n",
    "                             df__customers_pb_read, df__cus_user, df__sales_invoice,  df__sales_bz_disc2nd,\n",
    "                             df__sales_bz, df__sales_consi_disc2nd, df__sales_online_disc2nd, df__otsdm, df__otsdo,df__artikel_status ) \n",
    "from modules.proses import (save_to_sql, add_index_and_foreign_keys, clean_and_transform_data, \n",
    "                            insert_disc2_grouping, insert_margin, rename_col_py_to_sql, insert_pk,\n",
    "                            get_tgl, get_stock, get_stock_data, merge_and_calculate, merge_detail, \n",
    "                            generate_rename_dict,trx_rt, process_data_margin,rename_col_sql_to_xlsx_gis,\n",
    "                            get_artikel_md, get_new_or_changed_rows,upsert_data, process_data_cus,\n",
    "                            sls_disctok,add_ornal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artikel_md():\n",
    "    # Load artikel data\n",
    "    file_name = fr'\\\\192.168.1.254\\Share\\_From Valdi\\For MD\\FOLLOW UP 2023 (4) (5).xlsx'\n",
    "    df_nfa = pd.read_excel(file_name, sheet_name=8, header=5).dropna(how='all')\n",
    "    df_nfa= df_nfa [['Article Code','Artikel/ Detail 1']]\n",
    "    df_nfa=df_nfa.loc[~df_nfa['Article Code'].isnull()]\n",
    "    df_nfa= df_nfa.loc[~df_nfa['Artikel/ Detail 1'].isnull()]\n",
    "    df_nfa=df_nfa.rename(columns={'Article Code':'fv_barcode','Artikel/ Detail 1':'fv_neo_link_foto'})\n",
    "    df_nfa=df_nfa.loc[~df_nfa['fv_barcode'].duplicated()]   \n",
    "    return df_nfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df__artikel_pb_read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df__artikel_pb_read()\n",
    "df_nfa= get_artikel_md()\n",
    "df=pd.merge(df,df_nfa,on='fv_barcode',how='left')\n",
    "df=df.loc[~df['fv_artsizecode'].isnull()]\n",
    "df2=df__artikel_pb(\"*\")\n",
    "colist=df.columns.to_list()\n",
    "df[['fv_rating','fv_neo_link_foto']]=df[['fv_rating','fv_neo_link_foto']].fillna('none')\n",
    "df2[['fv_rating','fv_neo_link_foto','fv_ornal']]=df2[['fv_rating','fv_neo_link_foto','fv_ornal']].fillna('none')\n",
    "df['fv_ornal'] = df.apply(add_ornal, axis=1)\n",
    "df3=get_new_or_changed_rows(df, df2,subset=colist)\n",
    "df3=df3.fillna('none')\n",
    "df_art_status=df__artikel_status()\n",
    "df3=pd.merge(df3,df_art_status,on='fn_articleid',how='left')\n",
    "df3=df3[['fv_barcode',\n",
    "'fv_artsizecode',\n",
    "'fv_catname',\n",
    "'fv_brandname',\n",
    "'fv_divname',\n",
    "'fv_sizename',\n",
    "'fm_price',\n",
    "'fm_amount',\n",
    "'fv_namemark',\n",
    "'fv_colorname',\n",
    "'fv_fitname',\n",
    "'fv_tomname',\n",
    "'fv_configname',\n",
    "'motif',\n",
    "'fv_picture',\n",
    "'fv_rating',\n",
    "'fm_cogs',\n",
    "'fn_articleid',\n",
    "'fv_neo_link_foto',\n",
    "'fv_fitting',\n",
    "'fv_status',\n",
    "'fv_ornal']]\n",
    "table_name = 'artikel_pb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['fm_amount'] = pd.to_numeric(df3['fm_amount'], errors='coerce') \n",
    "df3['fm_amount'] = df3['fm_amount'].fillna(0)\n",
    "df3['fm_price'] = pd.to_numeric(df3['fm_price'], errors='coerce')  # jadi NaN kalau gak valid\n",
    "df3['fm_price'] = df3['fm_price'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df__artikel_pb(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2.duplicated(subset=[\"fv_artsizecode\"], keep=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cari baris yang punya spasi di awal, akhir, atau tengah\n",
    "mask_spasi = df2['fv_artsizecode'].str.contains(r\"\\s\", regex=True)\n",
    "\n",
    "# Tampilkan artikel bermasalah\n",
    "df_spasi = df2.loc[mask_spasi, ['fv_barcode','fv_artsizecode']]\n",
    "print(df_spasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= df__artikel_pb_read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df__artikel_pb(\"*\")\n",
    "# df['fv_ornal'] = df.apply(add_ornal, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = (\n",
    "    df.loc[\n",
    "        df['fv_fitting'].notna() & df['fv_status'].notna(),\n",
    "        ['fv_barcode', 'fv_fitting', 'fv_status']\n",
    "    ]\n",
    "    .drop_duplicates()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df1 dengan df3 di kolom 'fv_barcode'\n",
    "df_merge = df.merge(df3, on='fv_barcode', how='left', suffixes=('', '_df3'))\n",
    "\n",
    "# isi nilai null di df1 dengan dari df3\n",
    "df_merge['fv_fitting'] = df_merge['fv_fitting'].combine_first(df_merge['fv_fitting_df3'])\n",
    "df_merge['fv_status'] = df_merge['fv_status'].combine_first(df_merge['fv_status_df3'])\n",
    "\n",
    "# drop kolom tambahan dari df3\n",
    "df_merge = df_merge.drop(columns=['fv_fitting_df3', 'fv_status_df3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[ (df['fv_barcode']=='BABC032L111')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['fv_ornal'] = df.apply(add_ornal, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['fv_namemark']=='SP 299']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df[['fv_artsizecode','fv_namemark']].fillna('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df[['fv_artsizecode','fv_fitting','fv_status']].fillna('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'artikel_pb'\n",
    "for col in df3.columns:\n",
    "    if col != 'fv_artsizecode':\n",
    "        df_sub = df3[['fv_artsizecode', col]].copy()\n",
    "        upsert_data('imp', table_name, df_sub, primary_key='fv_artsizecode')\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_data('imp', table_name, df3, primary_key=\"fv_artsizecode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'artikel_pb'\n",
    "def df__artikel_status():\n",
    "    query='''select fn_articleid, fv_status from gis_db.articlestatus_tb'''\n",
    "    with get_engine('write') as engine:  # Menggunakan context manager dengan with\n",
    "        df = pd.read_sql(query, engine)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_art_status=df__artikel_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.merge(df3,df_art_status,on='fn_articleid',how='left')\n",
    "df3=df3[['fv_barcode',\n",
    " 'fv_artsizecode',\n",
    " 'fv_catname',\n",
    " 'fv_brandname',\n",
    " 'fv_divname',\n",
    " 'fv_sizename',\n",
    " 'fm_price',\n",
    " 'fm_amount',\n",
    " 'fv_namemark',\n",
    " 'fv_colorname',\n",
    " 'fv_fitname',\n",
    " 'fv_tomname',\n",
    " 'fv_configname',\n",
    " 'motif',\n",
    " 'fv_picture',\n",
    " 'fv_rating',\n",
    " 'fm_cogs',\n",
    " 'fn_articleid',\n",
    " 'fv_neo_link_foto',\n",
    " 'fv_fitting',\n",
    " 'fv_status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_art_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AS6 TARik stok mang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.Queries_API import (df__customers_pb_n,df__customers_pb,df_artikel_pb_,df__main,df__aging,df__sales,\n",
    "                                  df__retur,df__mutasi,df__obdo,change_cusid_to_whconsiid,df_ostdo_,df_ostdm_)\n",
    "from services.Process_API import (proc_add_aging, proc_pivot_table, proc_add_ornal, proc_sales_stock,\n",
    "                                   proc_add_retur, proc_add_mutasi, proc_add_obdo, proc_add_ostdo,\n",
    "                                   proc_add_ostdm, proc_add_stok_future, proc_add_cus_art, proc_selected_col_detail,\n",
    "                                   proc_add_summary, proc_cleaning_data_mobile_view,proc_selected_col_mobile_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_value = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "input_date = datetime.strptime(input_value, \"%Y-%m-%d\")\n",
    "# Konversi kembali ke string\n",
    "start=datetime.today()\n",
    "ends=start - relativedelta(months=3)\n",
    "# start_str = start.strftime(\"%Y-%m-%d\")\n",
    "# ends_str = ends.strftime(\"%Y-%m-%d\")\n",
    "alokator=''\n",
    "store_list = ['MD255']\n",
    "input_param={'alokator': alokator,\n",
    "'store_list': store_list,\n",
    "'start': start.strftime(\"%Y-%m-%d\"),\n",
    "'end': ends.strftime(\"%Y-%m-%d\"),\n",
    "'type': 'default',\n",
    "# 'format': output_format,\n",
    "'input_date': input_date.strftime(\"%Y-%m-%d\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df__main(input_param):\n",
    "    input_value = input_param['input_date']\n",
    "    store_list = input_param['store_list']\n",
    "    if store_list:\n",
    "        placeholders = \", \".join([f\"'{s}'\" for s in store_list])\n",
    "    df_cus = df__customers_pb_n(placeholders)\n",
    "    cus_list= df_cus['fn_cusid'].tolist()\n",
    "    cus_list =  \", \".join([f\"'{s}'\" for s in cus_list])\n",
    "    whconsi_list=df_cus['fn_whconsiid'].tolist()\n",
    "    whconsi_list =  \", \".join([f\"'{s}'\" for s in whconsi_list])\n",
    "    sales_thru = f\"sales_thru_pb_{input_value[2:4]}\"\n",
    "    trxstx = f\"trxstx_pb{input_value[2:4]}\"\n",
    "    stock = f\"fn_stock_{input_value[5:7]}\"\n",
    "\n",
    "    main_query = f\"\"\"\n",
    "    SELECT \n",
    "    stock_backdate.fn_whconsiid,\n",
    "    SUM(stock) AS stocks,\n",
    "    stock_backdate.fv_artsizecode, \n",
    "    fv_barcode, fv_sizename, fv_configname\n",
    "\n",
    "    FROM (\n",
    "    SELECT fn_whconsiid, {stock} AS stock, fv_artsizecode \n",
    "    FROM gis_local.{sales_thru}\n",
    "    UNION ALL\n",
    "    SELECT fn_whconsiid, fn_stock AS stock, fv_artsizecode \n",
    "    FROM gis_local.{trxstx} \n",
    "    WHERE fd_tgltrx BETWEEN '{input_value[0:4]}-{input_value[5:7]}-01' AND '{input_value}'\n",
    "    UNION ALL\n",
    "    SELECT fn_whconsiid, fn_stock AS stock, fv_artsizecode \n",
    "    FROM gis_local.trxrt_pb\n",
    "    UNION ALL\n",
    "    SELECT fn_whconsiid, fn_stock AS stock, fv_artsizecode \n",
    "    FROM gis_local.Stock_varian_25\n",
    "    WHERE fd_tgl BETWEEN '{input_value[0:4]}-{input_value[5:7]}-01' AND '{input_value}'\n",
    "    ) stock_backdate\n",
    "    left join artikel_pb b on stock_backdate.fv_artsizecode = b.fv_artsizecode\n",
    "    WHERE 1=1 and fn_whconsiid in ({whconsi_list})\n",
    "    GROUP BY fn_whconsiid, fv_artsizecode\n",
    "    \"\"\"\n",
    "    # df = pd.read_sql(main_query, engine_imp)\n",
    "    results = DatabaseConnector.execute_raw_sql(main_query, using=dbLoad)\n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detail(input_param):\n",
    "    col='fv_barcode'\n",
    "    type_table = 'detail'\n",
    "    type = input_param['type']\n",
    "    #Default API or Detail API\n",
    "    if type == 'default':\n",
    "        \n",
    "        df_main = df__main(input_param)\n",
    "        # print(f'test df_main')\n",
    "        df_aging = df__aging(input_param)\n",
    "        # print('test df_aging')\n",
    "        df_sales = df__sales(input_param)\n",
    "        # print('test df_sales')\n",
    "        df_retur = df__retur(input_param)\n",
    "        # print('test df_retur')\n",
    "        df_mutasi = df__mutasi(input_param)\n",
    "        # print('test df_mutasi')\n",
    "        df_obdo = df__obdo(input_param)\n",
    "        # print('test df_obdo')\n",
    "        df_ostdo = df_ostdo_(input_param,col)\n",
    "        # print('test df_ostdo')\n",
    "        df_ostdm=df_ostdm_(input_param,col)\n",
    "        # print('test df_ostdm')\n",
    "        df = proc_add_aging(df_main, df_aging)\n",
    "        # print('test proc_add_aging')\n",
    "        pivot_table = proc_pivot_table(df)\n",
    "        # print('test proc_pivot_table')\n",
    "        pivot_table = proc_add_ornal(pivot_table)\n",
    "        # print('test proc_add_ornal')\n",
    "        pivot_table = proc_sales_stock(pivot_table, df_sales)\n",
    "        # print('test proc_sales_stock')\n",
    "        pivot_table = proc_add_retur(pivot_table, df_retur)\n",
    "        # print('test df_osproc_add_returtdm')\n",
    "        pivot_table = proc_add_mutasi(pivot_table, df_mutasi)\n",
    "        # print('test proc_add_mutasi')\n",
    "        pivot_table = proc_add_obdo(pivot_table, df_obdo)\n",
    "        # print('test proc_add_obdo')\n",
    "        pivot_table = proc_add_ostdo(pivot_table, df_ostdo,col)\n",
    "        # print('test proc_add_ostdo')\n",
    "        pivot_table = proc_add_ostdm(pivot_table, df_ostdm,col)\n",
    "        # print('test proc_add_ostdm')\n",
    "        pivot_table = proc_add_stok_future(pivot_table)\n",
    "        # print('test proc_add_stok_future')\n",
    "        pivot_table = proc_add_cus_art(pivot_table,input_param,type_table,col)\n",
    "        # print('test proc_add_cus_art')\n",
    "        pivot_table = proc_selected_col_detail(pivot_table)\n",
    "        # pivot_table.info()\n",
    "        return pivot_table\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=get_detail(input_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AS7 broken 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sizes(row, size_columns):\n",
    "    # Ambil qty per size\n",
    "    size_qty = row[size_columns]\n",
    "    total = size_qty.sum()\n",
    "    if total == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Urutkan size dari qty terbesar\n",
    "    sorted_sizes = size_qty.sort_values(ascending=False)\n",
    "    \n",
    "    # Ambil size sampai kumulatif >= 50%\n",
    "    cum_sum = 0\n",
    "    selected = []\n",
    "    for size, qty in sorted_sizes.items():\n",
    "        cum_sum += qty\n",
    "        selected.append(size)\n",
    "        if cum_sum >= total * 0.5:\n",
    "            break\n",
    "    \n",
    "    return \", \".join(selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "date_now = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "date_last_3m = (datetime.today() - timedelta(days=90)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# periode tahun lalu\n",
    "date_now_last_year = (datetime.today().replace(year=datetime.today().year-1)).strftime(\"%Y-%m-%d\")\n",
    "date_3m_last_year = ((datetime.today() + timedelta(days=90)).replace(year=datetime.today().year-1)).strftime(\"%Y-%m-%d\")\n",
    "query = f'''\n",
    "        SELECT distinct fv_barcode,fv_catname\n",
    "        FROM gis_local.artikel_pb\n",
    "        '''\n",
    "with get_engine('imp') as engine:  # Menggunakan context manager dengan with\n",
    "    df_art = pd.read_sql(query, engine)\n",
    "query = f'''\n",
    "    SELECT \n",
    "    a.fv_artsizecode, \n",
    "    fn_cusid, \n",
    "    a.fn_whconsiid,\n",
    "    fv_barcode,\n",
    "    fv_sizename, \n",
    "    c.fv_configname,\n",
    "    sum(fn_jualpersize) qty, \n",
    "    sum(fn_totalpenjualan) net_casa, \n",
    "    fd_tgltrx\n",
    "    FROM gis_local.pb_penjualan_2025 a\n",
    "    left join gis_local.customers_pb b on a.fn_whconsiid=b.fn_whconsiid\n",
    "    left join gis_local.artikel_pb c on a.fv_artsizecode=c.fv_artsizecode\n",
    "    where fd_tgltrx between '{date_last_3m}' and '{date_now}'\n",
    "    group by fv_artsizecode, a.fn_whconsiid, fd_tgltrx;\n",
    "        '''\n",
    "with get_engine('imp') as engine:  # Menggunakan context manager dengan with\n",
    "    df_cons25 = pd.read_sql(query, engine)\n",
    "query = f'''\n",
    "    SELECT \n",
    "    a.fv_artsizecode, \n",
    "    fn_cusid, \n",
    "    a.fn_whconsiid,\n",
    "    fv_barcode,\n",
    "    fv_sizename, \n",
    "    c.fv_configname,\n",
    "    sum(fn_jualpersize) qty, \n",
    "    sum(fn_totalpenjualan) net_casa, \n",
    "    fd_tgltrx\n",
    "    FROM gis_local.pb_penjualan_2024 a\n",
    "    left join gis_local.customers_pb b on a.fn_whconsiid=b.fn_whconsiid\n",
    "    left join gis_local.artikel_pb c on a.fv_artsizecode=c.fv_artsizecode\n",
    "    where fd_tgltrx between '{date_now_last_year}' and '{date_3m_last_year}'\n",
    "    group by fv_artsizecode, a.fn_whconsiid, fd_tgltrx;\n",
    "        '''\n",
    "with get_engine('imp') as engine:  # Menggunakan context manager dengan with\n",
    "    df_cons24 = pd.read_sql(query, engine)\n",
    "\n",
    "df=df_cons24\n",
    "values:str='qty'\n",
    "col_index:list=['fv_barcode', 'fn_whconsiid']\n",
    "columns:list=['fv_sizename']\n",
    "col_ket:list=['fv_configname']\n",
    "    \n",
    "all_sizes_bottom = ['25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38',\n",
    "                    '39', '40', '42', '44', 'ALLSIZE']\n",
    "all_sizes_top = ['FS', 'XS', 'SS', 'S', 'M', 'L', 'XL', 'XXL', 'XXXL']\n",
    "all_sizes = all_sizes_bottom + all_sizes_top\n",
    "df_ket = df[col_index + col_ket].drop_duplicates()\n",
    "# Buat pivot table\n",
    "pivot_df = pd.pivot_table(\n",
    "    df,\n",
    "    values=values,\n",
    "    index=col_index,\n",
    "    columns=columns,\n",
    "    aggfunc='sum',\n",
    "    fill_value=0).reset_index()\n",
    "\n",
    "# Merge kembali kolom tambahan\n",
    "pivot_table = pd.merge(pivot_df, df_ket, on=col_index, how='left')\n",
    "for size in all_sizes:\n",
    "    if size not in pivot_table.columns:\n",
    "        pivot_table[size] = 0\n",
    "    pivot_table[size] = pd.to_numeric(pivot_table[size], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Atur ulang urutan: non-size columns dulu, lalu size columns\n",
    "size_columns = [size for size in all_sizes if size in pivot_table.columns]\n",
    "other_columns = [col for col in pivot_table.columns if col not in size_columns]\n",
    "pivot_table = pivot_table[other_columns + size_columns]\n",
    "\n",
    "# Hitung status (BROKEN / SEHAT)\n",
    "top_columns = [\"L\", \"M\", \"S\", \"XL\", \"XS\", \"XXL\", \"XXXL\"]\n",
    "bottom_columns = ['27', '28', '29', '30', '31', '32', '33', '34',\n",
    "                    '35', '36', '37', '38', '39', '40']\n",
    "\n",
    "pivot_table['status'] = np.where(\n",
    "    (pivot_table[\"fv_configname\"] == \"TOP\") & (pivot_table[top_columns].gt(0).sum(axis=1) < 3), \"BROKEN\",\n",
    "    np.where(\n",
    "        (pivot_table[\"fv_configname\"] == \"BOTTOM\") & (pivot_table[bottom_columns].gt(0).sum(axis=1) < 5), \"BROKEN\",\n",
    "        \"SEHAT\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Hitung total jumlah qty size (sumQty)\n",
    "pivot_table['sumQty'] = pivot_table[size_columns].sum(axis=1)\n",
    "pivot_table=pivot_table.drop(columns=['fv_configname'])\n",
    "\n",
    "df=pivot_table\n",
    "# size kecil = kolom 25–31 + kolom S + M\n",
    "df[\"size_kecil\"] = df['25'] + df['26'] + df[\"27\"] + df[\"28\"] + df['29'] + df['30'] + df[\"S\"] + df[\"M\"]\n",
    "\n",
    "# size sedang = kolom 30–34 + kolom M + L\n",
    "df[\"size_sedang\"] = df[\"34\"] + df[\"33\"] + df['32'] + df['30'] + df[\"31\"] + df[\"L\"] + df[\"M\"]\n",
    "\n",
    "# size besar = kolom 34 ke atas + kolom L, XL, XXL\n",
    "df[\"size_besar\"] = df[\"35\"] + df[\"36\"] + df['37'] + df['38'] + df[\"39\"] + df['40'] + df['42'] + df[\"44\"] + df[\"L\"] + df[\"XL\"] + df[\"XXL\"]\n",
    "df_detail=df\n",
    "df=df[['fv_barcode','fn_whconsiid','size_kecil','size_sedang','size_besar']]\n",
    "df=pd.merge(df,df_art, how='left',on='fv_barcode')\n",
    "import numpy as np\n",
    "\n",
    "def get_dominant_size(row):\n",
    "    k, s, b = row[\"size_kecil\"], row[\"size_sedang\"], row[\"size_besar\"]\n",
    "\n",
    "    # semua sama\n",
    "    if k == s == b:\n",
    "        return \"ksb\"\n",
    "    # dua sama besar\n",
    "    if k == s and k > b:\n",
    "        return \"ks\"\n",
    "    if k == b and k > s:\n",
    "        return \"kb\"\n",
    "    if s == b and s > k:\n",
    "        return \"sb\"\n",
    "    # salah satu paling besar\n",
    "    if k > s and k > b:\n",
    "        return \"k\"\n",
    "    if s > k and s > b:\n",
    "        return \"s\"\n",
    "    if b > k and b > s:\n",
    "        return \"b\"\n",
    "    return None  # fallback\n",
    "\n",
    "df[\"dominant_size\"] = df.apply(get_dominant_size, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "date_now = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "date_last_3m = (datetime.today() - timedelta(days=90)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# periode tahun lalu\n",
    "date_now_last_year = (datetime.today().replace(year=datetime.today().year-1)).strftime(\"%Y-%m-%d\")\n",
    "date_3m_last_year = ((datetime.today() + timedelta(days=90)).replace(year=datetime.today().year-1)).strftime(\"%Y-%m-%d\")\n",
    "query = f'''\n",
    "        SELECT distinct fv_barcode,fv_catname\n",
    "        FROM gis_local.artikel_pb\n",
    "        '''\n",
    "with get_engine('imp') as engine:  # Menggunakan context manager dengan with\n",
    "    df_art = pd.read_sql(query, engine)\n",
    "query = f'''\n",
    "    SELECT \n",
    "    a.fv_artsizecode, \n",
    "    fn_cusid, \n",
    "    a.fn_whconsiid,\n",
    "    fv_barcode,\n",
    "    fv_sizename, \n",
    "    c.fv_configname,\n",
    "    sum(fn_jualpersize) qty, \n",
    "    sum(fn_totalpenjualan) net_casa, \n",
    "    fd_tgltrx\n",
    "    FROM gis_local.pb_penjualan_2025 a\n",
    "    left join gis_local.customers_pb b on a.fn_whconsiid=b.fn_whconsiid\n",
    "    left join gis_local.artikel_pb c on a.fv_artsizecode=c.fv_artsizecode\n",
    "    where fd_tgltrx between '{date_last_3m}' and '{date_now}'\n",
    "    group by fv_artsizecode, a.fn_whconsiid, fd_tgltrx;\n",
    "        '''\n",
    "with get_engine('imp') as engine:  # Menggunakan context manager dengan with\n",
    "    df_cons25 = pd.read_sql(query, engine)\n",
    "query = f'''\n",
    "    SELECT \n",
    "    a.fv_artsizecode, \n",
    "    fn_cusid, \n",
    "    a.fn_whconsiid,\n",
    "    fv_catname,\n",
    "    fv_sizename, \n",
    "    c.fv_configname,\n",
    "    sum(fn_jualpersize) qty, \n",
    "    sum(fn_totalpenjualan) net_casa, \n",
    "    fd_tgltrx\n",
    "    FROM gis_local.pb_penjualan_2024 a\n",
    "    left join gis_local.customers_pb b on a.fn_whconsiid=b.fn_whconsiid\n",
    "    left join gis_local.artikel_pb c on a.fv_artsizecode=c.fv_artsizecode\n",
    "    where fd_tgltrx between '{date_now_last_year}' and '{date_3m_last_year}'\n",
    "    group by fv_artsizecode, a.fn_whconsiid, fd_tgltrx;\n",
    "        '''\n",
    "with get_engine('imp') as engine:  # Menggunakan context manager dengan with\n",
    "    df_cons24 = pd.read_sql(query, engine)\n",
    "\n",
    "df=df_cons24\n",
    "values:str='qty'\n",
    "col_index:list=['fv_catname', 'fn_whconsiid']\n",
    "columns:list=['fv_sizename']\n",
    "col_ket:list=['fv_configname']\n",
    "    \n",
    "all_sizes_bottom = ['25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38',\n",
    "                    '39', '40', '42', '44', 'ALLSIZE']\n",
    "all_sizes_top = ['FS', 'XS', 'SS', 'S', 'M', 'L', 'XL', 'XXL', 'XXXL']\n",
    "all_sizes = all_sizes_bottom + all_sizes_top\n",
    "df_ket = df[col_index + col_ket].drop_duplicates()\n",
    "# Buat pivot table\n",
    "pivot_df = pd.pivot_table(\n",
    "    df,\n",
    "    values=values,\n",
    "    index=col_index,\n",
    "    columns=columns,\n",
    "    aggfunc='sum',\n",
    "    fill_value=0).reset_index()\n",
    "\n",
    "# Merge kembali kolom tambahan\n",
    "pivot_table = pd.merge(pivot_df, df_ket, on=col_index, how='left')\n",
    "for size in all_sizes:\n",
    "    if size not in pivot_table.columns:\n",
    "        pivot_table[size] = 0\n",
    "    pivot_table[size] = pd.to_numeric(pivot_table[size], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Atur ulang urutan: non-size columns dulu, lalu size columns\n",
    "size_columns = [size for size in all_sizes if size in pivot_table.columns]\n",
    "other_columns = [col for col in pivot_table.columns if col not in size_columns]\n",
    "pivot_table = pivot_table[other_columns + size_columns]\n",
    "\n",
    "# Hitung status (BROKEN / SEHAT)\n",
    "top_columns = [\"L\", \"M\", \"S\", \"XL\", \"XS\", \"XXL\", \"XXXL\"]\n",
    "bottom_columns = ['27', '28', '29', '30', '31', '32', '33', '34',\n",
    "                    '35', '36', '37', '38', '39', '40']\n",
    "\n",
    "pivot_table['status'] = np.where(\n",
    "    (pivot_table[\"fv_configname\"] == \"TOP\") & (pivot_table[top_columns].gt(0).sum(axis=1) < 3), \"BROKEN\",\n",
    "    np.where(\n",
    "        (pivot_table[\"fv_configname\"] == \"BOTTOM\") & (pivot_table[bottom_columns].gt(0).sum(axis=1) < 5), \"BROKEN\",\n",
    "        \"SEHAT\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Hitung total jumlah qty size (sumQty)\n",
    "pivot_table['sumQty'] = pivot_table[size_columns].sum(axis=1)\n",
    "pivot_table=pivot_table.drop(columns=['fv_configname'])\n",
    "\n",
    "df=pivot_table\n",
    "# size kecil = kolom 25–31 + kolom S + M\n",
    "df[\"size_kecil\"] = df['25'] + df['26'] + df[\"27\"] + df[\"28\"] + df['29'] + df['30'] + df[\"S\"] + df[\"M\"]\n",
    "\n",
    "# size sedang = kolom 30–34 + kolom M + L\n",
    "df[\"size_sedang\"] = df[\"34\"] + df[\"33\"] + df['32'] + df['30'] + df[\"31\"] + df[\"L\"] + df[\"M\"]\n",
    "\n",
    "# size besar = kolom 34 ke atas + kolom L, XL, XXL\n",
    "df[\"size_besar\"] = df[\"35\"] + df[\"36\"] + df['37'] + df['38'] + df[\"39\"] + df['40'] + df['42'] + df[\"44\"] + df[\"L\"] + df[\"XL\"] + df[\"XXL\"]\n",
    "df_detail=df\n",
    "df=df[['fv_barcode','fn_whconsiid','size_kecil','size_sedang','size_besar']]\n",
    "df=pd.merge(df,df_art, how='left',on='fv_barcode')\n",
    "import numpy as np\n",
    "\n",
    "def get_dominant_size(row):\n",
    "    k, s, b = row[\"size_kecil\"], row[\"size_sedang\"], row[\"size_besar\"]\n",
    "\n",
    "    # semua sama\n",
    "    if k == s == b:\n",
    "        return \"ksb\"\n",
    "    # dua sama besar\n",
    "    if k == s and k > b:\n",
    "        return \"ks\"\n",
    "    if k == b and k > s:\n",
    "        return \"kb\"\n",
    "    if s == b and s > k:\n",
    "        return \"sb\"\n",
    "    # salah satu paling besar\n",
    "    if k > s and k > b:\n",
    "        return \"k\"\n",
    "    if s > k and s > b:\n",
    "        return \"s\"\n",
    "    if b > k and b > s:\n",
    "        return \"b\"\n",
    "    return None  # fallback\n",
    "\n",
    "df[\"dominant_size\"] = df.apply(get_dominant_size, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terapkan ke pivot_table\n",
    "df_detail[\"dominant_sizes\"] = df_detail.apply(lambda row: get_top_sizes(row, size_columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detail[df_detail.columns[20:]].sort_values('sumQty',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terapkan ke pivot_table\n",
    "pivot_table[\"dominant_sizes\"] = pivot_table.apply(lambda row: get_top_sizes(row, size_columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table.loc[pivot_table['dominant_sizes']=='XL, L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detail['dominant_sizes'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['fv_barcode','fn_whconsiid','size_kecil','size_sedang','size_besar']]\n",
    "df=pd.merge(df,df_art, how='left',on='fv_barcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terapkan ke pivot_table\n",
    "pivot_table[\"dominant_sizes\"] = pivot_table.apply(lambda row: get_top_sizes(row, size_columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_dom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_dom = (\n",
    "    df.groupby(['fn_whconsiid', 'fv_catname'], as_index=False)\n",
    "      .agg({\n",
    "          'size_kecil': 'sum',\n",
    "          'size_sedang': 'sum',\n",
    "          'size_besar': 'sum'\n",
    "      })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_dom[\"dominant_size\"] = df_cat_dom.apply(get_dominant_size, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_dom['dominant_size'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_dom.loc[(df_cat_dom['dominant_size']=='ksb'),'fv_catname'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['fn_whconsiid']==19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AS8 Update whtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.queries import (df__whtype_name) \n",
    "from modules.proses import (save_to_sql)\n",
    "\n",
    "from sqlalchemy import Integer, String, CHAR, DateTime, Text, text, Float, Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_sql_types(df):\n",
    "    \"\"\"Menentukan tipe data untuk SQL sesuai dengan tipe kolom.\"\"\"\n",
    "    sql_types = {}\n",
    "    for col in df.columns:\n",
    "        col_dtype = df[col].dtype\n",
    "\n",
    "        # Jika kolom tipe float\n",
    "        if col_dtype == 'float64':\n",
    "            sql_types[col] = Numeric(15, 2)\n",
    "        # Jika kolom diawali 'fn' dan bukan float\n",
    "        elif col.startswith('fn'):\n",
    "            sql_types[col] = Integer\n",
    "        # Jika kolom diawali 'fv'\n",
    "        elif col.startswith('fv'):\n",
    "            max_length = df[col].astype(str).map(len).max()\n",
    "            if max_length > 255:\n",
    "                sql_types[col] = Text\n",
    "            else:\n",
    "                sql_types[col] = String(255)\n",
    "        # Jika kolom diawali 'fc'\n",
    "        elif col.startswith('fc'):\n",
    "            sql_types[col] = CHAR(20)\n",
    "        # Jika kolom diawali 'fd'\n",
    "        elif col.startswith('fd'):\n",
    "            sql_types[col] = DateTime\n",
    "        # Kalau tidak masuk kriteria di atas\n",
    "        else:\n",
    "            sql_types[col] = String(50)\n",
    "    \n",
    "    return sql_types\n",
    "\n",
    "\n",
    "def save_to_sql(df, table_name, engine):\n",
    "    \"\"\"Menyimpan data ke dalam database menggunakan SQLAlchemy.\"\"\"\n",
    "    with get_engine(engine) as engine:\n",
    "        sql_types = define_sql_types(df)\n",
    "        df.to_sql(table_name, con=engine, if_exists='replace', index=False, dtype=sql_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df__whtype_name()\n",
    "nama_tabel='whtype'\n",
    "save_to_sql(df, nama_tabel, 'imp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AS9 Aging gudang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gr = '''\n",
    "    SELECT a.fc_nodoc, fd_arrivaldate, fv_nopo, fn_articleid, fn_qty FROM gis_db1.grdtl_tb a\n",
    "    inner join gis_db1.grmst_tb b on a.fc_nodoc = b.fc_nodoc\n",
    "    '''\n",
    "with get_engine('read') as engine:  # Menggunakan context manager dengan with\n",
    "    df_gr = pd.read_sql(query_gr, engine)\n",
    "# Cari tanggal dengan qty tertinggi per store & product\n",
    "result = df_gr.sort_values('fn_qty', ascending=False).groupby(['fv_nopo', 'fn_articleid']).first().reset_index()\n",
    "\n",
    "# Urutkan dari tanggal terbaru\n",
    "df_sorted = result.sort_values('fd_arrivaldate', ascending=False)\n",
    "\n",
    "# Ambil baris pertama (arrival terbaru) untuk tiap articleid\n",
    "df_latest = df_sorted.drop_duplicates(subset='fn_articleid', keep='first')\n",
    "\n",
    "query_gr = '''\n",
    "    SELECT * FROM gis_local.artikel_pb \n",
    "    '''\n",
    "with get_engine('imp') as engine:  # Menggunakan context manager dengan with\n",
    "    df_art = pd.read_sql(query_gr, engine)\n",
    "\n",
    "unique_ids = pd.DataFrame(df_art['fn_articleid'].unique(), columns=['fn_articleid'])\n",
    "df_latest = pd.merge(df_latest, unique_ids, on='fn_articleid', how='right')\n",
    "\n",
    "df_latest=df_latest.fillna({'fn_qty':0, 'fd_arrivaldate':'2023-01-01', 'fv_nopo':'', 'fc_nodoc':''})\n",
    "\n",
    "\n",
    "today = pd.Timestamp(datetime.now().date())\n",
    "df_latest[\"fn_hari\"] = (today - df_latest[\"fd_arrivaldate\"]).dt.days\n",
    "df_latest[\"fn_bulan\"] = (today.year - df_latest[\"fd_arrivaldate\"].dt.year) * 12 + (today.month - df_latest[\"fd_arrivaldate\"].dt.month)\n",
    "\n",
    "# df_latest.to_sql(f'aging_gudang_pb', engine_imp, if_exists='replace', index=False)\n",
    "\n",
    "nama_tabel='aging_gudang_pb'\n",
    "save_to_sql(df_latest, nama_tabel, 'imp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# troubel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.queries import (df__penju, df__tokoawal, df__transaction, df__cus, get_monthly_sales_data, \n",
    "                             df__sales_thru_24, df__sales_thru_25, df__artsizecode, df__gdmd, fetch_data_margin,\n",
    "                             df__sales_, df__customers_pb, df__artikel_pb, df__artikel_pb_read, \n",
    "                             df__customers_pb_read, df__cus_user, df__sales_invoice,  df__sales_bz_disc2nd,\n",
    "                             df__sales_bz, df__sales_consi_disc2nd, df__sales_online_disc2nd, df__otsdm, df__otsdo,\n",
    "                             df__artikel_status ) \n",
    "from modules.proses import (save_to_sql, add_index_and_foreign_keys, clean_and_transform_data, \n",
    "                            insert_disc2_grouping, insert_margin, rename_col_py_to_sql, insert_pk,\n",
    "                            get_tgl, get_stock, get_stock_data, merge_and_calculate, merge_detail, \n",
    "                            generate_rename_dict,trx_rt, process_data_margin,rename_col_sql_to_xlsx_gis,\n",
    "                            get_artikel_md, get_new_or_changed_rows,upsert_data, process_data_cus,\n",
    "                            sls_disctok)\n",
    "from modules.db_connection import get_engine, get_db\n",
    "from main import md_artikel_size2\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stock monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_monthly(tglEND,expot_excel):\n",
    "    '''Main Stock Monthly'''\n",
    "    tglEND2= datetime.strptime(tglEND, \"%Y-%m-%d\").date()\n",
    "    tglEND = datetime.strptime(tglEND, '%Y-%m-%d').strftime('%Y-%m-%d 23:59:59')\n",
    "    # Dapatkan tanggal kemarin\n",
    "    today = datetime.now()\n",
    "    d2ago = today - timedelta(days=2)\n",
    "    # Ubah ke string (format default: YYYY-MM-DD)\n",
    "    # kemarin_str = kemarin.strftime(\"%Y-%m-%d\")\n",
    "    # d2ago_str = d2ago.strftime(\"%Y-%m-%d\")\n",
    "    year = int(tglEND[:4])\n",
    "    month = int(tglEND[5:7])\n",
    "    day = int(tglEND[8:10])\n",
    "    last_day = calendar.monthrange(year, month)[1]\n",
    "    if day == last_day:\n",
    "        if month == 12:  \n",
    "            month = 1\n",
    "            year += 1\n",
    "        else:\n",
    "            month += 1\n",
    "    bulan = calendar.month_name[month].lower()[:3]\n",
    "    expot_excel = expot_excel.lower()\n",
    "    if expot_excel not in ['yes', 'no']:\n",
    "        raise ValueError(\"Parameter 'expot_excel' harus berupa 'yes' atau 'no'.\")\n",
    "    tglnow = datetime.today().date() # Tanggal Perhari Ini \n",
    "    df_tokoawal = df__tokoawal()\n",
    "    idwhtoko_list_awal = df_tokoawal['fn_whconsiid'].tolist() # Ubah kedalam List\n",
    "\n",
    "    # Mencari tanggal SO terakhir utnuk masing-masing toko yang masih buka per tanggal yang dipilih\n",
    "    df_tglLSO = pd.concat([get_tgl(idwhtoko, tglnow, tglEND) for idwhtoko in idwhtoko_list_awal])\n",
    "    df_tglLSO['fd_tglflagIndo'] = pd.to_datetime(df_tglLSO['fd_tglflagIndo'])\n",
    "\n",
    "    if tglEND2 > tglnow:  # Jika tanggal yang dipilih lebih besar dari hari ini\n",
    "        df_tglLSO.loc[df_tglLSO['fc_status']==\"T\",'fd_tglflagIndo'] -= pd.Timedelta(days=1)\n",
    "\n",
    "    df_tglLSO['fd_tglflagIndo'] = df_tglLSO['fd_tglflagIndo'].dt.strftime('%Y-%m-%d')\n",
    "    idwhtoko_list = df_tglLSO['fn_whconsiid'].tolist() # Ubah kedalam List untuk kode toko\n",
    "    tgl_list = df_tglLSO['fd_tglflagIndo'].tolist() # Ubah kedalam List untuk tanggal SO terakhir\n",
    "\n",
    "    # Mencari Stock pertanggal yang dipilih dengan menjumlah stock awal SO terakhir dengan transaksi dari SO terakhir sampai tanggal yang dipilih\n",
    "    df_tok = pd.DataFrame() # Dataframe Kosong\n",
    "    for idwhtoko, tglSTR in zip(idwhtoko_list, tgl_list): # Looping untuk masing-masing toko dan tanggal SO terakhir\n",
    "        df_stock = get_stock(idwhtoko, tglSTR, tglEND) # Mencari Stock awal\n",
    "        df_stock['fv_artsizecode'] = df_stock['fv_artsizecode'].str.upper()\n",
    "        df_stock['fv_artsizecode'] = df_stock['fv_artsizecode'].replace({'BBRD103V042333-': 'BBRD103V042333'})\n",
    "        df_transactions = df__transaction(idwhtoko, tglSTR, tglEND) # Mencari semua transaksi\n",
    "        df_transactions['fv_artsizecode'] = df_transactions['fv_artsizecode'].str.upper()\n",
    "        df_transactions['fv_artsizecode'] = df_transactions['fv_artsizecode'].replace({'BBRD103V042333-': 'BBRD103V042333'})\n",
    "        df_total = pd.concat([df_stock, df_transactions]).groupby('fv_artsizecode').sum().reset_index() # menjumlah stock awal dan transaksi\n",
    "        df_total['fn_whconsiid'] = idwhtoko \n",
    "        df_total['tgltrx'] = tglEND\n",
    "        \n",
    "        df_tok = pd.concat([df_tok, df_total], ignore_index=True)\n",
    "    fdate= str(year)[2:4]\n",
    "    df_tok=df_tok.loc[~((df_tok['fn_whconsiid'].isin([1131,1130,1129]))&(df_tok['qty']==0))]\n",
    "    df_tok= df_tok[['fn_whconsiid','fv_artsizecode','qty','tgltrx']].rename(columns={'qty':f'stock {bulan}_{fdate}'})\n",
    "    \n",
    "    if expot_excel == 'yes':\n",
    "        max_rows = 1048570\n",
    "        output_file = f'/opt/share/PB DB/pb_stock_{bulan}_{fdate}.xlsx'\n",
    "        print(len(df_tok))\n",
    "        print(max_rows)\n",
    "        if len(df_tok) <= max_rows:\n",
    "            # Simpan 1 sheet saja\n",
    "            df_tok.to_excel(output_file, index=False)\n",
    "        else:\n",
    "            # Potong menjadi 2 bagian\n",
    "            df1 = df_tok.iloc[:max_rows]\n",
    "            df2 = df_tok.iloc[max_rows:]\n",
    "            print(df1.info())\n",
    "            print(df2.info())\n",
    "            with pd.ExcelWriter(output_file) as writer:\n",
    "                df1.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "                df2.to_excel(writer, sheet_name='Sheet2', index=False)\n",
    "        print(f'pb_stock_{bulan}_{fdate} Selesai')\n",
    "        return df_tok\n",
    "    print(f'pb_stock_{bulan}_{fdate} Selesai')\n",
    "    return df_tok\n",
    "\n",
    "def get_last_three_months_including_current():\n",
    "    timestart = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"[INFO] Script get_last_three_months_including_current dimulai.\")\n",
    "    print(f\"[INFO] Timestart: {timestart}\\n\")\n",
    "\n",
    "    today = datetime.now()\n",
    "    last_dates = []\n",
    "\n",
    "    for i in range(2, -1, -1):  # Loop mundur dari 2 bulan lalu sampai bulan ini\n",
    "        target_month = (today.month - i - 1) % 12 + 1\n",
    "        target_year = today.year - ((today.month - i - 1) < 0)\n",
    "\n",
    "        # Dapatkan hari terakhir bulan tersebut\n",
    "        last_day = calendar.monthrange(target_year, target_month)[1]\n",
    "        last_dates.append(f\"{target_year}-{target_month:02d}-{last_day:02d}\")\n",
    "    for tgl in last_dates:\n",
    "        get_stock_monthly(tgl, 'yes')\n",
    "    # Print tanggal yang dipilih sebelum return\n",
    "    print(\"[INFO] Tanggal yang dipilih:\")\n",
    "    print(\"\\n\".join(last_dates))\n",
    "\n",
    "    return last_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_last_three_months_including_current()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tglEND='2025-08-31'\n",
    "expot_excel='yes'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Main Stock Monthly'''\n",
    "tglEND2= datetime.strptime(tglEND, \"%Y-%m-%d\").date()\n",
    "tglEND = datetime.strptime(tglEND, '%Y-%m-%d').strftime('%Y-%m-%d 23:59:59')\n",
    "# Dapatkan tanggal kemarin\n",
    "today = datetime.now()\n",
    "d2ago = today - timedelta(days=2)\n",
    "# Ubah ke string (format default: YYYY-MM-DD)\n",
    "# kemarin_str = kemarin.strftime(\"%Y-%m-%d\")\n",
    "# d2ago_str = d2ago.strftime(\"%Y-%m-%d\")\n",
    "year = int(tglEND[:4])\n",
    "month = int(tglEND[5:7])\n",
    "day = int(tglEND[8:10])\n",
    "last_day = calendar.monthrange(year, month)[1]\n",
    "if day == last_day:\n",
    "    if month == 12:  \n",
    "        month = 1\n",
    "        year += 1\n",
    "    else:\n",
    "        month += 1\n",
    "bulan = calendar.month_name[month].lower()[:3]\n",
    "expot_excel = expot_excel.lower()\n",
    "if expot_excel not in ['yes', 'no']:\n",
    "    raise ValueError(\"Parameter 'expot_excel' harus berupa 'yes' atau 'no'.\")\n",
    "tglnow = datetime.today().date() # Tanggal Perhari Ini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_tokoawal = df__tokoawal()\n",
    "idwhtoko_list_awal = df_tokoawal['fn_whconsiid'].tolist() # Ubah kedalam List\n",
    "\n",
    "# Mencari tanggal SO terakhir utnuk masing-masing toko yang masih buka per tanggal yang dipilih\n",
    "df_tglLSO = pd.concat([get_tgl(idwhtoko, tglnow, tglEND) for idwhtoko in idwhtoko_list_awal])\n",
    "df_tglLSO['fd_tglflagIndo'] = pd.to_datetime(df_tglLSO['fd_tglflagIndo'])\n",
    "\n",
    "if tglEND2 > tglnow:  # Jika tanggal yang dipilih lebih besar dari hari ini\n",
    "    df_tglLSO.loc[df_tglLSO['fc_status']==\"T\",'fd_tglflagIndo'] -= pd.Timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tglLSO['fd_tglflagIndo'] = df_tglLSO['fd_tglflagIndo'].dt.strftime('%Y-%m-%d')\n",
    "idwhtoko_list = df_tglLSO['fn_whconsiid'].tolist() # Ubah kedalam List untuk kode toko\n",
    "tgl_list = df_tglLSO['fd_tglflagIndo'].tolist() # Ubah kedalam List untuk tanggal SO terakhir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock(idwhtoko, tglSTR, tglEND):\n",
    "    df_stock = df__stock_so(idwhtoko, tglSTR, tglEND)\n",
    "    if df_stock['qty'].sum() == 0:\n",
    "        df_stock = df__stock_sa(idwhtoko, tglEND)\n",
    "    return df_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df__stock_so(idwhtoko, tglSTR, tglEND):\n",
    "    query = '''\n",
    "    select c.fv_artsizecode, IF(a.fn_qtyopname IS NULL, c.fn_qtyskrg, a.fn_qtyopname) AS qty\n",
    "    from gis_db1.t_historysimpanstokopnamecounter a\n",
    "    right join gis_db1.fl_stockarticleconsipostingmst b on a.fc_nodoc=b.fc_nodoc\n",
    "    right join gis_db1.fl_stockarticleconsipostingdtl c on c.fc_nodoc=b.fc_nodoc\n",
    "    where b.fn_whconsiid= '%s' AND SUBSTR(b.fd_tglflag, 1, 10) between %s AND %s\n",
    "    GROUP BY c.fv_artsizecode\n",
    "    '''\n",
    "    with get_engine('read') as engine:  # Menggunakan context manager dengan with\n",
    "                # Mencari tanggal Opname terakhir per tglnow\n",
    "        df = pd.read_sql(query, engine, params=(idwhtoko, tglSTR, tglEND))\n",
    "    return df\n",
    "\n",
    "def df__stock_sa(idwhtoko, tglEND):\n",
    "    query = '''\n",
    "    select a.fv_artsizecode, a.fn_qty qty\n",
    "    from saldoawalarticleconsi_tb a\n",
    "    where a.fn_whconsiid= '%s' AND SUBSTR(fd_tglinject, 1, 10) <= %s\n",
    "    '''\n",
    "    with get_engine('read') as engine:  # Menggunakan context manager dengan with\n",
    "                # Mencari tanggal Opname terakhir per tglnow\n",
    "        df = pd.read_sql(query, engine, params=(idwhtoko, tglEND))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mencari Stock pertanggal yang dipilih dengan menjumlah stock awal SO terakhir dengan transaksi dari SO terakhir sampai tanggal yang dipilih\n",
    "df_tok = pd.DataFrame() # Dataframe Kosong\n",
    "for idwhtoko, tglSTR in zip(idwhtoko_list, tgl_list): # Looping untuk masing-masing toko dan tanggal SO terakhir\n",
    "    df_stock = get_stock(idwhtoko, tglSTR, tglEND) # Mencari Stock awal\n",
    "    df_stock['fv_artsizecode'] = df_stock['fv_artsizecode'].str.upper()\n",
    "    df_stock['fv_artsizecode'] = df_stock['fv_artsizecode'].replace({'BBRD103V042333-': 'BBRD103V042333'})\n",
    "    df_transactions = df__transaction(idwhtoko, tglSTR, tglEND) # Mencari semua transaksi\n",
    "    df_transactions['fv_artsizecode'] = df_transactions['fv_artsizecode'].str.upper()\n",
    "    df_transactions['fv_artsizecode'] = df_transactions['fv_artsizecode'].replace({'BBRD103V042333-': 'BBRD103V042333'})\n",
    "    df_total = pd.concat([df_stock, df_transactions]).groupby('fv_artsizecode').sum().reset_index() # menjumlah stock awal dan transaksi\n",
    "    df_total['fn_whconsiid'] = idwhtoko \n",
    "    df_total['tgltrx'] = tglEND\n",
    "    \n",
    "    df_tok = pd.concat([df_tok, df_total], ignore_index=True)\n",
    "fdate= str(year)[2:4]\n",
    "df_tok=df_tok.loc[~((df_tok['fn_whconsiid'].isin([1131,1130,1129]))&(df_tok['qty']==0))]\n",
    "df_tok= df_tok[['fn_whconsiid','fv_artsizecode','qty','tgltrx']].rename(columns={'qty':f'stock {bulan}_{fdate}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "939298\n",
      "1048570\n",
      "pb_stock_sep_25 Selesai\n",
      "pb_stock_sep_25 Selesai\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if expot_excel == 'yes':\n",
    "    max_rows = 1048570\n",
    "    output_file = fr'\\\\192.168.1.254\\Share\\_From Valdi\\PB DB\\pb_stock_{bulan}_{fdate}.xlsx'\n",
    "    print(len(df_tok))\n",
    "    print(max_rows)\n",
    "    if len(df_tok) <= max_rows:\n",
    "        # Simpan 1 sheet saja\n",
    "        df_tok.to_excel(output_file, index=False)\n",
    "    else:\n",
    "        # Potong menjadi 2 bagian\n",
    "        df1 = df_tok.iloc[:max_rows]\n",
    "        df2 = df_tok.iloc[max_rows:]\n",
    "        print(df1.info())\n",
    "        print(df2.info())\n",
    "        with pd.ExcelWriter(output_file) as writer:\n",
    "            df1.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "            df2.to_excel(writer, sheet_name='Sheet2', index=False)\n",
    "    print(f'pb_stock_{bulan}_{fdate} Selesai')\n",
    "print(f'pb_stock_{bulan}_{fdate} Selesai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'placeholders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m----> 9\u001b[0m df_cus\u001b[38;5;241m=\u001b[39mdf__customers_pb_n(\u001b[43mplaceholders\u001b[49m)\n\u001b[0;32m     10\u001b[0m cus_list\u001b[38;5;241m=\u001b[39m df_cus[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfn_cusid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     11\u001b[0m cus_list \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m cus_list])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'placeholders' is not defined"
     ]
    }
   ],
   "source": [
    "def df__customers_pb_n(placeholders):\n",
    "        query = f'''\n",
    "            SELECT fn_cusid,fn_whconsiid FROM gis_local.customers_pb\n",
    "            where fc_code in ({placeholders})\n",
    "            '''\n",
    "        results = execute_raw_sql(query)\n",
    "        df = pd.DataFrame(results)\n",
    "        return df\n",
    "df_cus=df__customers_pb_n(placeholders)\n",
    "cus_list= df_cus['fn_cusid'].tolist()\n",
    "cus_list =  \", \".join([f\"'{s}'\" for s in cus_list])\n",
    "whconsi_list=df_cus['fn_whconsiid'].tolist()\n",
    "whconsi_list =  \", \".join([f\"'{s}'\" for s in whconsi_list])\n",
    "def change_cusid_to_whconsiid(df):\n",
    "    df_cus=df__customers_pb_n(placeholders)\n",
    "    df=pd.merge(df,df_cus,how='left',on='fn_cusid')\n",
    "    df['fn_cusid']=df['fn_whconsiid']\n",
    "    df=df.drop(columns='fn_whconsiid')\n",
    "    df=df.rename(columns={'fn_cusid':'fn_whconsiid'})\n",
    "    return df\n",
    "def get_ostdo_(cus_list):\n",
    "    query = f'''\n",
    "        SELECT fv_artsizecode,fv_barcode from artikel_pb\n",
    "        '''\n",
    "    results = execute_raw_sql(query)\n",
    "    df_art = pd.DataFrame(results)\n",
    "    query_1= f'''SELECT \n",
    "    c.fn_cusid ,fv_artsizecode, sum(fn_qty) fn_qty\n",
    "    FROM gis_db.dodtl_tb a \n",
    "    inner join gis_db.domst_tb b on a.fc_nodoc=b.fc_nodoc\n",
    "    inner join gis_db.t_ordermst c on b.fc_noreff=fv_noorder\n",
    "    left join gis_db.t_receivingmst d on b.fc_nodoc=d.fc_nosj\n",
    "    where  fv_noreceiving is null and c.fn_cusid in ({cus_list}) and b.fc_status=\"F\" \n",
    "    group by fn_cusid, fv_artsizecode, fd_date \n",
    "    '''\n",
    "    # DatabaseConnector.execute_raw_sql(OutstandingDO_query, using=StockDashboardAPIView.dbsave)\n",
    "    results_1 = DatabaseConnector.execute_raw_sql(query_1, using=StockDashboardAPIView.dbsave)\n",
    "    df = pd.DataFrame(results_1)\n",
    "    df=change_cusid_to_whconsiid(df)\n",
    "    df=pd.merge(df, df_art, on='fv_artsizecode', how='left')\n",
    "    df=df.groupby(['fn_whconsiid','fv_barcode',], as_index=False)['fn_qty'].sum()\n",
    "    return df\n",
    "OutstandingDO_df=get_ostdo_(cus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import date, timedelta\n",
    "import calendar\n",
    "from contextlib import contextmanager\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import Integer, String, CHAR, Date, Text, text, Float\n",
    "import os\n",
    "from datetime import datetime\n",
    "from modules.queries import (df__penju, df__tokoawal, df__transaction, df__cus, get_monthly_sales_data, \n",
    "                             df__sales_thru_24, df__sales_thru_25, df__artsizecode, df__gdmd, fetch_data_margin,\n",
    "                             df__sales_, df__customers_pb, df__artikel_pb, df__artikel_pb_read, \n",
    "                             df__customers_pb_read, df__cus_user) \n",
    "from modules.proses import (save_to_sql, add_index_and_foreign_keys, clean_and_transform_data, \n",
    "                            insert_disc2_grouping, insert_margin, rename_col_py_to_sql, insert_pk,\n",
    "                            get_tgl, get_stock, get_stock_data, merge_and_calculate, merge_detail, \n",
    "                            generate_rename_dict,trx_rt, process_data_margin,rename_col_sql_to_xlsx_gis,\n",
    "                            get_artikel_md, get_new_or_changed_rows,upsert_data, process_data_cus)\n",
    "# Proses\n",
    "from modules.queries import (df__tglso, df__tglsa, df__disc2, df__margin, df__artsizecode, df__stock_so,\n",
    "                             df__stock_sa, df__retur, df__size, generate_months_for_year, df__rtt)\n",
    "from modules.db_connection import get_engine\n",
    "from sqlalchemy import Integer, String, CHAR, Date, Text, text, Float\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sqlalchemy import MetaData\n",
    "from sqlalchemy.dialects.mysql import insert as mysql_insert\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import traceback\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (M) sales_to_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2023\n",
    "start_time2= datetime.now().strftime('%Y-%m-%d %H:%M:%S')    \n",
    "print(\"\\n[INFO] Script sales_to_sql mulai dijalankan.\")\n",
    "print(f\"[INFO] Timestamp: {start_time2}\\n\")\n",
    "\n",
    "df1= df__penju(year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df1.copy()\n",
    "df= clean_and_transform_data(df)\n",
    "# df=insert_disc2_grouping(df,'sum')\n",
    "# margin_tb(year)\n",
    "# df=insert_margin(df,year)\n",
    "# sum_col=['fn_jualpersize','total_penjualan2','total_disc2']\n",
    "# agg_col=['fn_whconsiid', 'fv_artsizecode', 'tgltrx']\n",
    "# fisrt_col=[col for col in df.columns if col not in sum_col + agg_col]\n",
    "# col_col=fisrt_col+[col for col in df.columns if col not in fisrt_col]\n",
    "# df=df[col_col]\n",
    "# df['event']=df['event'].fillna(df['status_artikel'])\n",
    "# df['fn_margin']=df['fn_margin'].fillna(0)\n",
    "# df=rename_col_py_to_sql(df).sort_values(by='fn_idurut')\n",
    "# df=insert_pk(df)\n",
    "# df['fn_nett_amount'] = (df['fn_totalpenjualan']-df['fn_total_disc2']) * (1-df['fn_margin']/100)\n",
    "# df['fn_sim_amount'] = np.where(df['fc_status'] == \"X\", df['fn_totalpenjualan'], df['fn_nett_amount'])\n",
    "# df['fn_dpp'] = df['fn_nett_amount']/1.11\n",
    "# df['fn_dpp2'] = df['fn_sim_amount']/1.11\n",
    "# table_name = f\"pb_penjualan_{year}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (P) insert_disc2_grouping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disc2= df__disc2()\n",
    "df = pd.merge(df, df_disc2, on='fv_nopenjualan', how='left')\n",
    "df = df.loc[~df.duplicated()]\n",
    "df['artsize_disc2nd'] = df['artsize_disc2nd'].fillna(0)\n",
    "df['total_disc2']=df['artsize_disc2nd']*df['fn_jualpersize']\n",
    "sum_col=['fn_jualpersize','total_penjualan2','total_disc2']\n",
    "agg_col=['fv_nopenjualan','fn_whconsiid', 'fv_artsizecode', 'tgltrx']\n",
    "fisrt_col=[col for col in df.columns if col not in sum_col + agg_col]\n",
    "print(f\" {sum_col}\")\n",
    "print(f\" {fisrt_col}\")\n",
    "\n",
    "# df = df.groupby(['fv_nopenjualan','fn_whconsiid', 'fv_artsizecode', 'tgltrx']).agg({\n",
    "#         **{col: 'sum' for col in sum_col} , **{col: 'first' for col in fisrt_col}\n",
    "#         }).reset_index()\n",
    "\n",
    "df = df.groupby(['fv_nopenjualan','fn_whconsiid', 'fv_artsizecode', 'tgltrx']).agg({\n",
    "        **{col: 'sum' for col in sum_col} , **{col: 'first' for col in fisrt_col}\n",
    "        }).reset_index()\n",
    "# if type == 'sum':\n",
    "#     df = df.groupby(['fv_nopenjualan','fn_whconsiid', 'fv_artsizecode', 'tgltrx']).agg({\n",
    "#         **{col: 'sum' for col in sum_col} , **{col: 'first' for col in fisrt_col}\n",
    "#         }).reset_index()\n",
    "# else: df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['fv_nopenjualan']=='PJ/DPS/2305/00014/77',\n",
    "         ['fv_artsizecode','fn_hargasatuan','fn_jualpersize','total_penjualan2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (M) upsert_customers_pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = fr'\\\\192.168.1.254\\Share\\_From Valdi\\PB DB\\\\'\n",
    "excel_path = f'{base_path}customers_pb_sup.xlsx'\n",
    "csv_path = f'{base_path}list user & employee.csv'\n",
    "pic_path = f'{base_path}LIST PIC MARKETING.xlsx'\n",
    "mic_path = f'{base_path}PIC MIC.xlsx'\n",
    "table_name = 'customers_pb'\n",
    "df = df__customers_pb_read()\n",
    "df_excel = pd.read_excel(excel_path)\n",
    "emp_tb=pd.read_csv(csv_path)\n",
    "emp_tb['fn_userid']=emp_tb['fn_userid'].astype(int)\n",
    "df_user=df__cus_user()\n",
    "df_user=pd.merge(df_user,emp_tb[['fn_userid','fv_nameemployees']],on='fn_userid',how='left')\n",
    "df_user['fn_cusid']=df_user['fn_cusid'].astype(int)\n",
    "df=pd.merge(df,df_user[['fn_cusid','fv_nameemployees']],on='fn_cusid',how='left')\n",
    "df=df.loc[~df['fn_whconsiid'].duplicated()]\n",
    "df=pd.merge(df, df_excel, on='fn_cusid', how='left')\n",
    "print(df.loc[df['fn_whconsiid']==\"-\",['fn_whconsiid']].value_counts())\n",
    "df_picmkt=pd.read_excel(pic_path, header=3).dropna(how='all')\n",
    "df_picmkt['SUPERVISOR/ OJT**'] = df_picmkt['SUPERVISOR/ OJT**'].str.replace(r'[^A-Za-z\\s]', '', regex=True).str.strip().str.title()\n",
    "df=pd.merge(df,df_picmkt[['MANAGER','SUPERVISOR/ OJT**','KODE TOKO GIS']],left_on='code',right_on='KODE TOKO GIS',how='left')\n",
    "print(df.loc[df['fn_whconsiid']==\"-\",['fn_whconsiid']].value_counts())\n",
    "df=df.rename(columns={'MANAGER_y':'MANAGER'})\n",
    "df['SUPERVISOR']=df['SUPERVISOR/ OJT**']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_picmic=pd.read_excel(mic_path,).dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.merge(df,df_picmic[['PIC MIC','STORE GIS']],left_on='code',right_on='STORE GIS',how='left').fillna('-')\n",
    "df['Alokator']=df['PIC MIC']\n",
    "df.loc[df['fn_cusid']==915,'fn_whconsiid']= 9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=['KODE TOKO GIS', 'MANAGER_x', 'SUPERVISOR/ OJT**','STORE GIS','PIC MIC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[df['fn_cusid']==915,].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is None or df.empty:\n",
    "        print(\"❌ DataFrame kosong! Upsert dibatalkan.\")\n",
    "else:\n",
    "    print(\"✅ Data berhasil di-load dengan kolom:\", df.columns.tolist())\n",
    "df = process_data_cus(df)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "upsert_data('imp',table_name,df, primary_key='customers_pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (M) stock and sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.queries import (df__penju, df__tokoawal, df__transaction, df__cus, get_monthly_sales_data, \n",
    "                             df__sales_thru_24, df__sales_thru_25, df__artsizecode, df__gdmd, fetch_data_margin,\n",
    "                             df__sales_, df__customers_pb, df__artikel_pb, df__artikel_pb_read, \n",
    "                             df__customers_pb_read, df__cus_user, df__sales_invoice,  df__sales_bz_disc2nd,\n",
    "                             df__sales_bz, df__sales_consi_disc2nd, df__sales_online_disc2nd, df__otsdm, df__otsdo,\n",
    "                             df__artikel_status ) \n",
    "from modules.proses import (save_to_sql, add_index_and_foreign_keys, clean_and_transform_data, \n",
    "                            insert_disc2_grouping, insert_margin, rename_col_py_to_sql, insert_pk,\n",
    "                            get_tgl, get_stock, get_stock_data, merge_and_calculate, merge_detail, \n",
    "                            generate_rename_dict,trx_rt, process_data_margin,rename_col_sql_to_xlsx_gis,\n",
    "                            get_artikel_md, get_new_or_changed_rows,upsert_data, process_data_cus,\n",
    "                            sls_disctok)\n",
    "from main import md_artikel_size2\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def md_artikel_size2():\n",
    "    df_artsq= df__artsizecode('imp')\n",
    "    df_sqq24= df__sales_thru_24()\n",
    "    df_sqq25= df__sales_thru_25()\n",
    "    df_sg= df__gdmd()\n",
    "\n",
    "    df_sq24=df_sqq24.copy()\n",
    "    df_sq25=df_sqq25.copy()\n",
    "    df_sq24=df_sq24.drop(columns=[df_sq24.columns[1]])\n",
    "    df_sq25=df_sq25.drop(columns=[df_sq25.columns[1]])\n",
    "    df_sq24['fn_artsizeid']=df_sq24['fn_artsizeid'].astype(str)\n",
    "    df_sq25['fn_artsizeid']=df_sq25['fn_artsizeid'].astype(str)\n",
    "    df_sq24 = df_sq24.groupby(['fv_artsizecode','fn_artsizeid'])[df_sq24.columns[2:]].sum().reset_index()\n",
    "    df_sq25 = df_sq25.groupby(['fv_artsizecode','fn_artsizeid'])[df_sq25.columns[2:]].sum().reset_index()\n",
    "\n",
    "    # Buat dictionary rename berdasarkan data\n",
    "    df_sq24 = generate_rename_dict(2024, 2024, df_sq24)\n",
    "    df_sq25 = generate_rename_dict(2025, 2025, df_sq25)\n",
    "    df=pd.merge(df_sq24,df_sq25,how='outer',on=['fn_artsizeid','fv_artsizecode'])\n",
    "    # df['qty retur']=df['qty retur_x'].fillna(0)+df['qty retur_y'].fillna(0)\n",
    "    print(df.columns)\n",
    "    df=df.rename(columns={\n",
    "        'qty retur_x': 'retur 2024',\n",
    "        'qty retur_y': 'retur 2025',})\n",
    "    df['stock akhir']=df['stock akhir_y']\n",
    "    df['total sales qty']=df['total sales qty_x'].fillna(0)+df['total sales qty_y'].fillna(0)\n",
    "    df['total stock']=df['total sales qty']+df['stock akhir'].fillna(0)\n",
    "    df=df.drop(columns=['total stock_x', 'stock akhir_x', 'stock akhir_y',\n",
    "                        'total stock_y','total sales qty_x','total sales qty_y'])\n",
    "    df_sg['fv_artsizecode']=df_sg['fv_artsizecode'].str.upper()\n",
    "    df = df.groupby(['fv_artsizecode'], as_index=False).sum()\n",
    "    df = pd.merge(df, df_sg, on='fv_artsizecode', how='outer')\n",
    "    df = pd.merge(df, df_artsq, on='fv_artsizecode', how='left')\n",
    "    df =generate_rename_dict(2025, 2025, df)\n",
    "    df = df.loc[~(df['artikel'].isnull())]\n",
    "    df2=df.fillna(0)\n",
    "    df2=df2[df2['artikel']!=0]\n",
    "    kolom_baru24 = ['artikel', 'size'] + [col for col in df2.columns if col not in ['artikel', 'size','fv_artsizecode','fn_artsizeid']]\n",
    "    df2 = df2[kolom_baru24]\n",
    "    df2r=df2.select_dtypes(include='number').sum()\n",
    "    df2r.to_excel(fr'\\\\192.168.1.254\\Share\\_From Valdi\\For MD\\artsize_gd_result.xlsx')\n",
    "    df2.to_excel(fr'\\\\192.168.1.254\\Share\\_From Valdi\\For MD\\stock artikel all 2025.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artsq= df__artsizecode('imp')\n",
    "df_sqq24= df__sales_thru_24()\n",
    "df_sqq25= df__sales_thru_25()\n",
    "df_sg= df__gdmd()\n",
    "\n",
    "df_sq24=df_sqq24.copy()\n",
    "df_sq25=df_sqq25.copy()\n",
    "df_sq24=df_sq24.drop(columns=[df_sq24.columns[1]])\n",
    "df_sq25=df_sq25.drop(columns=[df_sq25.columns[1]])\n",
    "df_sq24['fn_artsizeid']=df_sq24['fn_artsizeid'].astype(str)\n",
    "df_sq25['fn_artsizeid']=df_sq25['fn_artsizeid'].astype(str)\n",
    "df_sq24 = df_sq24.groupby(['fv_artsizecode','fn_artsizeid'])[df_sq24.columns[2:]].sum().reset_index()\n",
    "df_sq25 = df_sq25.groupby(['fv_artsizecode','fn_artsizeid'])[df_sq25.columns[2:]].sum().reset_index()\n",
    "\n",
    "# Buat dictionary rename berdasarkan data\n",
    "df_sq24 = generate_rename_dict(2024, 2024, df_sq24)\n",
    "df_sq25 = generate_rename_dict(2025, 2025, df_sq25)\n",
    "df=pd.merge(df_sq24,df_sq25,how='outer',on=['fn_artsizeid','fv_artsizecode'])\n",
    "# df['qty retur']=df['qty retur_x'].fillna(0)+df['qty retur_y'].fillna(0)\n",
    "print(df.columns)\n",
    "df=df.rename(columns={\n",
    "    'qty retur_x': 'retur 2024',\n",
    "    'qty retur_y': 'retur 2025',})\n",
    "df['stock akhir']=df['stock akhir_y']\n",
    "df['total sales qty']=df['total sales qty_x'].fillna(0)+df['total sales qty_y'].fillna(0)\n",
    "df['total stock']=df['total sales qty']+df['stock akhir'].fillna(0)\n",
    "df=df.drop(columns=['total stock_x', 'stock akhir_x', 'stock akhir_y',\n",
    "                    'total stock_y','total sales qty_x','total sales qty_y'])\n",
    "df_sg['fv_artsizecode']=df_sg['fv_artsizecode'].str.upper()\n",
    "df = df.groupby(['fv_artsizecode'], as_index=False).sum()\n",
    "df = pd.merge(df, df_sg, on='fv_artsizecode', how='outer')\n",
    "df = pd.merge(df, df_artsq, on='fv_artsizecode', how='left')\n",
    "df =generate_rename_dict(2025, 2025, df)\n",
    "df = df.loc[~(df['artikel'].isnull())]\n",
    "df2=df.fillna(0)\n",
    "df2=df2[df2['artikel']!=0]\n",
    "kolom_baru24 = ['artikel', 'size'] + [col for col in df2.columns if col not in ['artikel', 'size','fv_artsizecode','fn_artsizeid']]\n",
    "df2 = df2[kolom_baru24]\n",
    "df2r=df2.select_dtypes(include='number').sum()\n",
    "df2r.to_excel(fr'\\\\192.168.1.254\\Share\\_From Valdi\\For MD\\artsize_gd_result.xlsx')\n",
    "df2.to_excel(fr'\\\\192.168.1.254\\Share\\_From Valdi\\For MD\\stock artikel all 2025.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_artikel_size2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_monthly(tglEND,expot_excel):\n",
    "    '''Main Stock Monthly'''\n",
    "    tglEND2= datetime.strptime(tglEND, \"%Y-%m-%d\").date()\n",
    "    tglEND = datetime.strptime(tglEND, '%Y-%m-%d').strftime('%Y-%m-%d 23:59:59')\n",
    "    # Dapatkan tanggal kemarin\n",
    "    today = datetime.now()\n",
    "    d2ago = today - timedelta(days=2)\n",
    "    # Ubah ke string (format default: YYYY-MM-DD)\n",
    "    # kemarin_str = kemarin.strftime(\"%Y-%m-%d\")\n",
    "    # d2ago_str = d2ago.strftime(\"%Y-%m-%d\")\n",
    "    year = int(tglEND[:4])\n",
    "    month = int(tglEND[5:7])\n",
    "    day = int(tglEND[8:10])\n",
    "    last_day = calendar.monthrange(year, month)[1]\n",
    "    if day == last_day:\n",
    "        if month == 12:  \n",
    "            month = 1\n",
    "            year += 1\n",
    "        else:\n",
    "            month += 1\n",
    "    bulan = calendar.month_name[month].lower()[:3]\n",
    "    expot_excel = expot_excel.lower()\n",
    "    if expot_excel not in ['yes', 'no']:\n",
    "        raise ValueError(\"Parameter 'expot_excel' harus berupa 'yes' atau 'no'.\")\n",
    "    tglnow = datetime.today().date() # Tanggal Perhari Ini \n",
    "    df_tokoawal = df__tokoawal()\n",
    "    idwhtoko_list_awal = df_tokoawal['fn_whconsiid'].tolist() # Ubah kedalam List\n",
    "\n",
    "    # Mencari tanggal SO terakhir utnuk masing-masing toko yang masih buka per tanggal yang dipilih\n",
    "    df_tglLSO = pd.concat([get_tgl(idwhtoko, tglnow, tglEND) for idwhtoko in idwhtoko_list_awal])\n",
    "    df_tglLSO['fd_tglflagIndo'] = pd.to_datetime(df_tglLSO['fd_tglflagIndo'])\n",
    "\n",
    "    if tglEND2 > tglnow:  # Jika tanggal yang dipilih lebih besar dari hari ini\n",
    "        df_tglLSO.loc[df_tglLSO['fc_status']==\"T\",'fd_tglflagIndo'] -= pd.Timedelta(days=1)\n",
    "\n",
    "    df_tglLSO['fd_tglflagIndo'] = df_tglLSO['fd_tglflagIndo'].dt.strftime('%Y-%m-%d')\n",
    "    idwhtoko_list = df_tglLSO['fn_whconsiid'].tolist() # Ubah kedalam List untuk kode toko\n",
    "    tgl_list = df_tglLSO['fd_tglflagIndo'].tolist() # Ubah kedalam List untuk tanggal SO terakhir\n",
    "\n",
    "    # Mencari Stock pertanggal yang dipilih dengan menjumlah stock awal SO terakhir dengan transaksi dari SO terakhir sampai tanggal yang dipilih\n",
    "    df_tok = pd.DataFrame() # Dataframe Kosong\n",
    "    for idwhtoko, tglSTR in zip(idwhtoko_list, tgl_list): # Looping untuk masing-masing toko dan tanggal SO terakhir\n",
    "        df_stock = get_stock(idwhtoko, tglSTR, tglEND) # Mencari Stock awal\n",
    "        df_stock['fv_artsizecode'] = df_stock['fv_artsizecode'].str.upper()\n",
    "        df_stock['fv_artsizecode'] = df_stock['fv_artsizecode'].replace({'BBRD103V042333-': 'BBRD103V042333'})\n",
    "        df_transactions = df__transaction(idwhtoko, tglSTR, tglEND) # Mencari semua transaksi\n",
    "        df_transactions['fv_artsizecode'] = df_transactions['fv_artsizecode'].str.upper()\n",
    "        df_transactions['fv_artsizecode'] = df_transactions['fv_artsizecode'].replace({'BBRD103V042333-': 'BBRD103V042333'})\n",
    "        df_total = pd.concat([df_stock, df_transactions]).groupby('fv_artsizecode').sum().reset_index() # menjumlah stock awal dan transaksi\n",
    "        df_total['fn_whconsiid'] = idwhtoko \n",
    "        df_total['tgltrx'] = tglEND\n",
    "        \n",
    "        df_tok = pd.concat([df_tok, df_total], ignore_index=True)\n",
    "    fdate= str(year)[2:4]\n",
    "    df_tok=df_tok.loc[~((df_tok['fn_whconsiid'].isin([1131,1130,1129]))&(df_tok['qty']==0))]\n",
    "    df_tok= df_tok[['fn_whconsiid','fv_artsizecode','qty','tgltrx']].rename(columns={'qty':f'stock {bulan}_{fdate}'})\n",
    "    \n",
    "    if expot_excel == 'yes':\n",
    "        max_rows = 1048570\n",
    "        output_file = f'/opt/share/PB DB/pb_stock_{bulan}_{fdate}.xlsx'\n",
    "        print(len(df_tok))\n",
    "        print(max_rows)\n",
    "        if len(df_tok) <= max_rows:\n",
    "            # Simpan 1 sheet saja\n",
    "            df_tok.to_excel(output_file, index=False)\n",
    "        else:\n",
    "            # Potong menjadi 2 bagian\n",
    "            df1 = df_tok.iloc[:max_rows]\n",
    "            df2 = df_tok.iloc[max_rows:]\n",
    "            print(df1.info())\n",
    "            print(df2.info())\n",
    "            with pd.ExcelWriter(output_file) as writer:\n",
    "                df1.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "                df2.to_excel(writer, sheet_name='Sheet2', index=False)\n",
    "        print(f'pb_stock_{bulan}_{fdate} Selesai')\n",
    "        return df_tok\n",
    "    print(f'pb_stock_{bulan}_{fdate} Selesai')\n",
    "    return df_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestart = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"[INFO] Script get_last_three_months_including_current dimulai.\")\n",
    "print(f\"[INFO] Timestart: {timestart}\\n\")\n",
    "\n",
    "today = datetime.now()\n",
    "last_dates = []\n",
    "\n",
    "for i in range(2, -1, -1):  # Loop mundur dari 2 bulan lalu sampai bulan ini\n",
    "    target_month = (today.month - i - 1) % 12 + 1\n",
    "    target_year = today.year - ((today.month - i - 1) < 0)\n",
    "\n",
    "    # Dapatkan hari terakhir bulan tersebut\n",
    "    last_day = calendar.monthrange(target_year, target_month)[1]\n",
    "    last_dates.append(f\"{target_year}-{target_month:02d}-{last_day:02d}\")\n",
    "for tgl in last_dates:\n",
    "    get_stock_monthly(tgl, 'yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_dates = get_last_three_months_including_current()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_detail(final_data, stock_data, start_year, end_year):\n",
    "    df_articlesize = df__artsizecode('read')\n",
    "    df_size = df__size()\n",
    "\n",
    "    final_data = pd.merge(final_data, df_articlesize, on='fv_artsizecode', how='left')\n",
    "    final_data = pd.merge(final_data, df_size, on='fn_sizeid', how='left')\n",
    "    final_data = final_data[~final_data['fv_sizename'].isnull()]\n",
    "\n",
    "    # Membuat kolom secara dinamis untuk semua bulan di rentang tahun\n",
    "    columns = ['fn_artsizeid', 'fn_whconsiid', 'fv_artsizecode']\n",
    "    rename_columns = {'qty': 'fn_qty_retur'}\n",
    "    month_names = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "\n",
    "    current_year = datetime.now().year\n",
    "    current_month = datetime.now().month +1\n",
    "    # last_stock_col = None  # Variabel untuk menyimpan kolom stock terakhir\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        last_month = current_month if year == current_year else 12  # Sampai bulan saat ini di tahun berjalan\n",
    "\n",
    "        for month in range(1, last_month + 1):\n",
    "            month_str = f\"{month_names[month-1]}_{str(year)[-2:]}\"  # Format jan_25, feb_25, dst.\n",
    "            col_prefix = f'{month:02d}'\n",
    "            \n",
    "            stock_col = f'stock {month_str}'\n",
    "\n",
    "            # Hanya menyimpan stock bulan terakhir\n",
    "            if month == last_month:\n",
    "                last_stock_col = stock_col\n",
    "                columns.append(stock_col)\n",
    "                rename_columns[stock_col] = 'fn_stock_at'\n",
    "            else:\n",
    "                total_stock_col = f'total_stock_{month_str}'\n",
    "                qty_col = f'qty_{month_str}'\n",
    "                \n",
    "                columns.extend([stock_col, total_stock_col, qty_col])\n",
    "                rename_columns.update({\n",
    "                    qty_col: f'fn_qty_{col_prefix}',\n",
    "                    stock_col: f'fn_stock_{col_prefix}',\n",
    "                    total_stock_col: f'fn_ts_{col_prefix}'\n",
    "                })\n",
    "            \n",
    "            if month == 12:\n",
    "                columns[:-1]\n",
    "                available_stock_cols = [col for col in stock_data.columns if col.startswith('stock ')]\n",
    "\n",
    "                # Pilih kolom stok terakhir (sesuai urutan bulan)\n",
    "                if available_stock_cols:\n",
    "                    last_stock_col = (available_stock_cols)[-1]  # Ambil stok terakhir berdasarkan nama kolom\n",
    "                else:\n",
    "                    last_stock_col = None  # Jika tidak ada stok, gunakan None\n",
    "                \n",
    "                total_stock_dec = f'total_stock_dec_{str(year)[-2:]}'\n",
    "                qty_dec = f'qty_dec_{str(year)[-2:]}'\n",
    "                stock_dec=f'stock dec_{str(year)[-2:]}'\n",
    "                columns.extend([total_stock_dec, qty_dec,last_stock_col])\n",
    "                rename_columns.update({\n",
    "                    stock_dec:'fn_stock_12',\n",
    "                    total_stock_dec: 'fn_ts_12',\n",
    "                    qty_dec: 'fn_qty_12',\n",
    "                    last_stock_col:'fn_stock_at'\n",
    "                })\n",
    "    # additional condition\n",
    "    if tgl_today=tgl1diawalbulan:\n",
    "        columns.extend([])\n",
    "        columns.extend([f'fn_stock_12','qty', 'total_stock', 'total_qty'])\n",
    "        rename_columns.update({\n",
    "            'total_qty': 'fn_total_sales_qty',\n",
    "            'total_stock': 'fn_ts_all'})\n",
    "    else:\n",
    "        columns.extend([])\n",
    "        columns.extend(['qty', 'total_stock', 'total_qty'])\n",
    "        rename_columns.update({\n",
    "            'total_qty': 'fn_total_sales_qty',\n",
    "            'total_stock': 'fn_ts_all'\n",
    "        })\n",
    "    final_data = final_data[columns]\n",
    "    final_data = final_data.rename(columns=rename_columns).sort_values(by='fn_whconsiid')\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Script stock_and_sales mulai dijalankan.\n",
      "[INFO] Timestamp: 2025-10-01 09:49:48\n",
      "\n",
      "Base path: \\\\192.168.1.254\\Share\\_From Valdi\\PB DB\\\\\n",
      "True\n",
      "Daftar file yang dicek: ['\\\\\\\\192.168.1.254\\\\Share\\\\_From Valdi\\\\PB DB\\\\\\\\pb_stock_jan_25.xlsx', '\\\\\\\\192.168.1.254\\\\Share\\\\_From Valdi\\\\PB DB\\\\\\\\pb_stock_feb_25.xlsx', '\\\\\\\\192.168.1.254\\\\Share\\\\_From Valdi\\\\PB DB\\\\\\\\pb_stock_mar_25.xlsx', '\\\\\\\\192.168.1.254\\\\Share\\\\_From Valdi\\\\PB DB\\\\\\\\pb_stock_apr_25.xlsx', '\\\\\\\\192.168.1.254\\\\Share\\\\_From Valdi\\\\PB DB\\\\\\\\pb_stock_may_25.xlsx', '\\\\\\\\192.168.1.254\\\\Share\\\\_From Valdi\\\\PB DB\\\\\\\\pb_stock_jun_25.xlsx', '\\\\\\\\192.168.1.254\\\\Share\\\\_From Valdi\\\\PB DB\\\\\\\\pb_stock_jul_25.xlsx', '\\\\\\\\192.168.1.254\\\\Share\\\\_From Valdi\\\\PB DB\\\\\\\\pb_stock_aug_25.xlsx', '\\\\\\\\192.168.1.254\\\\Share\\\\_From Valdi\\\\PB DB\\\\\\\\pb_stock_sep_25.xlsx', '\\\\\\\\192.168.1.254\\\\Share\\\\_From Valdi\\\\PB DB\\\\\\\\pb_stock_oct_25.xlsx', '\\\\\\\\192.168.1.254\\\\Share\\\\_From Valdi\\\\PB DB\\\\\\\\pb_stock_nov_25.xlsx', '\\\\\\\\192.168.1.254\\\\Share\\\\_From Valdi\\\\PB DB\\\\\\\\pb_stock_dec_25.xlsx', '\\\\\\\\192.168.1.254\\\\Share\\\\_From Valdi\\\\PB DB\\\\\\\\pb_stock_jan_26.xlsx']\n",
      "✅ Membaca: \\\\192.168.1.254\\Share\\_From Valdi\\PB DB\\\\pb_stock_jan_25.xlsx\n",
      "✅ Membaca: \\\\192.168.1.254\\Share\\_From Valdi\\PB DB\\\\pb_stock_feb_25.xlsx\n",
      "✅ Membaca: \\\\192.168.1.254\\Share\\_From Valdi\\PB DB\\\\pb_stock_mar_25.xlsx\n",
      "✅ Membaca: \\\\192.168.1.254\\Share\\_From Valdi\\PB DB\\\\pb_stock_apr_25.xlsx\n",
      "✅ Membaca: \\\\192.168.1.254\\Share\\_From Valdi\\PB DB\\\\pb_stock_may_25.xlsx\n",
      "✅ Membaca: \\\\192.168.1.254\\Share\\_From Valdi\\PB DB\\\\pb_stock_jun_25.xlsx\n",
      "✅ Membaca: \\\\192.168.1.254\\Share\\_From Valdi\\PB DB\\\\pb_stock_jul_25.xlsx\n",
      "✅ Membaca: \\\\192.168.1.254\\Share\\_From Valdi\\PB DB\\\\pb_stock_aug_25.xlsx\n",
      "✅ Membaca: \\\\192.168.1.254\\Share\\_From Valdi\\PB DB\\\\pb_stock_sep_25.xlsx\n",
      "✅ Membaca: \\\\192.168.1.254\\Share\\_From Valdi\\PB DB\\\\pb_stock_oct_25.xlsx\n",
      "❌ File tidak ditemukan: \\\\192.168.1.254\\Share\\_From Valdi\\PB DB\\\\pb_stock_nov_25.xlsx\n",
      "❌ File tidak ditemukan: \\\\192.168.1.254\\Share\\_From Valdi\\PB DB\\\\pb_stock_dec_25.xlsx\n",
      "❌ File tidak ditemukan: \\\\192.168.1.254\\Share\\_From Valdi\\PB DB\\\\pb_stock_jan_26.xlsx\n",
      "Sebelum groupby: (9559368, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IT-ANALIS\\Documents\\airflow-DPS\\src\\modules\\proses.py:375: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  combined_data = combined_data.groupby(['fn_whconsiid', 'fv_artsizecode'], as_index=False).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setelah groupby: (1149058, 12)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1149058 entries, 0 to 1149057\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count    Dtype  \n",
      "---  ------          --------------    -----  \n",
      " 0   fn_whconsiid    1149058 non-null  int64  \n",
      " 1   fv_artsizecode  1149058 non-null  object \n",
      " 2   stock jan_25    1149058 non-null  float64\n",
      " 3   stock feb_25    1149058 non-null  float64\n",
      " 4   stock mar_25    1149058 non-null  float64\n",
      " 5   stock apr_25    1149058 non-null  float64\n",
      " 6   stock may_25    1149058 non-null  float64\n",
      " 7   stock jun_25    1149058 non-null  float64\n",
      " 8   stock jul_25    1149058 non-null  float64\n",
      " 9   stock aug_25    1149058 non-null  float64\n",
      " 10  stock sep_25    1149058 non-null  float64\n",
      " 11  stock oct_25    1149058 non-null  float64\n",
      "dtypes: float64(10), int64(1), object(1)\n",
      "memory usage: 105.2+ MB\n",
      "None\n",
      "stock_data prosess complite\n",
      "Index(['fn_whconsiid', 'fv_artsizecode', 'qty_jan_25', 'qty_feb_25',\n",
      "       'qty_mar_25', 'qty_apr_25', 'qty_may_25', 'qty_jun_25', 'qty_jul_25',\n",
      "       'qty_aug_25', 'qty_sep_25', 'qty_oct_25'],\n",
      "      dtype='object')\n",
      "Sales_data prosess complite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IT-ANALIS\\Documents\\airflow-DPS\\src\\modules\\queries.py:469: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  combined_sales = combined_sales.groupby(['fn_whconsiid', 'fv_artsizecode'], as_index=False).sum()\n"
     ]
    }
   ],
   "source": [
    "start_year=2025\n",
    "end_year=2025\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"\\n[INFO] Script stock_and_sales mulai dijalankan.\")\n",
    "print(f\"[INFO] Timestamp: {timestamp}\\n\")\n",
    "base_path = fr'\\\\192.168.1.254\\Share\\_From Valdi\\PB DB\\\\'\n",
    "print(\"Base path:\", base_path)\n",
    "print(os.path.exists(base_path)) \n",
    "\n",
    "stock_data_25 = get_stock_data(base_path, start_year, end_year)\n",
    "print(stock_data_25.info())\n",
    "print(\"stock_data prosess complite\")\n",
    "\n",
    "sales_data_25 = get_monthly_sales_data(start_year, end_year)\n",
    "print(sales_data_25.columns)\n",
    "print(\"Sales_data prosess complite\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data_25['qty_nov_25']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fn_whconsiid</th>\n",
       "      <th>fv_artsizecode</th>\n",
       "      <th>qty_jan_25</th>\n",
       "      <th>qty_feb_25</th>\n",
       "      <th>qty_mar_25</th>\n",
       "      <th>qty_apr_25</th>\n",
       "      <th>qty_may_25</th>\n",
       "      <th>qty_jun_25</th>\n",
       "      <th>qty_jul_25</th>\n",
       "      <th>qty_aug_25</th>\n",
       "      <th>qty_sep_25</th>\n",
       "      <th>qty_oct_25</th>\n",
       "      <th>qty_nov_25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AAAC096H0822L</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AAAC096H0822M</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>AAAC096H0822S</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>AAAC174M0323XXL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>AAAC287M0323L</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332216</th>\n",
       "      <td>1163</td>\n",
       "      <td>BABC352M1123M</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332217</th>\n",
       "      <td>1163</td>\n",
       "      <td>BBAC306M1123S</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332218</th>\n",
       "      <td>1163</td>\n",
       "      <td>GBRD138X112329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332219</th>\n",
       "      <td>1163</td>\n",
       "      <td>GBTC203T122431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332220</th>\n",
       "      <td>1163</td>\n",
       "      <td>TA01186TS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>332221 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fn_whconsiid   fv_artsizecode  qty_jan_25  qty_feb_25  qty_mar_25  \\\n",
       "0                  1    AAAC096H0822L         0.0         0.0         1.0   \n",
       "1                  1    AAAC096H0822M         0.0         0.0         1.0   \n",
       "2                  1    AAAC096H0822S         0.0         0.0         0.0   \n",
       "3                  1  AAAC174M0323XXL         0.0         0.0         1.0   \n",
       "4                  1    AAAC287M0323L         0.0         0.0         0.0   \n",
       "...              ...              ...         ...         ...         ...   \n",
       "332216          1163    BABC352M1123M         0.0         0.0         0.0   \n",
       "332217          1163    BBAC306M1123S         0.0         0.0         0.0   \n",
       "332218          1163   GBRD138X112329         0.0         0.0         0.0   \n",
       "332219          1163   GBTC203T122431         0.0         0.0         0.0   \n",
       "332220          1163        TA01186TS         0.0         0.0         0.0   \n",
       "\n",
       "        qty_apr_25  qty_may_25  qty_jun_25  qty_jul_25  qty_aug_25  \\\n",
       "0              0.0         0.0         0.0         0.0         0.0   \n",
       "1              0.0         0.0         0.0         0.0         0.0   \n",
       "2              0.0         0.0         0.0         1.0         0.0   \n",
       "3              0.0         0.0         0.0         0.0         0.0   \n",
       "4              0.0         0.0         0.0         2.0         0.0   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "332216         0.0         0.0         0.0         0.0         1.0   \n",
       "332217         0.0         0.0         0.0         0.0         1.0   \n",
       "332218         0.0         0.0         0.0         0.0         1.0   \n",
       "332219         0.0         0.0         0.0         0.0         2.0   \n",
       "332220         0.0         0.0         0.0         0.0         1.0   \n",
       "\n",
       "        qty_sep_25  qty_oct_25  qty_nov_25  \n",
       "0              0.0         0.0           0  \n",
       "1              0.0         0.0           0  \n",
       "2              0.0         0.0           0  \n",
       "3              0.0         0.0           0  \n",
       "4              0.0         0.0           0  \n",
       "...            ...         ...         ...  \n",
       "332216         0.0         0.0           0  \n",
       "332217         0.0         0.0           0  \n",
       "332218         0.0         0.0           0  \n",
       "332219         0.0         0.0           0  \n",
       "332220         0.0         0.0           0  \n",
       "\n",
       "[332221 rows x 13 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_data_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_25['stock nov_25']=stock_data_25['stock oct_25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_25=stock_data_25.drop(columns='qty_nov_25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_data prosess complite\n"
     ]
    }
   ],
   "source": [
    "# Menggabungkan dan menghitung\n",
    "final_data_25 = merge_and_calculate(sales_data_25, stock_data_25, start_year, end_year)\n",
    "print(\"final_data prosess complite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finale prosess complite 01-10-2025 10:00:44\n",
      "✅ Index 'idx_fn_whconsiid_fv_artsizecode' berhasil ditambahkan.\n",
      "✅ Foreign key 'fk_whconsiid_pb_sales_thru_2025' berhasil ditambahkan.\n",
      "✅ Foreign key 'fk_artsize_pb_sales_thru_2025' berhasil ditambahkan.\n",
      "\n",
      "[INFO] Script telah di input di gis.local.\n",
      "[INFO] Timestamp: 2025-10-01 10:01:53\n",
      "\n",
      "[INFO] Script telah berhasil dijalankan.\n",
      "[INFO] Timestamp: 2025-10-01 10:04:57\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "finale_25 = merge_detail(final_data_25, stock_data_25, start_year, end_year )\n",
    "tanggal_selesai = datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "print(f\"finale prosess complite {tanggal_selesai}\")\n",
    "\n",
    "nama_tabel=f'sales_thru_pb_{str(start_year)[-2:]}'\n",
    "# finale_25.to_sql(nama_tabel, engine2, if_exists='replace', index=False, dtype=sql_types)\n",
    "save_to_sql(finale_25, nama_tabel, 'imp')\n",
    "add_index_and_foreign_keys(\n",
    "'imp',\n",
    "nama_tabel,\n",
    "index_cols=[\"fn_whconsiid\", \"fv_artsizecode\"],\n",
    "foreign_keys=[\n",
    "    {\n",
    "        \"column\": \"fn_whconsiid\",\n",
    "        \"ref_table\": \"customers_pb\",\n",
    "        \"ref_column\": \"fn_whconsiid\",\n",
    "        \"constraint_name\": f\"fk_whconsiid_pb_sales_thru_{start_year}\"\n",
    "    },\n",
    "    {\n",
    "        \"column\": \"fv_artsizecode\",\n",
    "        \"ref_table\": \"artikel_pb\",\n",
    "        \"ref_column\": \"fv_artsizecode\",\n",
    "        \"constraint_name\": f\"fk_artsize_pb_sales_thru_{start_year}\"\n",
    "    }\n",
    "]\n",
    ")\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"\\n[INFO] Script telah di input di gis.local.\")\n",
    "print(f\"[INFO] Timestamp: {timestamp}\")\n",
    "    # Mendapatkan tanggal saat ini\n",
    "\n",
    "save_to_sql(finale_25, nama_tabel, 'read')\n",
    "# add_index_and_foreign_keys(nama_tabel, ['fn_whconsiid', 'fv_artsizecode'])\n",
    "\n",
    "tanggal_selesai = datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "    # Menambahkan pesan status berhasil\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"\\n[INFO] Script telah berhasil dijalankan.\")\n",
    "print(f\"[INFO] Timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finale_25.to_sql(nama_tabel, engine2, if_exists='replace', index=False, dtype=sql_types)\n",
    "save_to_sql(finale_25, nama_tabel, 'imp')\n",
    "add_index_and_foreign_keys(\n",
    "'imp',\n",
    "nama_tabel,\n",
    "index_cols=[\"fn_whconsiid\", \"fv_artsizecode\"],\n",
    "foreign_keys=[\n",
    "    {\n",
    "        \"column\": \"fn_whconsiid\",\n",
    "        \"ref_table\": \"customers_pb\",\n",
    "        \"ref_column\": \"fn_whconsiid\",\n",
    "        \"constraint_name\": f\"fk_whconsiid_pb_{start_year}\"\n",
    "    },\n",
    "    {\n",
    "        \"column\": \"fv_artsizecode\",\n",
    "        \"ref_table\": \"artikel_pb\",\n",
    "        \"ref_column\": \"fv_artsizecode\",\n",
    "        \"constraint_name\": f\"fk_artsize_pb_{start_year}\"\n",
    "    }\n",
    "]\n",
    ")\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"\\n[INFO] Script telah di input di gis.local.\")\n",
    "print(f\"[INFO] Timestamp: {timestamp}\")\n",
    "    # Mendapatkan tanggal saat ini\n",
    "\n",
    "save_to_sql(finale_25, nama_tabel, 'read')\n",
    "# add_index_and_foreign_keys(nama_tabel, ['fn_whconsiid', 'fv_artsizecode'])\n",
    "\n",
    "tanggal_selesai = datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "    # Menambahkan pesan status berhasil\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"\\n[INFO] Script telah berhasil dijalankan.\")\n",
    "print(f\"[INFO] Timestamp: {timestamp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_param={\n",
    "            # 'alokator': alokator,\n",
    "            'store_list': ['MD255'],\n",
    "            # 'start': start.strftime(\"%Y-%m-%d\"),\n",
    "            # 'end': ends.strftime(\"%Y-%m-%d\"),\n",
    "            # 'type': tipe,\n",
    "            # 'format': output_format,\n",
    "            # 'artikel':artikel,\n",
    "            # 'input_date': input_date.strftime(\"%Y-%m-%d\"),\n",
    "            # 'subid':subid,\n",
    "            # 's_all':s_all,\n",
    "            'divisi':'JEANS / YOUTH',\n",
    "            'ornal':'Normal',\n",
    "            # 'bulan':bulan,\n",
    "            # 'category':category\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df__customers_pb_n(placeholders):\n",
    "    query = f'''\n",
    "        SELECT fn_cusid,fn_whconsiid FROM gis_local.customers_pb\n",
    "        where fc_code in ({placeholders})\n",
    "        '''\n",
    "    with get_engine('imp') as engine:\n",
    "        df= pd.read_sql(query, engine)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 add whare artikel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_where(input_param,cus='',art=''):\n",
    "    if input_param['store_list'] is not None and input_param['divisi']!='':\n",
    "        if store_list:\n",
    "            placeholders = \", \".join([f\"'{s}'\" for s in store_list])\n",
    "            df_cus = df__customers_pb_n(placeholders)\n",
    "            whconsi_list = df_cus['fn_whconsiid'].tolist()\n",
    "            \n",
    "            if whconsi_list:\n",
    "                whconsi_placeholders = \", \".join([str(s) for s in whconsi_list])\n",
    "                whconsi = f\"and {cus}fn_whconsiid IN ({whconsi_placeholders})\"\n",
    "            else: whconsi = \"\"  # atau kondisi default kalau kosong\n",
    "    else: whconsi = \"\"  # jika input_param['store_list'] None\n",
    "\n",
    "    if input_param['divisi'] is not None and input_param['divisi']!='':\n",
    "        divisi = input_param['divisi']\n",
    "        divisi = f'and {art}fv_divname=\"{divisi}\"'\n",
    "    else: divisi = ''\n",
    "\n",
    "    if input_param['ornal'] is not None and input_param['divisi']!='':\n",
    "        ornal = input_param['ornal']\n",
    "        ornal = f'and {art}fv_ornal=\"{ornal}\"'\n",
    "    else: ornal = ''\n",
    "    return {\n",
    "        'whconsi':whconsi,\n",
    "        'divisi':divisi,\n",
    "        'ornal':ornal}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where=add_where(input_param,cus='cus.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input dan def global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.db_connection import get_engine\n",
    "input_value = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "input_date = datetime.strptime(input_value, \"%Y-%m-%d\")\n",
    "# Konversi kembali ke string\n",
    "start=datetime.today()\n",
    "ends=start - relativedelta(months=3)\n",
    "# start_str = start.strftime(\"%Y-%m-%d\")\n",
    "# ends_str = ends.strftime(\"%Y-%m-%d\")\n",
    "alokator=''\n",
    "store_list = ['MD255']\n",
    "input_param={'alokator': alokator,\n",
    "'store_list': store_list,\n",
    "'start': start.strftime(\"%Y-%m-%d\"),\n",
    "'end': ends.strftime(\"%Y-%m-%d\"),\n",
    "'type': 'default',\n",
    "# 'format': output_format,\n",
    "'input_date': input_date.strftime(\"%Y-%m-%d\"),\n",
    "'artikel':'QY02066W'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whconsi_list=168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df__customers_pb_n(placeholders):\n",
    "    query = f'''\n",
    "        SELECT fn_cusid,fn_whconsiid FROM gis_local.customers_pb\n",
    "        where fc_code in ({placeholders})\n",
    "        '''\n",
    "    with get_engine('imp') as engine:\n",
    "        df= pd.read_sql(query, engine)\n",
    "    # results = DatabaseConnector.execute_raw_sql(query, using=dbLoad)\n",
    "    # df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df__all_customers(col):\n",
    "    query = f'''\n",
    "            SELECT {col} FROM gis_local.customers_pb\n",
    "            '''\n",
    "    with get_engine('imp') as engine:\n",
    "        df= pd.read_sql(query, engine)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_artikel_pb_(colom):\n",
    "    query = f'''\n",
    "        SELECT {colom}\n",
    "        FROM gis_local.artikel_pb\n",
    "        '''\n",
    "    with get_engine('imp') as engine:\n",
    "        df= pd.read_sql(query, engine)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Q) df__req_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstore_list=f'and fn_whconsiid in ({whconsi_list})'\n",
    "query = f'''\n",
    "        SELECT fn_whconsiid, (CAST(fn_qtykirim AS SIGNED) * -1) as stocks, fv_artsizecode from t_returdtl_pre t\n",
    "        LEFT JOIN t_returmst_pre m on t.fv_noretur = m.fv_noretur\n",
    "        WHERE t.fc_status in ('T') {qstore_list}\n",
    "        '''\n",
    "\n",
    "with get_engine('write') as engine:\n",
    "        df_freestock= pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freestock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Q) API retur filter artikel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_value = input_param['input_date']\n",
    "alokator = input_param['alokator']\n",
    "artikel=input_param['artikel']\n",
    "query = f\"\"\"\n",
    "select a.fn_whconsiid, fv_barcode, sum(qty)*-1 as retur FROM gis_local.trx_retur a\n",
    "left join artikel_pb b on a.fv_artsizecode=b.fv_artsizecode \n",
    "left join customers_pb c on a.fn_whconsiid=c.fn_whconsiid\n",
    "WHERE tgltrx <= '{input_value}' and fv_barcode in ('{artikel}')\n",
    "group by fv_toko, fv_barcode\n",
    "\"\"\"\n",
    "with get_engine('imp') as engine:\n",
    "        df_retur= pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Q) df__main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_value = input_param['input_date']\n",
    "store_list = input_param['store_list']\n",
    "if store_list:\n",
    "    placeholders = \", \".join([f\"'{s}'\" for s in store_list])\n",
    "df_cus = df__customers_pb_n(placeholders)\n",
    "cus_list= df_cus['fn_cusid'].tolist()\n",
    "cus_list =  \", \".join([f\"'{s}'\" for s in cus_list])\n",
    "whconsi_list=df_cus['fn_whconsiid'].tolist()\n",
    "whconsi_list =  \", \".join([f\"'{s}'\" for s in whconsi_list])\n",
    "sales_thru = f\"sales_thru_pb_{input_value[2:4]}\"\n",
    "trxstx = f\"trxstx_pb{input_value[2:4]}\"\n",
    "stock = f\"fn_stock_{input_value[5:7]}\"\n",
    "\n",
    "input_value = '2025-07-10'\n",
    "whconsi_list=168\n",
    "\n",
    "main_query_ = f\"\"\"\n",
    "SELECT \n",
    "stock_backdate.fn_whconsiid,\n",
    "SUM(stock) AS stocks,\n",
    "stock_backdate.fv_artsizecode, \n",
    "fv_barcode, fv_sizename, fv_configname\n",
    "\n",
    "FROM (\n",
    "SELECT fn_whconsiid, {stock} AS stock, fv_artsizecode \n",
    "FROM gis_local.{sales_thru}\n",
    "UNION ALL\n",
    "SELECT fn_whconsiid, fn_stock AS stock, fv_artsizecode \n",
    "FROM gis_local.{trxstx} \n",
    "WHERE fd_tgltrx BETWEEN '{input_value[0:4]}-{input_value[5:7]}-01' AND '{input_value}'\n",
    "UNION ALL\n",
    "SELECT fn_whconsiid, fn_stock AS stock, fv_artsizecode \n",
    "FROM gis_local.trxrt_pb\n",
    "WHERE fd_tgltrx BETWEEN '{input_value[0:4]}-{input_value[5:7]}-01' AND '{input_value}'\n",
    "UNION ALL\n",
    "SELECT fn_whconsiid, fn_stock AS stock, fv_artsizecode \n",
    "FROM gis_local.Stock_varian_25\n",
    "WHERE fd_tgl BETWEEN '{input_value[0:4]}-{input_value[5:7]}-01' AND '{input_value}'\n",
    ") stock_backdate\n",
    "left join artikel_pb b on stock_backdate.fv_artsizecode = b.fv_artsizecode\n",
    "WHERE 1=1 and fn_whconsiid in ({whconsi_list})\n",
    "GROUP BY fn_whconsiid, fv_artsizecode\n",
    "\"\"\"\n",
    "with get_engine('imp') as engine:\n",
    "    df_main= pd.read_sql(main_query_, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Q) df__trx_freestock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_value = '2025-07-11'\n",
    "input_now = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "# input_value = input_param['input_date']\n",
    "trxstx = f\"trxstx_pb{input_value[2:4]}\"\n",
    "# artikel=input_param['artikel']\n",
    "artikel=None\n",
    "if artikel==None:\n",
    "    store_list = input_param['store_list']\n",
    "    if store_list:\n",
    "        placeholders = \", \".join([f\"'{s}'\" for s in store_list])\n",
    "    df_cus = df__customers_pb_n(placeholders)\n",
    "    cus_list= df_cus['fn_cusid'].tolist()\n",
    "    cus_list =  \", \".join([f\"'{s}'\" for s in cus_list])\n",
    "    # whconsi_list=df_cus['fn_whconsiid'].tolist()\n",
    "    # whconsi_list =  \", \".join([f\"'{s}'\" for s in whconsi_list])\n",
    "    qstore_list= f'and b.fn_whconsiid in ({whconsi_list})'\n",
    "else:\n",
    "    qstore_list= f'and fv_barcode in (\"{artikel}\")'\n",
    "main_query = f\"\"\"\n",
    "SELECT \n",
    "    stock_backdate.fn_whconsiid,\n",
    "    SUM(stock) AS stocks,\n",
    "    stock_backdate.fv_artsizecode\n",
    "FROM (\n",
    "    SELECT fn_whconsiid, fn_stock AS stock, fv_artsizecode , fv_stat stat\n",
    "    FROM gis_local.{trxstx} \n",
    "    WHERE fd_tgltrx BETWEEN '{input_value}' AND '{input_now}'\n",
    "    UNION ALL\n",
    "    SELECT fn_whconsiid, fn_stock AS stock, fv_artsizecode , 'txrt' stat\n",
    "    FROM gis_local.trxrt_pb \n",
    "    WHERE fd_tgltrx BETWEEN '{input_value}' AND '{input_now}'\n",
    "    UNION ALL\n",
    "    SELECT fn_whconsiid, fv_freestock AS stock, fv_artsizecode , 'var' stat\n",
    "    FROM gis_local.Freestock_varian_25\n",
    "    WHERE fd_tgl BETWEEN '{input_value}' AND '{input_now}'\n",
    ") stock_backdate\n",
    "LEFT JOIN artikel_pb a ON a.fv_artsizecode = stock_backdate.fv_artsizecode\n",
    "LEFT JOIN customers_pb b ON stock_backdate.fn_whconsiid = b.fn_whconsiid\n",
    "where 1=1 {qstore_list}\n",
    "GROUP BY fn_whconsiid, fv_artsizecode\n",
    "\"\"\"\n",
    "with get_engine('imp') as engine:\n",
    "    df_frees= pd.read_sql(main_query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(main_query_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(main_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frees.loc[(df_frees['fn_whconsiid']==168)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.loc[df_main['fv_artsizecode']=='GB02378FS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combi=pd.concat([df_main, df_frees,df_freestock], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_combi.groupby(['fn_whconsiid', 'fv_artsizecode'], as_index=False)['stocks'].sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WB03395GV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Q) df__main_filt_arts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_value = input_param['input_date']\n",
    "    # Konversi kembali ke string\n",
    "input_value = '2025-07-10'\n",
    "sales_thru = f\"sales_thru_pb_{input_value[2:4]}\"\n",
    "trxstx = f\"trxstx_pb{input_value[2:4]}\"\n",
    "stock = f\"fn_stock_{input_value[5:7]}\"\n",
    "artikel=input_param['artikel']\n",
    "whconsi_list=168\n",
    "main_query = f\"\"\"\n",
    "SELECT \n",
    "        stock_backdate.fn_whconsiid,\n",
    "        SUM(stock) AS stocks,\n",
    "        stock_backdate.fv_artsizecode,\n",
    "        a.fv_barcode,\n",
    "        a.fv_sizename,\n",
    "        a.fv_configname,\n",
    "        a.fv_namemark,\n",
    "        b.fc_code,\n",
    "        concat(fc_code, ' - ', fv_toko) fv_toko, stat\n",
    "    FROM (\n",
    "        SELECT fn_whconsiid, {stock} AS stock, fv_artsizecode, 'st' stat\n",
    "        FROM gis_local.{sales_thru}\n",
    "        UNION ALL\n",
    "        SELECT fn_whconsiid, fn_stock AS stock, fv_artsizecode, fv_stat stat\n",
    "        FROM gis_local.{trxstx} \n",
    "        WHERE fd_tgltrx BETWEEN '{input_value[0:4]}-{input_value[5:7]}-01' AND '{input_value}'\n",
    "        UNION ALL\n",
    "        SELECT fn_whconsiid, fn_stock AS stock, fv_artsizecode, 'trs' stat\n",
    "        FROM gis_local.trxrt_pb \n",
    "        WHERE fd_tgltrx BETWEEN '{input_value[0:4]}-{input_value[5:7]}-01' AND '{input_value}'\n",
    "        UNION ALL\n",
    "        SELECT fn_whconsiid, fn_stock AS stock, fv_artsizecode, 'var' stat\n",
    "        FROM gis_local.Stock_varian_25\n",
    "        WHERE fd_tgl BETWEEN '{input_value[0:4]}-{input_value[5:7]}-01' AND '{input_value}'\n",
    "    ) stock_backdate\n",
    "    LEFT JOIN artikel_pb a ON a.fv_artsizecode = stock_backdate.fv_artsizecode\n",
    "    LEFT JOIN customers_pb b ON stock_backdate.fn_whconsiid = b.fn_whconsiid\n",
    "    where fv_barcode in ('{artikel}')\n",
    "    GROUP BY fn_whconsiid, fv_artsizecode\n",
    "\"\"\"\n",
    "with get_engine('imp') as engine:\n",
    "    df= pd.read_sql(main_query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['fn_whconsiid']==168)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if input_param==None:\n",
    "qstore_list=''\n",
    "# else:\n",
    "#     store_list = input_param['store_list']\n",
    "#     if store_list:\n",
    "#         placeholders = \", \".join([f\"'{s}'\" for s in store_list])\n",
    "#     df_cus = df__customers_pb_n(placeholders)\n",
    "#     cus_list= df_cus['fn_cusid'].tolist()\n",
    "#     cus_list =  \", \".join([f\"'{s}'\" for s in cus_list])\n",
    "#     qstore_list= f'and fn_cusid in ({cus_list})'\n",
    "query = f'''\n",
    "        SELECT fv_artsizecode,fv_barcode from artikel_pb\n",
    "        '''\n",
    "with get_engine('imp') as engine:\n",
    "    df= pd.read_sql(query, engine)\n",
    "# results = DatabaseConnector.execute_raw_sql(query)\n",
    "# df_art = pd.DataFrame(results)\n",
    "query_1= f'''SELECT \n",
    "    c.fn_cusid ,fv_artsizecode, sum(fn_qty) OstDO\n",
    "    FROM gis_db.dodtl_tb a \n",
    "    inner join gis_db.domst_tb b on a.fc_nodoc=b.fc_nodoc\n",
    "    inner join gis_db.t_ordermst c on b.fc_noreff=fv_noorder\n",
    "    left join gis_db.t_receivingmst d on b.fc_nodoc=d.fc_nosj\n",
    "    where  fv_noreceiving is null {qstore_list} and b.fc_status=\"F\" \n",
    "    group by fn_cusid, fv_artsizecode, fd_date \n",
    "'''\n",
    "with get_engine('write') as engine:\n",
    "    df= pd.read_sql(query_1, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(main_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_now = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "input_value = '2025-05-05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_query = f\"\"\"\n",
    "SELECT \n",
    "        stock_backdate.fn_whconsiid,\n",
    "        SUM(stock) AS stocks,\n",
    "        stock_backdate.fv_artsizecode,\n",
    "        a.fv_barcode,\n",
    "        a.fv_sizename,\n",
    "        a.fv_configname,\n",
    "        a.fv_namemark,\n",
    "        b.fc_code,\n",
    "        concat(fc_code, ' - ', fv_toko) fv_toko, stat\n",
    "    FROM (\n",
    "        SELECT fn_whconsiid, fn_stock AS stock, fv_artsizecode, fv_stat stat\n",
    "        FROM gis_local.{trxstx} \n",
    "        WHERE fd_tgltrx BETWEEN '{input_value}' AND '{input_now}'\n",
    "        UNION ALL\n",
    "        SELECT fn_whconsiid, fn_stock AS stock, fv_artsizecode, 'trs' stat\n",
    "        FROM gis_local.trxrt_pb \n",
    "        WHERE fd_tgltrx BETWEEN '{input_value}' AND '{input_now}'\n",
    "        UNION ALL\n",
    "        SELECT fn_whconsiid, fn_stock AS stock, fv_artsizecode, 'var' stat\n",
    "        FROM gis_local.Stock_varian_25\n",
    "        WHERE fd_tgl BETWEEN '{input_value}' AND '{input_now}'\n",
    "    ) stock_backdate\n",
    "    LEFT JOIN artikel_pb a ON a.fv_artsizecode = stock_backdate.fv_artsizecode\n",
    "    LEFT JOIN customers_pb b ON stock_backdate.fn_whconsiid = b.fn_whconsiid\n",
    "    where fv_barcode in ('{artikel}')\n",
    "    GROUP BY fn_whconsiid, fv_artsizecode, stat\n",
    "\"\"\"\n",
    "with get_engine('imp') as engine:\n",
    "    df_trx= pd.read_sql(main_query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''SELECT * FROM gis_local.Stock_varian_25;\n",
    "'''\n",
    "with get_engine('imp') as engine:  # Menggunakan context manager dengan with\n",
    "                # Mencari tanggal Opname terakhir per tglnow\n",
    "        dvar = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Q) API df_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_str = input_param['start']\n",
    "ends_str = input_param['end']\n",
    "artikel=input_param['artikel']\n",
    "query = f\"\"\"\n",
    "    select fv_barcode, sum(fn_jualpersize) as sales, fn_jualpersize, fv_sizename, fn_whconsiid, trx.fv_artsizecode from (\n",
    "    SELECT fv_barcode, fn_jualpersize, fd_tgltrx, fv_sizename, fn_whconsiid, a.fv_artsizecode FROM gis_local.pb_penjualan_2025 a\n",
    "    left join artikel_pb b on a.fv_artsizecode=b.fv_artsizecode where fv_barcode in ('{artikel}')\n",
    "    union all \n",
    "    SELECT fv_barcode, fn_jualpersize, fd_tgltrx, fv_sizename, fn_whconsiid, a.fv_artsizecode FROM gis_local.pb_penjualan_2024 a\n",
    "    left join artikel_pb b on a.fv_artsizecode=b.fv_artsizecode where fv_barcode in ('{artikel}')) trx\n",
    "    where trx.fd_tgltrx between '{ends_str}' and '{start_str}' \n",
    "    group by fv_barcode, fn_whconsiid\"\"\"\n",
    "with get_engine('imp') as engine:\n",
    "    df_sales= pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (P) API proc_sales_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_value = input_param['input_date']\n",
    "artikel=input_param['artikel']\n",
    "query = f\"\"\"\n",
    "SELECT a.fn_whconsiid, fv_barcode, fn_stock, fd_tgltrx FROM \n",
    "(SELECT * FROM gis_local.trxstx_pb24 \n",
    "where fv_stat like 'receive%%' and fd_tgltrx <= '{input_value}'\n",
    "union all\n",
    "SELECT * FROM gis_local.trxstx_pb25 \n",
    "where fv_stat like 'receive%%' and fd_tgltrx <= '{input_value}') a\n",
    "left join artikel_pb b on a.fv_artsizecode = b.fv_artsizecode\n",
    "WHERE 1=1 and fv_barcode in ('{artikel}')\n",
    "group by fn_whconsiid,fv_barcode, fd_tgltrx \n",
    "\"\"\"\n",
    "with get_engine('imp') as engine:\n",
    "    df_aging= pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_aging.empty:\n",
    "    # Group df_aging by fn_whconsiid and fv_barcode, and get the minimum fd_tgltrx\n",
    "    aging_min_date = df_aging.groupby(['fn_whconsiid', 'fv_barcode'])['fd_tgltrx'].min().reset_index()\n",
    "    aging_min_date.rename(columns={'fd_tgltrx': '1st DO'}, inplace=True)\n",
    "\n",
    "    # Group df_aging by fn_whconsiid and fv_barcode, and get the maximum fd_tgltrx (last_receive)\n",
    "    aging_max_date = df_aging.groupby(['fn_whconsiid', 'fv_barcode'])['fd_tgltrx'].max().reset_index()\n",
    "    aging_max_date.rename(columns={'fd_tgltrx': 'last_receive'}, inplace=True)\n",
    "\n",
    "    # Merge df with aging_min_date on fn_whconsiid and fv_barcode\n",
    "    df = pd.merge(df, aging_min_date, on=['fn_whconsiid', 'fv_barcode'], how='left')\n",
    "    df = pd.merge(df, aging_max_date, on=['fn_whconsiid', 'fv_barcode'], how='left')\n",
    "\n",
    "    # Get stock on 1st DO\n",
    "    # Merge aging_df with aging_min_date to get stock on 1st DO\n",
    "    aging_stock_on_1st_do = pd.merge(\n",
    "        df_aging,\n",
    "        aging_min_date,\n",
    "        on=['fn_whconsiid', 'fv_barcode'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Filter rows where fd_tgltrx matches the 1st DO\n",
    "    aging_stock_on_1st_do = aging_stock_on_1st_do[\n",
    "    aging_stock_on_1st_do['fd_tgltrx'] == aging_stock_on_1st_do['1st DO']]\n",
    "\n",
    "    # Select only the required columns\n",
    "    aging_stock_on_1st_do = aging_stock_on_1st_do[['fn_whconsiid', 'fv_barcode', 'fn_stock']]\n",
    "    # aging_stock_on_1st_do.rename(columns={'fn_stock': 'stock_on_1st_do'}, inplace=True)\n",
    "\n",
    "    # Merge df with aging_stock_on_1st_do to add stock_on_1st_do column\n",
    "    df = pd.merge(df, aging_stock_on_1st_do, on=['fn_whconsiid', 'fv_barcode'], how='left')\n",
    "    df_sum = df.groupby(['fn_whconsiid', 'fv_barcode'], as_index=False)['fn_stock'].sum().drop_duplicates(subset=['fv_barcode'])\n",
    "    df = pd.merge(df, df_sum, on=['fn_whconsiid', 'fv_barcode'], how='left')\n",
    "    df = df.rename(columns={'fn_stock_y': 'stock_on_1st_do'}).drop(columns=['fn_stock_x'])\n",
    "    df['stock_on_1st_do'] = df['stock_on_1st_do'].fillna(0)\n",
    "    # Convert '1st DO' to datetime\n",
    "    df['1st DO'] = pd.to_datetime(df['1st DO'])\n",
    "    df['last_receive'] = pd.to_datetime(df['last_receive'])\n",
    "\n",
    "    # Hitung aging (selisih hari dari last_receive ke hari ini)\n",
    "    df['aging'] = (pd.Timestamp.today().normalize() - df['last_receive']).dt.days.fillna(0).astype(int)\n",
    "\n",
    "    # Format '1st DO' to 'YYYY-MM-DD'\n",
    "    df['1st DO'] = df['1st DO'].dt.strftime('%Y-%m-%d')\n",
    "    df['last_receive'] = df['last_receive'].dt.strftime('%Y-%m-%d')\n",
    "else:\n",
    "    # Jika aging_df kosong, buat kolom dengan nilai default\n",
    "    df['1st DO'] = '2024-01-01'\n",
    "    df['last_receive'] = '2024-01-01'\n",
    "    df['aging'] = '2024-01'\n",
    "    df['stock_on_1st_do'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sizes_bottom = ['25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38',\n",
    "                        '39', '40', '42', '44', 'ALLSIZE']\n",
    "all_sizes_top = ['FS', 'XS', 'SS', 'S', 'M', 'L', 'XL', 'XXL', 'XXXL']\n",
    "all_sizes = all_sizes_bottom + all_sizes_top\n",
    "\n",
    "# Buat pivot table\n",
    "pivot_table = pd.pivot_table(\n",
    "    df,\n",
    "    values='stocks',\n",
    "    index=['fv_barcode', 'fn_whconsiid'],\n",
    "    columns=['fv_sizename'],\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Reset index agar bisa merge\n",
    "pivot_table = pivot_table.reset_index()\n",
    "\n",
    "# Kolom detail yang ingin ditambahkan kembali\n",
    "kolom_detail = ['fv_barcode', '1st DO', 'last_receive', 'aging','fv_configname',\n",
    "                'stock_on_1st_do', 'fn_whconsiid']\n",
    "# Merge detail\n",
    "pivot_table = pd.merge(pivot_table, df[kolom_detail], on=['fv_barcode', 'fn_whconsiid'], how='left')\n",
    "pivot_table = pivot_table.drop_duplicates()\n",
    "\n",
    "# Urutkan kolom agar kolom ukuran di akhir\n",
    "urutan_kolom = kolom_detail + [col for col in pivot_table.columns if col not in kolom_detail]\n",
    "pivot_table = pivot_table[urutan_kolom]\n",
    "\n",
    "# Pastikan semua ukuran muncul\n",
    "for size in all_sizes:\n",
    "    if size not in pivot_table.columns:\n",
    "        pivot_table[size] = 0\n",
    "    pivot_table[size] = pd.to_numeric(pivot_table[size], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Atur ulang urutan: non-size columns dulu, lalu size columns\n",
    "size_columns = [size for size in all_sizes if size in pivot_table.columns]\n",
    "other_columns = [col for col in pivot_table.columns if col not in size_columns]\n",
    "pivot_table = pivot_table[other_columns + size_columns]\n",
    "\n",
    "# Hitung status (BROKEN / SEHAT)\n",
    "top_columns = [\"L\", \"M\", \"S\", \"XL\", \"XS\", \"XXL\", \"XXXL\"]\n",
    "bottom_columns = ['25', '26', '27', '28', '29', '30', '31', '32', '33', '34',\n",
    "                    '35', '36', '37', '38', '39', '40', '42', '44']\n",
    "\n",
    "pivot_table['status'] = np.where(\n",
    "    (pivot_table[\"fv_configname\"] == \"TOP\") & (pivot_table[top_columns].gt(0).sum(axis=1) < 3), \"BROKEN\",\n",
    "    np.where(\n",
    "        (pivot_table[\"fv_configname\"] == \"BOTTOM\") & (pivot_table[bottom_columns].gt(0).sum(axis=1) < 5), \"BROKEN\",\n",
    "        \"SEHAT\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Hitung total jumlah qty size (sumQty)\n",
    "pivot_table['sumQty'] = pivot_table[size_columns].sum(axis=1)\n",
    "pivot_table=pivot_table.drop(columns=['fv_configname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pivot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (P) API proc_sales_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_sales.empty:\n",
    "    # Merge df with df_sales on 'fv_barcode' and 'fn_whconsiid'\n",
    "    df = pd.merge(df, df_sales[['fv_barcode', 'sales', 'fn_whconsiid']], on=['fv_barcode', 'fn_whconsiid'], how='left')\n",
    "    df['sales'] = df['sales'].fillna(0)\n",
    "    # Hitung total stock per barcode\n",
    "    df['sumQty'] = pd.to_numeric(df['sumQty'], errors='coerce')\n",
    "    df['sales'] = pd.to_numeric(df['sales'], errors='coerce')\n",
    "    df['sumQty'] = df['sumQty'].fillna(0)\n",
    "    df['sales'] = df['sales'].fillna(0)\n",
    "    df['stockAwal'] = df['sumQty'] + df['sales']\n",
    "    df = df.reset_index(drop=True)\n",
    "else: \n",
    "    df['sales'] = 0\n",
    "    df['sumQty'] = 0\n",
    "    df['stockAwal'] = df['sumQty'] + df['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_retur.empty:\n",
    "    # Merge df with df_retur on 'fv_barcode' and 'fn_whconsiid'\n",
    "    df = pd.merge(df, df_retur[['fv_barcode', 'retur', 'fn_whconsiid']], on=['fv_barcode', 'fn_whconsiid'], how='left')\n",
    "    df['retur'] = df['retur'].fillna(0)\n",
    "else:\n",
    "    df['retur'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['fv_artsizecode']=='AAAC096H0822S') & (df['fn_whconsiid']==1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Q) df_main_draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whconsi_list=73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_value = input_param['input_date']\n",
    "store_list = input_param['store_list']\n",
    "if store_list:\n",
    "    placeholders = \", \".join([f\"'{s}'\" for s in store_list])\n",
    "df_cus = df__customers_pb_n(placeholders)\n",
    "cus_list= df_cus['fn_cusid'].tolist()\n",
    "cus_list =  \", \".join([f\"'{s}'\" for s in cus_list])\n",
    "whconsi_list=df_cus['fn_whconsiid'].tolist()\n",
    "whconsi_list =  \", \".join([f\"'{s}'\" for s in whconsi_list])\n",
    "sales_thru = f\"sales_thru_pb_{input_value[2:4]}\"\n",
    "trxstx = f\"trxstx_pb{input_value[2:4]}\"\n",
    "stock = f\"fn_stock_{input_value[5:7]}\"\n",
    "\n",
    "main_query = f\"\"\"\n",
    "SELECT \n",
    "fn_whcosiid_asal fn_whconsiid,\n",
    "SUM(fn_qtykirim) AS stocks,\n",
    "a.fv_artsizecode, fv_barcode, fv_sizename\n",
    "from gis_local.t_draftdtl a\n",
    "left join artikel_pb b on a.fv_artsizecode = b.fv_artsizecode\n",
    "WHERE 1=1 and fn_whcosiid_asal in ({whconsi_list})\n",
    "GROUP BY fn_whcosiid_asal , a.fv_artsizecode\n",
    "\"\"\"\n",
    "with get_engine('imp') as engine:\n",
    "    df= pd.read_sql(main_query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kolom_formalitas = ['1st DO', 'last_receive', 'aging', 'fv_configname', 'stock_on_1st_do']\n",
    "\n",
    "# Tambahkan kolom kosong jika belum ada\n",
    "for kolom in kolom_formalitas:\n",
    "    if kolom not in df.columns:\n",
    "        df[kolom] = None  # atau np.nan jika ingin float kosong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sizes_bottom = ['25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38',\n",
    "                    '39', '40', '42', '44', 'ALLSIZE']\n",
    "all_sizes_top = ['FS', 'XS', 'SS', 'S', 'M', 'L', 'XL', 'XXL', 'XXXL']\n",
    "all_sizes = all_sizes_bottom + all_sizes_top\n",
    "\n",
    "# Buat pivot table\n",
    "pivot_table = pd.pivot_table(\n",
    "    df,\n",
    "    values='stocks',\n",
    "    index=['fv_barcode', 'fn_whconsiid'],\n",
    "    columns=['fv_sizename'],\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Reset index agar bisa merge\n",
    "pivot_table = pivot_table.reset_index()\n",
    "\n",
    "# Kolom detail yang ingin ditambahkan kembali\n",
    "kolom_detail = ['fv_barcode', '1st DO', 'last_receive', 'aging','fv_configname',\n",
    "                'stock_on_1st_do', 'fn_whconsiid']\n",
    "# Merge detail\n",
    "pivot_table = pd.merge(pivot_table, df[kolom_detail], on=['fv_barcode', 'fn_whconsiid'], how='left')\n",
    "pivot_table = pivot_table.drop_duplicates()\n",
    "\n",
    "# Urutkan kolom agar kolom ukuran di akhir\n",
    "urutan_kolom = kolom_detail + [col for col in pivot_table.columns if col not in kolom_detail]\n",
    "pivot_table = pivot_table[urutan_kolom]\n",
    "\n",
    "# Pastikan semua ukuran muncul\n",
    "for size in all_sizes:\n",
    "    if size not in pivot_table.columns:\n",
    "        pivot_table[size] = 0\n",
    "    pivot_table[size] = pd.to_numeric(pivot_table[size], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Atur ulang urutan: non-size columns dulu, lalu size columns\n",
    "size_columns = [size for size in all_sizes if size in pivot_table.columns]\n",
    "other_columns = [col for col in pivot_table.columns if col not in size_columns]\n",
    "pivot_table = pivot_table[other_columns + size_columns]\n",
    "\n",
    "# Hitung status (BROKEN / SEHAT)\n",
    "top_columns = [\"L\", \"M\", \"S\", \"XL\", \"XS\", \"XXL\", \"XXXL\"]\n",
    "bottom_columns = ['25', '26', '27', '28', '29', '30', '31', '32', '33', '34',\n",
    "                    '35', '36', '37', '38', '39', '40', '42', '44']\n",
    "\n",
    "pivot_table['status'] = np.where(\n",
    "    (pivot_table[\"fv_configname\"] == \"TOP\") & (pivot_table[top_columns].gt(0).sum(axis=1) < 3), \"BROKEN\",\n",
    "    np.where(\n",
    "        (pivot_table[\"fv_configname\"] == \"BOTTOM\") & (pivot_table[bottom_columns].gt(0).sum(axis=1) < 5), \"BROKEN\",\n",
    "        \"SEHAT\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Hitung total jumlah qty size (sumQty)\n",
    "pivot_table['sumQty'] = pivot_table[size_columns].sum(axis=1)\n",
    "pivot_table=pivot_table.drop(columns=['fv_configname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pivot_table\n",
    "type_table='detail'\n",
    "col='fv_barcode'\n",
    "df_cus = df__all_customers(\"concat(fc_code, ' - ', fv_toko) fv_toko, fn_whconsiid, fc_code\")\n",
    "if col == 'fv_artsizecode':\n",
    "    df_art= df_artikel_pb_('fv_barcode,fv_catname,fv_brandname,fv_divname,fv_colorname,fv_rating,fv_configname,fv_namemark,fm_price,fv_sizename,fv_artsizecode,fv_fitname')\n",
    "    df = pd.merge(df, df_art, on='fv_artsizecode', how='left')\n",
    "else:\n",
    "    df_art= df_artikel_pb_('fv_barcode,fv_catname,fv_brandname,fv_divname,fv_colorname,fv_rating,fv_configname,fv_namemark,fm_price,fv_status').drop_duplicates('fv_barcode')\n",
    "    df = pd.merge(df, df_art, on='fv_barcode', how='left')\n",
    "df = pd.merge(df, df_cus, on='fn_whconsiid', how='left')\n",
    "if type_table == 'mobile_view':\n",
    "    df['fv_sizename_x']=df['fv_sizename_x'].fillna(df['fv_sizename_y'])\n",
    "    df=df.rename(columns={'fv_sizename_x': 'fv_sizename'})\n",
    "df['artikel']= df['fv_barcode']\n",
    "df['fv_artsizecode']= df.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['fv_toko',\n",
    "        'fv_barcode',\n",
    "        '25',\n",
    "        '26',\n",
    "        '27',\n",
    "        '28',\n",
    "        '29',\n",
    "        '30',\n",
    "        '31',\n",
    "        '32',\n",
    "        '33',\n",
    "        '34',\n",
    "        '35',\n",
    "        '36',\n",
    "        '37',\n",
    "        '38',\n",
    "        '39',\n",
    "        '40',\n",
    "        '42',\n",
    "        '44',\n",
    "        'ALLSIZE',\n",
    "        'FS',\n",
    "        'XS',\n",
    "        'SS',\n",
    "        'S',\n",
    "        'M',\n",
    "        'L',\n",
    "        'XL',\n",
    "        'XXL',\n",
    "        'XXXL',\n",
    "        'fn_whconsiid',\n",
    "        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(main_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_mic_dict = {\n",
    "    3079: 'IREN',\n",
    "    26: 'TYAS / YUNI',\n",
    "    3348: 'TYAS / YUNI',\n",
    "    200: 'VINDRA',\n",
    "    1585: 'DIAN/ARUM',\n",
    "    3107: 'NOPAL',\n",
    "    147: 'SHOFI',\n",
    "    3350: 'IMAS / ANGGI',\n",
    "    1804: 'MILI/NENI',\n",
    "    3341: 'MILI/NENI',\n",
    "    1759: 'NOVA',\n",
    "    40: 'SYAFII',\n",
    "    1385: 'RILYA'\n",
    "}\n",
    "# Buat dict baru dengan key = user id, value = nama PIC\n",
    "# user_id_to_name = {uid: name for name, uid in pic_mic_dict.items() if uid is not None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_mic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = pic_mic_dict[3079]\n",
    "print(user_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''SELECT distinct main.fv_nomutasi, fd_tgltransaksi, a.fc_nodoc, c.fv_noreceiving, fd_tglreceive, fn_whconsiid, fn_whconsiid2, d.fv_artsizecode, sub_main.fn_qtykirim, fn_qtyterima FROM gis_db.t_mutasimst main\n",
    "\n",
    "left join gis_db.domst_tb a on main.fv_nomutasi=  a.fc_noreff \n",
    "left join gis_db.t_mutasidtl sub_main on main.fv_nomutasi=  sub_main.fv_nomutasi\n",
    "left join gis_db.t_receivingmst c on a.fc_nodoc=  c.fc_nosj\n",
    "left join gis_db.t_receivingdtl d on c.fv_noreceiving=  d.fv_noreceiving and sub_main.fv_artsizecode=d.fv_artsizecode\n",
    "where a.fd_date between '2025-04-01 00:00:00' and '2025-09-31 23:59:59' and \n",
    "a.fc_status <> \"F\"'''\n",
    "with get_engine('write') as engine:  # Menggunakan context manager dengan with\n",
    "    df = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cuspb=df__customers_pb('fn_whconsiid,fv_toko \"Toko Pengirim\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cuspb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.merge(df,df_cuspb, on='fn_whconsiid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cuspb=df__customers_pb('fn_whconsiid,fv_toko \"Toko Terima\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.merge(df,df_cuspb, left_on='fn_whconsiid2', right_on='fn_whconsiid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_col = {\n",
    "        'fv_nomutasi': 'No Mutasi',\n",
    "        'fc_nodoc': 'No DM',\n",
    "        'fv_noreceiving': 'No Receiving',\n",
    "        'fv_artsizecode': 'Artikel Size',\n",
    "        'fn_qtykirim':'Qty Kirim',\n",
    "        'fn_qtyterima': 'Qty Terima',\n",
    "        'fd_tgltransaksi':'Tgl Mutasi',\n",
    "        'fd_tglreceive':'Tgl Receive'}\n",
    "    # Lakukan rename dan sort\n",
    "df = df.rename(columns=name_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['No Mutasi','Tgl Mutasi','No DM','No Receiving','Tgl Receive','Toko Pengirim','Toko Terima','Artikel Size','Qty Kirim','Qty Terima'\n",
    "    \n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('riwayat mutasi.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''select a.fv_artsizecode, a.fn_qtyopname qty, b.fc_status, fn_qtyskrg qty_fl\n",
    "    from gis_db1.t_historysimpanstokopnamecounter a\n",
    "    right join gis_db1.fl_stockarticleconsipostingmst b on a.fc_nodoc=b.fc_nodoc\n",
    "    right join gis_db1.fl_stockarticleconsipostingdtl c on c.fc_nodoc=b.fc_nodoc\n",
    "    where b.fn_whconsiid= 84 AND SUBSTR(b.fd_tglflag, 1, 10) between '2025-07-01 00:00:01' AND '2025-09-31 00:00:01'\n",
    "    GROUP BY a.fv_artsizecode'''\n",
    "with get_engine('read') as engine:  # Menggunakan context manager dengan with\n",
    "    df = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# ==== Contoh data sederhana ====\n",
    "data = {\n",
    "    \"Year\": [2023]*12 + [2024]*12,\n",
    "    \"Month\": list(range(1,13))*2,\n",
    "    \"Category\": [\"Shirt\"]*24,\n",
    "    \"Size\": [\"M\"]*24,\n",
    "    \"Price\": [200,200,200,180,200,200,200,190,190,200,200,200] * 2,\n",
    "    \"Season\": [\"Rainy\",\"Rainy\",\"Dry\",\"Dry\",\"Dry\",\"Dry\",\"Dry\",\"Dry\",\"Dry\",\"Rainy\",\"Rainy\",\"Rainy\"]*2,\n",
    "    \"Promo\": [0,0,1,0,0,0,1,0,0,0,1,0]*2,\n",
    "    \"Holiday\": [0,0,0,1,0,0,0,0,0,0,0,0]*2,\n",
    "    \"Qty_Sold\": [\n",
    "        # 2023 (training)\n",
    "        120,130,180,220,160,150,200,210,205,190,195,210,\n",
    "        # 2024 (actual for evaluation)\n",
    "        125,135,190,230,170,160,210,220,215,200,210,225\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ==== Pilih kategori & size tertentu ====\n",
    "cat = \"Shirt\"\n",
    "size = \"M\"\n",
    "df_cs = df[(df[\"Category\"]==cat) & (df[\"Size\"]==size)].copy()\n",
    "\n",
    "# Encode categorical (Season)\n",
    "df_cs = pd.get_dummies(df_cs, columns=[\"Season\"], drop_first=True)\n",
    "\n",
    "# Feature set\n",
    "features = [\"Month\",\"Price\",\"Promo\",\"Holiday\",\"Season_Rainy\"]\n",
    "X_train = df_cs[df_cs[\"Year\"]==2023][features]\n",
    "y_train = df_cs[df_cs[\"Year\"]==2023][\"Qty_Sold\"]\n",
    "\n",
    "X_test = df_cs[df_cs[\"Year\"]==2024][features]\n",
    "y_test = df_cs[df_cs[\"Year\"]==2024][\"Qty_Sold\"]\n",
    "\n",
    "# ==== Model ====\n",
    "model = XGBRegressor(objective=\"reg:squarederror\", n_estimators=200, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prediksi\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# ==== Evaluasi ====\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"MAE (Mean Absolute Error): {mae:.2f}\")\n",
    "\n",
    "# ==== Plot ====\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(X_train[\"Month\"], y_train, label=\"Actual 2023\", marker=\"o\")\n",
    "# plt.plot(X_test[\"Month\"], y_test, label=\"Actual 2024\", marker=\"o\")\n",
    "plt.plot(X_test[\"Month\"], y_pred, label=\"Predicted 2024\", marker=\"x\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Qty Sold\")\n",
    "plt.title(f\"Forecast vs Actual - {cat} Size {size}\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==== contoh data ====\n",
    "data = {\n",
    "    \"Month\": [1,2,3,4,5],\n",
    "    \"Category\": [\"Shirt\",\"Shirt\",\"Shirt\",\"Pants\",\"Pants\"],\n",
    "    \"Size\": [\"M\",\"M\",\"L\",\"M\",\"L\"],\n",
    "    \"Color\": [\"Black\",\"White\",\"Blue\",\"Black\",\"White\"],\n",
    "    \"Fitting\": [\"Regular\",\"Slim\",\"Oversize\",\"Regular\",\"Slim\"],\n",
    "    \"Price\": [199000,229000,199000,259000,279000],\n",
    "    \"Season\": [\"Rainy\",\"Rainy\",\"Holiday\",\"Dry\",\"Dry\"],\n",
    "    \"Promo\": [0,1,1,0,1],\n",
    "    \"Holiday\": [0,0,1,0,0],\n",
    "    \"Qty_Sold\": [120,170,210,90,130]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ==== definisi fitur & target ====\n",
    "X = df.drop(\"Qty_Sold\", axis=1)\n",
    "y = df[\"Qty_Sold\"]\n",
    "\n",
    "# Kolom kategorikal\n",
    "categorical = [\"Category\",\"Size\",\"Color\",\"Fitting\",\"Season\"]\n",
    "numeric = [\"Month\",\"Price\",\"Promo\",\"Holiday\"]\n",
    "\n",
    "# One-hot encoding\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\"), categorical),\n",
    "        (\"num\", \"passthrough\", numeric)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ==== Model 1: Linear Regression untuk interpretasi ====\n",
    "linreg_model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "\n",
    "linreg_model.fit(X, y)\n",
    "\n",
    "# Ambil koefisien Linear Regression\n",
    "feature_names = (linreg_model.named_steps[\"preprocessor\"]\n",
    "                 .get_feature_names_out())\n",
    "coef = linreg_model.named_steps[\"regressor\"].coef_\n",
    "\n",
    "coef_df = pd.DataFrame({\"Feature\": feature_names, \"Coefficient\": coef})\n",
    "print(\"=== Pengaruh Fitur (Linear Regression) ===\")\n",
    "print(coef_df.sort_values(by=\"Coefficient\", ascending=False))\n",
    "\n",
    "# ==== Model 2: XGBoost untuk prediksi ====\n",
    "xgb_model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", XGBRegressor(n_estimators=50, random_state=42))\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Hasil Prediksi vs Data Asli (XGBoost) ===\")\n",
    "comparison = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred})\n",
    "print(comparison)\n",
    "\n",
    "# Plot\n",
    "comparison.plot(kind=\"bar\", figsize=(6,4))\n",
    "plt.title(\"Prediksi vs Aktual\")\n",
    "plt.ylabel(\"Qty Sold\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Prog"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
